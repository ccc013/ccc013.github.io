<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[一个算法汪的日常]]></title>
  <subtitle><![CDATA[Stay hungry, stay foolish]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://ccc013.github.io/"/>
  <updated>2019-02-01T03:35:34.418Z</updated>
  <id>http://ccc013.github.io/</id>
  
  <author>
    <name><![CDATA[cai]]></name>
    <email><![CDATA[429546420@qq.com]]></email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[[资源]基于 Pytorch 的 TorchGAN开源了!]]></title>
    <link href="http://ccc013.github.io/2019/02/01/%E8%B5%84%E6%BA%90-%E5%9F%BA%E4%BA%8E-Pytorch-%E7%9A%84-TorchGAN%E5%BC%80%E6%BA%90%E4%BA%86!/"/>
    <id>http://ccc013.github.io/2019/02/01/资源-基于-Pytorch-的-TorchGAN开源了!/</id>
    <published>2019-02-01T03:30:58.000Z</published>
    <updated>2019-02-01T03:35:34.418Z</updated>
    <content type="html"><![CDATA[<p>之前推荐过一个基于 TensorFlow 的 GAN 框架—<a href="https://mp.weixin.qq.com/s/Kd_nsit-JMaEjT5o8rEkKQ" target="_blank" rel="external">谷歌开源的 GAN 库—TFGAN</a>。</p>
<p>而最近也有一个新的 GAN 框架工具，并且是基于 Pytorch 实现的，项目地址如下：</p>
<p><a href="https://github.com/torchgan/torchgan" target="_blank" rel="external">https://github.com/torchgan/torchgan</a></p>
<p>对于习惯使用 Pytorch 框架的同学，现在可以采用这个开源项目快速搭建一个 GAN 网络模型了！</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/torchgan.png" alt=""></p>
<p>目前该开源项目有 400+ 星，它给出了安装的教程、API 文档以及使用教程，文档的地址如下：</p>
<p><a href="https://torchgan.readthedocs.io/en/latest/" target="_blank" rel="external">https://torchgan.readthedocs.io/en/latest/</a></p>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>对于 TorchGAN 的安装，官网给出 3 种方法，但实际上目前仅支持两种安装方式，分别是<code>pip</code>方式安装以及源码安装，采用<code>conda</code>安装的方法目前还不支持。</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/torchgan_install.png" alt=""></p>
<h5 id="Pip-安装方法"><a href="#Pip-安装方法" class="headerlink" title="Pip 安装方法"></a>Pip 安装方法</h5><p>安装最新的发布版本的命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip3 install torchgan</span><br></pre></td></tr></table></figure>
<p>而如果是最新版本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip3 install git+https://github.com/torchgan/torchgan.git</span><br></pre></td></tr></table></figure>
<h5 id="Conda-安装"><a href="#Conda-安装" class="headerlink" title="Conda 安装"></a>Conda 安装</h5><p>这是目前版本还不支持的安装方式，将会在<code>v0.1</code>版本实现这种安装方法。</p>
<h5 id="源码方式安装"><a href="#源码方式安装" class="headerlink" title="源码方式安装"></a>源码方式安装</h5><p>按照下列命令的顺序执行来进行从源码安装</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/torchgan/torchgan&#10;$ cd torchgan&#10;$ python setup.py install</span><br></pre></td></tr></table></figure>
<h5 id="依赖库"><a href="#依赖库" class="headerlink" title="依赖库"></a>依赖库</h5><p><strong>必须按照的依赖库</strong>：</p>
<ul>
<li>Numpy</li>
<li>Pytorch 0.4.1</li>
<li>Torchvision</li>
</ul>
<p><strong>可选</strong></p>
<ul>
<li>TensorboardX：主要是为了采用<code>Tensorboard</code>来观察和记录实验结果。安装通过命令<code>pip install tensorboardX</code></li>
<li>Visdom：为了采用<code>Xisdom</code>进行记录。安装通过命令<code>pip install visdom</code></li>
</ul>
<h4 id="API-文档"><a href="#API-文档" class="headerlink" title="API 文档"></a>API 文档</h4><p>API 的文档目录如下：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/torchgan_api.png" alt=""></p>
<p>从目录主要分为以下几个大类：</p>
<ul>
<li>torchgan.layers：包含当前常用的用于构建 GAN 结构的一些网络层，包括残差块，Self-Attention，谱归一化(Spectral Normalization)等等</li>
<li>torchgan.logging：提供了很强的可视化工具接口，包括对损失函数、梯度、测量标准以及生成图片的可视化等</li>
<li>torchgan.losses：常见的训练 GANs 模型的损失函数，包括原始的对抗损失、最小二乘损失、WGAN的损失函数等；</li>
<li>torchgan.metrics：主要是提供了不同的评判测量标准</li>
<li>torchgan.models：包含常见的 GAN 网络结构，可以直接使用并且也可以进行拓展，包括 DCGAN、cGAN等</li>
<li>torchgan.trainer：主要是提供训练模型的函数接口</li>
</ul>
<h4 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h4><p>教程部分如下所示：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/torchgan_tutorials.png" alt=""></p>
<p>教程给出了几个例子，包括 DCGAN、Self-Attention GAN、CycleGAN 例子，以及如何自定义损伤的方法。</p>
<p>对于 Self-Attention GAN，还提供了一个在谷歌的 Colab 运行的例子，查看链接：</p>
<p><a href="https://torchgan.readthedocs.io/en/latest/tutorials/sagan.html" target="_blank" rel="external">https://torchgan.readthedocs.io/en/latest/tutorials/sagan.html</a></p>
<hr>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>最后，再给出 Github 项目的链接和文档的对应链接地址：</p>
<p><a href="https://github.com/torchgan/torchgan" target="_blank" rel="external">https://github.com/torchgan/torchgan</a></p>
<p><a href="https://torchgan.readthedocs.io/en/latest/index.html" target="_blank" rel="external">https://torchgan.readthedocs.io/en/latest/index.html</a></p>
<p>欢迎关注我的微信公众号—机器学习与计算机视觉，或者扫描下方的二维码，大家一起交流，学习和进步！</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/qrcode_new.jpg" alt=""></p>
<p>之前分享的资源和教程文章有：</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483683&amp;idx=1&amp;sn=3a75e0eb3f2c897bf14777a311017c9a&amp;chksm=fe3b0f56c94c8640f7bf90f0cbdbf5ebab838c6a90b24d43984b8fbdb94405552fada4946fc4&amp;token=985117826&amp;lang=zh_CN#rd" target="_blank" rel="external">推荐几本数据结构算法书籍和课程</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483737&amp;idx=1&amp;sn=5e9a27bd2b88a608a49685213cc0d481&amp;chksm=fe3b0f2cc94c863a0f86a062d4bab98d333332be4b546101fd15f0dd5269f2407ca5f3618e2d&amp;token=985117826&amp;lang=zh_CN#rd" target="_blank" rel="external">[资源分享] Github上八千Star的深度学习500问教程</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483716&amp;idx=1&amp;sn=0dc336f5ef002dd0dd703908288cf6aa&amp;chksm=fe3b0f31c94c8627ad8329cb4688fe08118d79cceb3c27f96a48543253978688d1786cb7a79e&amp;token=985117826&amp;lang=zh_CN#rd" target="_blank" rel="external">[资源分享] 吴恩达最新《机器学习训练秘籍》中文版可以免费下载了！</a></li>
<li><a href="https://mp.weixin.qq.com/s/Si1YaYLfhL1upbjQkvireQ" target="_blank" rel="external">[资源分享] TensorFlow 官方中文版教程来了</a></li>
<li><a href="https://mp.weixin.qq.com/s/0J2raJqiYsYPqwAV1MALaw" target="_blank" rel="external">必读的AI和深度学习博客</a></li>
<li><a href="https://mp.weixin.qq.com/s/vXIM6Ttw37yzhVB_CvXmCA" target="_blank" rel="external">[教程]一份简单易懂的 TensorFlow 教程</a></li>
<li><a href="https://mp.weixin.qq.com/s/Kd_nsit-JMaEjT5o8rEkKQ" target="_blank" rel="external">谷歌开源的 GAN 库—TFGAN</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p>之前推荐过一个基于 TensorFlow 的 GAN 框架—<a href="https://mp.weixin.qq.com/s/Kd_nsit-JMaEjT5o8rEkKQ" target="_blank" rel="external">谷歌开源的 GAN 库—TFGA]]>
    </summary>
    
      <category term="Pytorch" scheme="http://ccc013.github.io/tags/Pytorch/"/>
    
      <category term="工具框架" scheme="http://ccc013.github.io/tags/%E5%B7%A5%E5%85%B7%E6%A1%86%E6%9E%B6/"/>
    
      <category term="深度学习" scheme="http://ccc013.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[[资源]181个Python开源项目分享!]]></title>
    <link href="http://ccc013.github.io/2019/02/01/%E8%B5%84%E6%BA%90-181%E4%B8%AAPython%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%88%86%E4%BA%AB/"/>
    <id>http://ccc013.github.io/2019/02/01/资源-181个Python开源项目分享/</id>
    <published>2019-02-01T03:03:44.000Z</published>
    <updated>2019-02-01T03:06:10.463Z</updated>
    <content type="html"><![CDATA[<p>在基于 GitHub 2018 年 Octoverse 报告中，简要分析了 Github 中哪些编程语言是最佳代表或是趋势。</p>
<p>有许多方法可以衡量编程语言的流行程度。 在Octoverse报告中，GitHub使用了：</p>
<ol>
<li>公共和私有存储库中贡献者使用的主要语言</li>
<li>以主要语言创建和标记的存储库的数量。</li>
</ol>
<p>其中，<strong>2008 - 2018年创建的存储库的顶级编程语言</strong></p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/Top_programming_languages.png" alt=""></p>
<p>其中 JavaScript 在这十年内创建的仓库是最多的，而 Java 和 Python 则是紧随其后。</p>
<p><strong>截至2018年9月30日，贡献者使用的顶级编程语言</strong></p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/Top_programming_languages2.png" alt=""></p>
<p>同样地，JavaScript 也是公共和私有存储库中贡献者使用最多的编程语言。而 Java 和 Python 则继续位列第二和第三名。</p>
<p>实际上，随着目前人工智能的兴起和火热，Python 的使用率是越来越高了，除了可以在人工智能方面会应用 Python，它还可以应用于爬虫、数据分析等领域，应用是非常的广泛。</p>
<p>最近，有位国外友人在 Github 上分享了一份资源，包含了 15 个领域，总共 181 个 Python 的开源项目，为 Github 上的 Awesome 系列又增添了一个。</p>
<h4 id="传送门"><a href="#传送门" class="headerlink" title="传送门"></a>传送门</h4><p><a href="https://github.com/mahmoud/awesome-python-applications" target="_blank" rel="external">https://github.com/mahmoud/awesome-python-applications</a></p>
<h4 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h4><p>这个 Awesome 项目如下图所示:</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/awesome-python-applications2.png" alt=""></p>
<p>这个项目包含的 15 个领域分别是：</p>
<p>互联网、音频、视频、图形、游戏、生产力、组织、通讯、教育、科学、CMS、ERP、静态站点、开发和其他。</p>
<p>每个项目都给出了一个简单的介绍，以及项目地址，即 Github 主页(Repo)，有的项目还附带项目成品主页的链接(Home)，文档链接(Docs)，维基百科页面(WP)，比如对于互联网这个领域：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/awesome-python-applications3.png" alt=""></p>
<hr>
<p>这就是本次分享的 Python 开源项目了！</p>
<p>欢迎关注我的微信公众号—机器学习与计算机视觉，或者扫描下方的二维码，大家一起交流，学习和进步！</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/qrcode_new.jpg" alt=""></p>
<p>之前分享的资源和教程文章有：</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483683&amp;idx=1&amp;sn=3a75e0eb3f2c897bf14777a311017c9a&amp;chksm=fe3b0f56c94c8640f7bf90f0cbdbf5ebab838c6a90b24d43984b8fbdb94405552fada4946fc4&amp;token=985117826&amp;lang=zh_CN#rd" target="_blank" rel="external">推荐几本数据结构算法书籍和课程</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483737&amp;idx=1&amp;sn=5e9a27bd2b88a608a49685213cc0d481&amp;chksm=fe3b0f2cc94c863a0f86a062d4bab98d333332be4b546101fd15f0dd5269f2407ca5f3618e2d&amp;token=985117826&amp;lang=zh_CN#rd" target="_blank" rel="external">[资源分享] Github上八千Star的深度学习500问教程</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483716&amp;idx=1&amp;sn=0dc336f5ef002dd0dd703908288cf6aa&amp;chksm=fe3b0f31c94c8627ad8329cb4688fe08118d79cceb3c27f96a48543253978688d1786cb7a79e&amp;token=985117826&amp;lang=zh_CN#rd" target="_blank" rel="external">[资源分享] 吴恩达最新《机器学习训练秘籍》中文版可以免费下载了！</a></li>
<li><a href="https://mp.weixin.qq.com/s/Si1YaYLfhL1upbjQkvireQ" target="_blank" rel="external">[资源分享] TensorFlow 官方中文版教程来了</a></li>
<li><a href="https://mp.weixin.qq.com/s/0J2raJqiYsYPqwAV1MALaw" target="_blank" rel="external">必读的AI和深度学习博客</a></li>
<li><a href="https://mp.weixin.qq.com/s/vXIM6Ttw37yzhVB_CvXmCA" target="_blank" rel="external">[教程]一份简单易懂的 TensorFlow 教程</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p>在基于 GitHub 2018 年 Octoverse 报告中，简要分析了 Github 中哪些编程语言是最佳代表或是趋势。</p>
<p>有许多方法可以衡量编程语言的流行程度。 在Octoverse报告中，GitHub使用了：</p>
<ol>
<li>公共和私有存储库中]]>
    </summary>
    
      <category term="Github" scheme="http://ccc013.github.io/tags/Github/"/>
    
      <category term="Python" scheme="http://ccc013.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[[GAN学习系列3]采用深度学习和 TensorFlow 实现图片修复(下）]]></title>
    <link href="http://ccc013.github.io/2018/12/22/GAN%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%973-%E9%87%87%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C-TensorFlow-%E5%AE%9E%E7%8E%B0%E5%9B%BE%E7%89%87%E4%BF%AE%E5%A4%8D-%E4%B8%8B%EF%BC%89/"/>
    <id>http://ccc013.github.io/2018/12/22/GAN学习系列3-采用深度学习和-TensorFlow-实现图片修复-下）/</id>
    <published>2018-12-22T10:24:58.000Z</published>
    <updated>2018-12-22T10:29:43.552Z</updated>
    <content type="html"><![CDATA[<p>这是本文的最后一部分内容了，前两部分内容的文章：</p>
<ol>
<li><a href="https://mp.weixin.qq.com/s/S_uiSe74Ti6N_u4Y5Fd6Fw" target="_blank" rel="external">[GAN学习系列3]采用深度学习和 TensorFlow 实现图片修复(上）</a></li>
<li><a href="https://mp.weixin.qq.com/s/nYDZA75JcfsADYyNdXjmJQ" target="_blank" rel="external">[GAN学习系列3]采用深度学习和 TensorFlow 实现图片修复(中）</a></li>
</ol>
<p>以及原文的地址：</p>
<p><a href="http://bamos.github.io/2016/08/09/deep-completion/" target="_blank" rel="external">http://bamos.github.io/2016/08/09/deep-completion/</a></p>
<p>最后一部分的目录如下：</p>
<ul>
<li>第三步：为图像修复寻找最佳的假图片<ul>
<li>利用 DCGANs 实现图像修复</li>
<li>[ML-Heavy] 损失函数</li>
<li>[ML-Heavy] TensorFlow 实现 DCGANs 模型来实现图像修复</li>
<li>修复你的图片</li>
</ul>
</li>
</ul>
<hr>
<h3 id="第三步：为图像修复寻找最佳的假图片"><a href="#第三步：为图像修复寻找最佳的假图片" class="headerlink" title="第三步：为图像修复寻找最佳的假图片"></a>第三步：为图像修复寻找最佳的假图片</h3><h4 id="利用-DCGANs-实现图像修复"><a href="#利用-DCGANs-实现图像修复" class="headerlink" title="利用 DCGANs 实现图像修复"></a>利用 DCGANs 实现图像修复</h4><p>在第二步中，我们定义并训练了判别器<code>D(x)</code>和生成器<code>G(z)</code>，那接下来就是如何利用<code>DCGAN</code>网络模型来完成图片的修复工作了。</p>
<p>在这部分，作者会参考论文<a href="https://arxiv.org/abs/1607.07539" target="_blank" rel="external">“Semantic Image Inpainting with Perceptual and Contextual Losses”</a> 提出的方法。</p>
<p>对于部分图片<code>y</code>，对于缺失的像素部分采用最大化<code>D(y)</code>这种看起来合理的做法并不成功，它会导致生成一些既不属于真实数据分布，也属于生成数据分布的像素值。如下图所示，我们需要一种合理的将<code>y</code>映射到生成数据分布上。</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/inpainting-projection.png" alt=""></p>
<h4 id="ML-Heavy-损失函数"><a href="#ML-Heavy-损失函数" class="headerlink" title="[ML-Heavy] 损失函数"></a>[ML-Heavy] 损失函数</h4><p>首先我们先定义几个符号来用于图像修复。用<code>M</code>表示一个二值的掩码(Mask)，即只有 0 或者是 1 的数值。其中 1 数值表示图片中要保留的部分，而 0 表示图片中需要修复的区域。定义好这个 Mask 后，接下来就是定义如何通过给定一个 Mask 来修复一张图片<code>y</code>，具体的方法就是让<code>y</code>和<code>M</code>的像素对应相乘，这种两个矩阵对应像素的方法叫做<a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices" target="_blank" rel="external"><strong>哈大马乘积</strong></a>)，并且表示为 <code>M ⊙ y</code> ，它们的乘积结果会得到图片中原始部分，如下图所示：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/mask-example.png" alt=""></p>
<p>接下来，假设我们从生成器<code>G</code>的生成结果找到一张图片，如下图公式所示，第二项表示的是<code>DCGAN</code>生成的修复部分：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/math_1.png" alt=""></p>
<p>根据上述公式，我们知道最重要的就是第二项生成部分，也就是需要实现很好修复图片缺失区域的做法。为了实现这个目的，这就需要回顾在第一步提出的两个重要的信息，上下文和感知信息。而这两个信息的获取主要是通过损失函数来实现。损失函数越小，表示生成的<code>G(z)</code>越适合待修复的区域。</p>
<h5 id="Contextual-Loss"><a href="#Contextual-Loss" class="headerlink" title="Contextual Loss"></a>Contextual Loss</h5><p>为了保证输入图片相同的上下文信息，需要让输入图片<code>y</code>（可以理解为标签）中已知的像素和对应在<code>G(z)</code>中的像素尽可能相似，因此需要对产生不相似像素的<code>G(z)</code>做出惩罚。该损失函数如下所示，采用的是 L1 正则化方法：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/math_2.png" alt=""></p>
<p>这里还可以选择采用 L2 正则化方法，但论文中通过实验证明了 L1 正则化的效果更好。</p>
<p>理想的情况是<code>y</code>和<code>G(z)</code>的所有像素值都是相同的，也就是说它们是完全相同的图片，这也就让上述损失函数值为0</p>
<h5 id="Perceptual-Loss"><a href="#Perceptual-Loss" class="headerlink" title="Perceptual Loss"></a>Perceptual Loss</h5><p>为了让修复后的图片看起来非常逼真，我们需要让判别器<code>D</code>具备正确分辨出真实图片的能力。对应的损失函数如下所示：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/math_3.png" alt=""></p>
<p>因此，最终的损失函数如下所示：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/math_4.png" alt=""></p>
<p>这里 λ 是一个超参数，用于控制两个函数的各自重要性。</p>
<p>另外，论文还采用<a href="http://dl.acm.org/citation.cfm?id=882269" target="_blank" rel="external">泊松混合(poisson blending)</a> 方法来平滑重构后的图片。</p>
<h4 id="ML-Heavy-TensorFlow-实现-DCGANs-模型来实现图像修复"><a href="#ML-Heavy-TensorFlow-实现-DCGANs-模型来实现图像修复" class="headerlink" title="[ML-Heavy] TensorFlow 实现 DCGANs 模型来实现图像修复"></a>[ML-Heavy] TensorFlow 实现 DCGANs 模型来实现图像修复</h4><p>代码实现的项目地址如下：</p>
<p><a href="https://github.com/bamos/dcgan-completion.tensorflow" target="_blank" rel="external">https://github.com/bamos/dcgan-completion.tensorflow</a></p>
<p>首先需要新添加的变量是表示用于修复的 mask，如下所示，其大小和输入图片一样</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.mask = tf.placeholder(tf.float32, [None] + self.image_shape, name=&#39;mask&#39;)</span><br></pre></td></tr></table></figure>
<p>对于最小化损失函数的方法是采用常用的梯度下降方法，而在 TensorFlow 中已经实现了<a href="https://en.wikipedia.org/wiki/Automatic_differentiation" target="_blank" rel="external">自动微分</a>的方法，因此只需要添加待实现的损失函数代码即可。添加的代码如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.contextual_loss = tf.reduce_sum(&#10;    tf.contrib.layers.flatten(&#10;        tf.abs(tf.mul(self.mask, self.G) - tf.mul(self.mask, self.images))), 1)&#10;self.perceptual_loss = self.g_loss&#10;self.complete_loss = self.contextual_loss + self.lam*self.perceptual_loss&#10;self.grad_complete_loss = tf.gradients(self.complete_loss, self.z)</span><br></pre></td></tr></table></figure>
<p>接着，就是定义一个 mask。这里作者实现的是位置在图片中心部分的 mask，可以根据需求来添加需要的任意随机位置的 mask，实际上代码中实现了多种 mask</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">if config.maskType == &#39;center&#39;:&#10;    scale = 0.25&#10;    assert(scale &#60;= 0.5)&#10;    mask = np.ones(self.image_shape)&#10;    l = int(self.image_size*scale)&#10;    u = int(self.image_size*(1.0-scale))&#10;    mask[l:u, l:u, :] = 0.0</span><br></pre></td></tr></table></figure>
<p>因为采用梯度下降，所以采用一个 mini-batch 的带有动量的映射梯度下降方法，将<code>z</code>映射到<code>[-1,1]</code>的范围。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for idx in xrange(0, batch_idxs):&#10;    batch_images = ...&#10;    batch_mask = np.resize(mask, [self.batch_size] + self.image_shape)&#10;    zhats = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim))&#10;&#10;    v = 0&#10;    for i in xrange(config.nIter):&#10;        fd = &#123;&#10;            self.z: zhats,&#10;            self.mask: batch_mask,&#10;            self.images: batch_images,&#10;        &#125;&#10;        run = [self.complete_loss, self.grad_complete_loss, self.G]&#10;        loss, g, G_imgs = self.sess.run(run, feed_dict=fd)&#10;        # &#26144;&#23556;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#10;        v_prev = np.copy(v)&#10;        v = config.momentum*v - config.lr*g[0]&#10;        zhats += -config.momentum * v_prev + (1+config.momentum)*v&#10;        zhats = np.clip(zhats, -1, 1)</span><br></pre></td></tr></table></figure>
<h4 id="修复你的图片"><a href="#修复你的图片" class="headerlink" title="修复你的图片"></a>修复你的图片</h4><p>选择需要进行修复的图片，并放在文件夹<code>dcgan-completion.tensorflow/your-test-data/raw</code>下面，然后根据之前第二步的做法来对人脸图片进行对齐操作，然后将操作后的图片放到文件夹<code>dcgan-completion.tensorflow/your-test-data/aligned</code>。作者随机从数据集<code>LFW</code>中挑选图片进行测试，并且保证其<code>DCGAN</code>模型的训练集没有包含<code>LFW</code>中的人脸图片。</p>
<p>接着可以运行下列命令来进行修复工作了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./complete.py ./data/your-test-data/aligned/* --outDir outputImages</span><br></pre></td></tr></table></figure>
<p>上面的代码会将修复图片结果保存在<code>--outDir</code>参数设置的输出文件夹下，接着可以采用<code>ImageMagick</code>工具来生成动图。这里因为动图太大，就只展示修复后的结果图片：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/completion.png" alt=""></p>
<p>而原始的输入待修复图片如下：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/missing_faces.png" alt=""></p>
<hr>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>最后，再给出前两步的文章链接：</p>
<ol>
<li><a href="https://mp.weixin.qq.com/s/S_uiSe74Ti6N_u4Y5Fd6Fw" target="_blank" rel="external">[GAN学习系列3]采用深度学习和 TensorFlow 实现图片修复(上）</a></li>
<li><a href="https://mp.weixin.qq.com/s/nYDZA75JcfsADYyNdXjmJQ" target="_blank" rel="external">[GAN学习系列3]采用深度学习和 TensorFlow 实现图片修复(中）</a></li>
</ol>
<p>当然这个图片修复方法由于也是2016年提出的方法了，所以效果不算特别好，这两年其实已经新出了好多篇新的图片修复方法的论文，比如：</p>
<ol>
<li><p>2016CVPR <a href="https://arxiv.org/abs/1604.07379" target="_blank" rel="external">Context encoders: Feature learning by inpainting</a></p>
</li>
<li><p>Deepfill 2018—<a href="https://arxiv.org/abs/1801.07892" target="_blank" rel="external">Generative Image Inpainting with Contextual Attention</a></p>
</li>
<li><p>Deepfillv2—<a href="https://arxiv.org/abs/1806.03589" target="_blank" rel="external">Free-Form Image Inpainting with Gated Convolution</a></p>
</li>
<li><p>2017CVPR—<a href="https://arxiv.org/abs/1611.09969" target="_blank" rel="external">High-resolution image inpainting using multi-scale neural patch synthesis</a></p>
</li>
<li><p>2018年的 NIPrus收录论文—<a href="https://arxiv.org/abs/1810.08771" target="_blank" rel="external">Image Inpainting via Generative Multi-column Convolutional Neural Networks</a></p>
</li>
</ol>
<hr>
<p>欢迎关注我的微信公众号—机器学习与计算机视觉，或者扫描下方的二维码，在后台留言，和我分享你的建议和看法，指正文章中可能存在的错误，大家一起交流，学习和进步！</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/qrcode_new.jpg" alt=""></p>
<hr>
<p><strong>往期精彩推荐</strong></p>
<p>1.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483667&amp;idx=1&amp;sn=c6b6feb241897ede16bd745d595cef92&amp;chksm=fe3b0f66c94c86701e9b071e62750d189c254fd3ebe9bb6251505162139efefdf866093b38c3&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">机器学习入门系列(1)—机器学习概览(上)</a></p>
<p>2.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483672&amp;idx=1&amp;sn=34b6687030db92fd3e04dcdebd09fffc&amp;chksm=fe3b0f6dc94c867b2a72c427ebb90e2a683e6ad97ea2c5fbdc3a3bb86a8b159b8e5f107d2dcc&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">机器学习入门系列(2)—机器学习概览(下)</a></p>
<p>3.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483711&amp;idx=1&amp;sn=ead88d5b21e08d9df853b72f31d4b5f4&amp;chksm=fe3b0f4ac94c865cfc243123eb4815539ef2d5babdc8346f79a29b681e55eee5f964bdc61d71&amp;token=1493836032&amp;lang=zh_CN#rd" target="_blank" rel="external">[GAN学习系列] 初识GAN</a></p>
<p>4.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483732&amp;idx=1&amp;sn=99cb91edf6fb6da3c7d62132c40b0f62&amp;chksm=fe3b0f21c94c8637a8335998c3fc9d0adf1ac7dea332c2bd45e63707eac6acad8d84c1b3d16d&amp;token=985117826&amp;lang=zh_CN#rd" target="_blank" rel="external">[GAN学习系列2] GAN的起源</a></p>
<p>5.<a href="https://mp.weixin.qq.com/s/S_uiSe74Ti6N_u4Y5Fd6Fw" target="_blank" rel="external">[GAN学习系列3]采用深度学习和 TensorFlow 实现图片修复(上）</a></p>
<p>6.<a href="https://mp.weixin.qq.com/s/nYDZA75JcfsADYyNdXjmJQ" target="_blank" rel="external">[GAN学习系列3]采用深度学习和 TensorFlow 实现图片修复(中）</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这是本文的最后一部分内容了，前两部分内容的文章：</p>
<ol>
<li><a href="https://mp.weixin.qq.com/s/S_uiSe74Ti6N_u4Y5Fd6Fw" target="_blank" rel="external">[GAN学习系列]]>
    </summary>
    
      <category term="GAN" scheme="http://ccc013.github.io/tags/GAN/"/>
    
      <category term="image inpainting" scheme="http://ccc013.github.io/tags/image-inpainting/"/>
    
      <category term="技术" scheme="http://ccc013.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="机器学习" scheme="http://ccc013.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://ccc013.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="http://ccc013.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[[GAN学习系列3]采用深度学习和 TensorFlow 实现图片修复(中）]]></title>
    <link href="http://ccc013.github.io/2018/12/22/GAN%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%973-%E9%87%87%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C-TensorFlow-%E5%AE%9E%E7%8E%B0%E5%9B%BE%E7%89%87%E4%BF%AE%E5%A4%8D-%E4%B8%AD%EF%BC%89/"/>
    <id>http://ccc013.github.io/2018/12/22/GAN学习系列3-采用深度学习和-TensorFlow-实现图片修复-中）/</id>
    <published>2018-12-22T10:24:38.000Z</published>
    <updated>2018-12-22T10:26:37.737Z</updated>
    <content type="html"><![CDATA[<p>上一篇文章—<a href="https://mp.weixin.qq.com/s/S_uiSe74Ti6N_u4Y5Fd6Fw" target="_blank" rel="external">[GAN学习系列3]采用深度学习和 TensorFlow 实现图片修复(上）</a>中，我们先介绍了对于图像修复的背景，需要利用什么信息来对缺失的区域进行修复，以及将图像当做概率分布采样的样本来看待，通过这个思路来开始进行图像的修复。</p>
<p>这篇文章将继续介绍原文的第二部分，利用对抗生成网络来快速生成假图片。目录如下：</p>
<ul>
<li>第二步：快速生成假的图片<ul>
<li>从未知的概率分布中学习生成新的样本</li>
<li>[ML-Heavy] 建立 GAN 模型</li>
<li>采用 G(z) 生成假的图片</li>
<li>[ML-Heavy] 训练 DCGAN</li>
<li>目前的 GAN 和 DCGAN 实现</li>
<li>[ML-Heavy] TensorFlow 实现 DCGAN</li>
<li>在你的数据集上运行 DCGAN 模型</li>
</ul>
</li>
</ul>
<p>同样的，标题带有 [ML-Heavy] 的会介绍比较多的细节，可以选择跳过。</p>
<hr>
<h3 id="第二步：快速生成假的图片"><a href="#第二步：快速生成假的图片" class="headerlink" title="第二步：快速生成假的图片"></a>第二步：快速生成假的图片</h3><h4 id="从未知的概率分布中学习生成新的样本"><a href="#从未知的概率分布中学习生成新的样本" class="headerlink" title="从未知的概率分布中学习生成新的样本"></a>从未知的概率分布中学习生成新的样本</h4><p>与其考虑如何计算概率密度函数，现在在统计学中更好的方法是采用一个<a href="https://en.wikipedia.org/wiki/Generative_model" target="_blank" rel="external">生成模型</a>来学习如何生成新的、随机的样本。过去生成模型一直是很难训练或者非常难以实现，但最近在这个领域已经有了一些让人惊讶的进展。<a href="http://yann.lecun.com/" target="_blank" rel="external">Yann LeCun</a>在这篇 Quora 上的问题<a href="https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning/answer/Yann-LeCun?srid=nZuy" target="_blank" rel="external">“最近在深度学习有什么潜在的突破的领域”</a>中给出了一种训练生成模型（对抗训练）方法的介绍，并将其描述为过去十年内机器学习最有趣的想法：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/lecun-quora.png" alt=""></p>
<p>Yann LeCun 在回答中简单介绍了 GAN 的基本原理，也就是两个网络相互博弈的过程。</p>
<p>实际上，深度学习还有其他方法来训练生成模型，比如 <a href="http://arxiv.org/abs/1312.6114" target="_blank" rel="external">Variational Autoencoders(VAEs)</a>。但在本文，主要介绍对抗生成网络（GANs）</p>
<h4 id="ML-Heavy-建立-GAN-模型"><a href="#ML-Heavy-建立-GAN-模型" class="headerlink" title="[ML-Heavy] 建立 GAN 模型"></a>[ML-Heavy] 建立 GAN 模型</h4><p>GANs 这个想法是 Ian Goodfellow 在其带有里程碑意义的论文<a href="http://papers.nips.cc/paper/5423-generative-adversarial" target="_blank" rel="external">“Generative Adversarial Nets” (GANs)</a>发表在 2014 年的  <a href="https://nips.cc/" target="_blank" rel="external">Neural Information Processing Systems (NIPS)</a> 会议上后开始火遍整个深度学习领域的。这个想法就是我们首先定义一个简单并众所周知的概率分布，并表示为$p_z$，在本文后面，我们用 $p_z$ 表示在[-1,1)（包含-1，但不包含1）范围的均匀分布。用$z \thicksim p_z$表示从这个分布中采样，如果$p_z$是一个五维的，我们可以利用下面一行的 Python 代码来进行采样得到，这里用到 <a href="http://www.numpy.org/" target="_blank" rel="external">numpy</a>这个库：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = np.random.uniform(-1, 1, 5)&#10;array([ 0.77356483,  0.95258473, -0.18345086,  0.69224724, -0.34718733])</span><br></pre></td></tr></table></figure>
<p>现在我们有一个简单的分布来进行采样，接下来可以定义一个函数<code>G(z)</code>来从原始的概率分布中生成样本，代码例子如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def G(z):&#10;   ...&#10;   return imageSample&#10;&#10;z = np.random.uniform(-1, 1, 5)&#10;imageSample = G(z)</span><br></pre></td></tr></table></figure>
<p>那么问题来了，怎么定义这个<code>G(Z)</code>函数，让它实现输入一个向量然后返回一张图片呢？答案就是采用一个深度神经网络。对于深度神经网络基础，网络上有很多的介绍，本文就不再重复介绍了。这里推荐的一些参考有斯坦福大学的 <a href="http://cs231n.github.io/" target="_blank" rel="external">CS231n 课程</a>、Ian Goodfellow 等人编著的<a href="http://www.deeplearningbook.org/" target="_blank" rel="external">《深度学习》书籍</a>、<a href="http://setosa.io/ev/image-kernels/" target="_blank" rel="external">形象解释图像的核心</a>以及论文<a href="https://arxiv.org/abs/1603.07285" target="_blank" rel="external">“A guide to convolution arithmetic for deep learning”</a>。</p>
<p>通过深度学习可以有多种方法来实现<code>G(z)</code>函数。在原始的 GAN 论文中提出一种训练方法并给出初步的实验结果，这个方法得到了极大的发展和改进。其中一种想法就是在论文<a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="external">“Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks”</a>中提出的，这篇论文的作者是 Alec Radford, Luke Metz, and Soumith Chintala，发表在 2016 年的 <a href="http://www.iclr.cc/" target="_blank" rel="external">International Conference on Learning Representations (ICLR)</a>会议上，<strong>这个方法因为提出采用深度卷积神经网络，被称为 DCGANs，它主要采用小步长卷积（ fractionally-strided convolution）方法来上采样图像</strong>。</p>
<p>那么什么是小步长卷积以及如何实现对图片的上采样呢？ Vincent Dumoulin and Francesco Visin’s 在论文<a href="https://arxiv.org/abs/1603.07285" target="_blank" rel="external">“A guide to convolution arithmetic for deep learning”</a>以及 Github 项目都给出了这种卷积算术的详细介绍，Github 地址如下：</p>
<p><a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank" rel="external">https://github.com/vdumoulin/conv_arithmetic</a></p>
<p>上述 Github 项目给出了非常直观的可视化，如下图所示，这让我们可以很直观了解小步长卷积是如何工作的。</p>
<p>首先，你要知道一个正常的卷积操作是一个卷积核划过输入区域（下图中蓝色区域）后生成一个输出区域（下图的绿色区域）。这里，输出区域的尺寸是小于输入区域的。（当然，如果你还不知道，可以先看下斯坦福大学的<a href="http://cs231n.github.io/" target="_blank" rel="external">CS231n 课程</a>或者论文<a href="https://arxiv.org/abs/1603.07285" target="_blank" rel="external">“A guide to convolution arithmetic for deep learning”</a>。）</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/padding_strides.gif" alt=""></p>
<p>接下来，假设输入是 3x3。我们的目标是通过上采样让输出尺寸变大。你可以认为小步长卷积就是在像素之间填充 0 值来拓展输入区域的方法，然后再对输入区域进行卷积操作，正如下图所示，得到一个 5x5 的输出。</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/padding_strides_transposed.gif" alt=""></p>
<p>注意，对于作为上采样的卷积层有很多不同的名字，比如<a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" target="_blank" rel="external">全卷积(full convolution)</a>, 网络内上采样（in-network upsampling）, 小步长卷积（fractionally-strided convolution）, 反向卷积（backwards convolution）, 反卷积（deconvolution）, 上卷积（upconvolution）, 转置卷积（transposed convolution）。这里并不鼓励使用反卷积（deconvolution）这个词语，因为在<a href="https://en.wikipedia.org/wiki/Deconvolution" target="_blank" rel="external">数学运算</a>或者<a href="http://www.matthewzeiler.com/pubs/iccv2011/iccv2011.pdf" target="_blank" rel="external">计算机视觉的其他应用</a>中，这个词语有着其他完全不同的意思，这是一个非常频繁使用的词语。</p>
<p>现在利用小步长卷积作为基础，我们可以实现<code>G(z)</code>函数，让它接收一个$z \thicksim p_z$的向量输入，然后输出一张尺寸是 64x64x3 的彩色图片，其网络结构如下图所示：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/gen-architecture.png" alt=""></p>
<p>在 DCGAN 这篇论文中还提出了其他的一些技巧和改进来训练 DCGANs，比如采用批归一化(batch normalization)或者是 leaky ReLUs 激活函数。</p>
<h4 id="采用-G-z-生成假的图片"><a href="#采用-G-z-生成假的图片" class="headerlink" title="采用 G(z) 生成假的图片"></a>采用 G(z) 生成假的图片</h4><p>现在先让我们暂停并欣赏下这种<code>G(z)</code>网络结构的强大，在 DCGAN 论文中给出了如何采用一个卧室图片数据集训练 一个 DCGAN 模型，然后采用<code>G(z)</code>生成如下的图片，它们都是生成器网络 G 认为的卧室图片，注意，<strong>下面这些图片都是原始训练数据集没有的！</strong></p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/generated-bedrooms.png" alt=""></p>
<p>此外，你还可以对 <code>z</code> 输入实现一个向量算术操作，下图就是一个例子：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/face-arithmetic.png" alt=""></p>
<h4 id="ML-Heavy-训练-DCGAN"><a href="#ML-Heavy-训练-DCGAN" class="headerlink" title="[ML-Heavy] 训练 DCGAN"></a>[ML-Heavy] 训练 DCGAN</h4><p>现在我们定义好了<code>G(z)</code>，也知道它的能力有多强大，问题来了，怎么训练呢？我们需要确定很多隐变量（或者说参数），这也是采用对抗网络的原因了。</p>
<p>首先，我们先定义几个符号。$p_data$表示训练数据，但概率分布未知，$p_z$表示从已知的概率分布采样的样本，一般从高斯分布或者均匀分布采样，<code>z</code>也被称为随机噪声，最后一个，$p_g$就是 G 网络生成的数据，也可以说是生成概率分布。</p>
<p>接着介绍下判别器（discriminator，D）网络，它是输入一批图片<code>x</code>，然后返回该图片来自训练数据$p_{data}$的概率。如果来自训练数据，D 应该返回一个接近 1 的数值，否则应该是一个接近 0 的值来表示图片是假的，来自 G 网络生成的。在 DCGANs 中，D 网络是一个传统的卷积神经网络，如下图所示，一个包含4层卷积层和1层全连接层的卷积神经网络结构。</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/discrim-architecture.png" alt=""></p>
<p>因此，训练 D 网络的目标有以下两个：</p>
<ol>
<li>如果<code>x</code>来自训练数据集，最大化<code>D(x)</code>；</li>
<li>如果<code>x</code>是来自 G 生成的数据，最小化<code>D(x)</code>。</li>
</ol>
<p>对应的 G 网络的目标就是要欺骗 D 网络，生成以假乱真的图片。它生成的图片也是 D 的输入，<strong>所以 G 的目标就是最大化<code>D(G(z))</code>，也等价于最小化<code>1-D(G(z))</code>，因为 D 其实是一个概率估计，且输出范围是在 0 到 1 之间。</strong></p>
<p>正如论文提到的，训练对抗网络就如同在实现一个最小化最大化游戏(minimax game)。如下面的公式所示，第一项是对真实数据分布的期望，第二项是对生成数据的期望值。</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/gan_maths.png" alt=""></p>
<p>训练的步骤如下图所示，具体可以看下我之前写的文章<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483732&amp;idx=1&amp;sn=99cb91edf6fb6da3c7d62132c40b0f62&amp;chksm=fe3b0f21c94c8637a8335998c3fc9d0adf1ac7dea332c2bd45e63707eac6acad8d84c1b3d16d&amp;token=985117826&amp;lang=zh_CN#rd" target="_blank" rel="external">[GAN学习系列2] GAN的起源</a>有简单介绍了这个训练过程，或者是看下 GAN 论文[5]的介绍</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/gan-training.png" alt=""></p>
<h4 id="目前的-GAN-和-DCGAN-实现"><a href="#目前的-GAN-和-DCGAN-实现" class="headerlink" title="目前的 GAN 和 DCGAN 实现"></a>目前的 GAN 和 DCGAN 实现</h4><p>目前在 Github 上有许多 GAN 和 DCGAN 的实现（原文是写于2016年八月份，现在的话代码就更多了）：</p>
<ul>
<li><a href="https://github.com/goodfeli/adversarial" target="_blank" rel="external">https://github.com/goodfeli/adversarial</a></li>
<li><a href="https://github.com/tqchen/mxnet-gan" target="_blank" rel="external">https://github.com/tqchen/mxnet-gan</a></li>
<li><a href="https://github.com/Newmu/dcgan_code" target="_blank" rel="external">https://github.com/Newmu/dcgan_code</a></li>
<li><a href="https://github.com/soumith/dcgan.torch" target="_blank" rel="external">https://github.com/soumith/dcgan.torch</a></li>
<li><a href="https://github.com/carpedm20/DCGAN-tensorflow" target="_blank" rel="external">https://github.com/carpedm20/DCGAN-tensorflow</a></li>
<li><a href="https://github.com/openai/improved-gan" target="_blank" rel="external">https://github.com/openai/improved-gan</a></li>
<li><a href="https://github.com/mattya/chainer-DCGAN" target="_blank" rel="external">https://github.com/mattya/chainer-DCGAN</a></li>
<li><a href="https://github.com/jacobgil/keras-dcgan" target="_blank" rel="external">https://github.com/jacobgil/keras-dcgan</a></li>
</ul>
<p>本文实现的代码是基于 <a href="https://github.com/carpedm20/DCGAN-tensorflow" target="_blank" rel="external">https://github.com/carpedm20/DCGAN-tensorflow</a></p>
<h4 id="ML-Heavy-TensorFlow-实现-DCGAN"><a href="#ML-Heavy-TensorFlow-实现-DCGAN" class="headerlink" title="[ML-Heavy] TensorFlow 实现 DCGAN"></a>[ML-Heavy] TensorFlow 实现 DCGAN</h4><p>这部分的实现的源代码可以在如下 Github 地址：</p>
<p><a href="https://github.com/bamos/dcgan-completion.tensorflow" target="_blank" rel="external">https://github.com/bamos/dcgan-completion.tensorflow</a></p>
<p>当然，主要实现部分代码是来自 <a href="https://github.com/carpedm20/DCGAN-tensorflow" target="_blank" rel="external">https://github.com/carpedm20/DCGAN-tensorflow</a> 。但采用这个项目主要是方便实现下一部分的图像修复工作。</p>
<p>主要实现代码是在<code>model.py</code>中的类<code>DCGAN</code>。采用类来实现模型是有助于训练后保存中间层的状态以及后续的加载使用。</p>
<p>首先，我们需要定义生成器和判别器网络结构。在<code>ops.py</code>会定义网络结构用到的函数，如<code>linear</code>,<code>conv2d_transpose</code>, <code>conv2d</code>以及 <code>lrelu</code>。代码如下所示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def generator(self, z):&#10;    self.z_, self.h0_w, self.h0_b = linear(z, self.gf_dim*8*4*4,&#10;                                           &#39;g_h0_lin&#39;, with_w=True)&#10;&#10;    self.h0 = tf.reshape(self.z_, [-1, 4, 4, self.gf_dim * 8])&#10;    h0 = tf.nn.relu(self.g_bn0(self.h0))&#10;&#10;    self.h1, self.h1_w, self.h1_b = conv2d_transpose(h0,&#10;        [self.batch_size, 8, 8, self.gf_dim*4], name=&#39;g_h1&#39;, with_w=True)&#10;    h1 = tf.nn.relu(self.g_bn1(self.h1))&#10;&#10;    h2, self.h2_w, self.h2_b = conv2d_transpose(h1,&#10;        [self.batch_size, 16, 16, self.gf_dim*2], name=&#39;g_h2&#39;, with_w=True)&#10;    h2 = tf.nn.relu(self.g_bn2(h2))&#10;&#10;    h3, self.h3_w, self.h3_b = conv2d_transpose(h2,&#10;        [self.batch_size, 32, 32, self.gf_dim*1], name=&#39;g_h3&#39;, with_w=True)&#10;    h3 = tf.nn.relu(self.g_bn3(h3))&#10;&#10;    h4, self.h4_w, self.h4_b = conv2d_transpose(h3,&#10;        [self.batch_size, 64, 64, 3], name=&#39;g_h4&#39;, with_w=True)&#10;&#10;    return tf.nn.tanh(h4)&#10;&#10;def discriminator(self, image, reuse=False):&#10;    if reuse:&#10;        tf.get_variable_scope().reuse_variables()&#10;&#10;    h0 = lrelu(conv2d(image, self.df_dim, name=&#39;d_h0_conv&#39;))&#10;    h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name=&#39;d_h1_conv&#39;)))&#10;    h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name=&#39;d_h2_conv&#39;)))&#10;    h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name=&#39;d_h3_conv&#39;)))&#10;    h4 = linear(tf.reshape(h3, [-1, 8192]), 1, &#39;d_h3_lin&#39;)&#10;&#10;    return tf.nn.sigmoid(h4), h4</span><br></pre></td></tr></table></figure>
<p>当初始化这个类的时候，就相当于用上述函数来构建了这个模型。我们需要创建两个 D 网络来共享参数，一个的输入是真实数据，另一个是来自 G 网络的生成数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.G = self.generator(self.z)&#10;self.D, self.D_logits = self.discriminator(self.images)&#10;self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True)</span><br></pre></td></tr></table></figure>
<p>接下来是定义损失函数。这里采用的是 D 的输出之间的交叉熵函数，并且它的效果也不错。D 是期望对真实数据的预测都是 1，对生成的假数据预测都是 0，相反，生成器 G 希望 D 的预测都是 1。代码的实现如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.d_loss_real = tf.reduce_mean(&#10;    tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits,&#10;                                            tf.ones_like(self.D)))&#10;self.d_loss_fake = tf.reduce_mean(&#10;    tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_,&#10;                                            tf.zeros_like(self.D_)))&#10;self.d_loss = self.d_loss_real + self.d_loss_fake&#10;&#10;self.g_loss = tf.reduce_mean(&#10;    tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_,&#10;                                            tf.ones_like(self.D_)))</span><br></pre></td></tr></table></figure>
<p>接着是分别对 G 和 D 的参数聚集到一起，方便后续的梯度计算：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t_vars = tf.trainable_variables()&#10;&#10;self.d_vars = [var for var in t_vars if &#39;d_&#39; in var.name]&#10;self.g_vars = [var for var in t_vars if &#39;g_&#39; in var.name]</span><br></pre></td></tr></table></figure>
<p>现在才有 ADAM 作为优化器来计算梯度，ADAM 是一个深度学习中常用的自适应非凸优化方法，它相比于随机梯度下降方法，不需要手动调整学习率、动量（momentum)以及其他的超参数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \&#10;                    .minimize(self.d_loss, var_list=self.d_vars)&#10;g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \&#10;                    .minimize(self.g_loss, var_list=self.g_vars)</span><br></pre></td></tr></table></figure>
<p>定义好模型和训练策略后，接下来就是开始输入数据进行训练了。在每个 epoch 中，先采样一个 mini-batch 的图片，然后运行优化器来更新网络。有趣的是如果 G 只更新一次，D 的 loss 是不会变为0的。此外，在后面额外调用<code>d_loss_fake</code>和<code>d_loss_real</code>会增加不必要的计算量，并且也是多余的，因为它们的数值在<code>d_optim</code>和<code>g_optim</code>计算的时候已经计算到了。这里你可以尝试优化这部分代码，然后发送一个 PR 到原始的 Github 项目中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for epoch in xrange(config.epoch):&#10;    ...&#10;    for idx in xrange(0, batch_idxs):&#10;        batch_images = ...&#10;        batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \&#10;                    .astype(np.float32)&#10;&#10;        # Update D network&#10;        _, summary_str = self.sess.run([d_optim, self.d_sum],&#10;            feed_dict=&#123; self.images: batch_images, self.z: batch_z &#125;)&#10;&#10;        # Update G network&#10;        _, summary_str = self.sess.run([g_optim, self.g_sum],&#10;            feed_dict=&#123; self.z: batch_z &#125;)&#10;&#10;        # Run g_optim twice to make sure that d_loss does not go to zero&#10;        # (different from paper)&#10;        _, summary_str = self.sess.run([g_optim, self.g_sum],&#10;            feed_dict=&#123; self.z: batch_z &#125;)&#10;&#10;        errD_fake = self.d_loss_fake.eval(&#123;self.z: batch_z&#125;)&#10;        errD_real = self.d_loss_real.eval(&#123;self.images: batch_images&#125;)&#10;        errG = self.g_loss.eval(&#123;self.z: batch_z&#125;)</span><br></pre></td></tr></table></figure>
<p>完整的代码可以在 <a href="https://github.com/bamos/dcgan-completion.tensorflow/blob/master/model.py" target="_blank" rel="external">https://github.com/bamos/dcgan-completion.tensorflow/blob/master/model.py</a> 中查看</p>
<h4 id="在你的数据集上运行-DCGAN-模型"><a href="#在你的数据集上运行-DCGAN-模型" class="headerlink" title="在你的数据集上运行 DCGAN 模型"></a>在你的数据集上运行 DCGAN 模型</h4><p>如果你跳过上一小节，但希望运行一些代码：这部分的实现的源代码可以在如下 Github 地址：</p>
<p><a href="https://github.com/bamos/dcgan-completion.tensorflow" target="_blank" rel="external">https://github.com/bamos/dcgan-completion.tensorflow</a></p>
<p>当然，主要实现部分代码是来自 <a href="https://github.com/carpedm20/DCGAN-tensorflow" target="_blank" rel="external">https://github.com/carpedm20/DCGAN-tensorflow</a> 。但采用这个项目主要是方便实现下一部分的图像修复工作。但必须注意的是，如果你没有一个可以使用 CUDA 的 GPU 显卡，那么训练网络将会非常慢。</p>
<p>首先需要克隆两份项目代码，地址分别如下：</p>
<p><a href="https://github.com/bamos/dcgan-completion.tensorflow" target="_blank" rel="external">https://github.com/bamos/dcgan-completion.tensorflow</a></p>
<p><a href="http://cmusatyalab.github.io/openface" target="_blank" rel="external">http://cmusatyalab.github.io/openface</a></p>
<p>第一份就是作者的项目代码，第二份是采用 OpenFace 的预处理图片的 Python 代码，并不需要安装它的 Torch 依赖包。先创建一个新的工作文件夹，然后开始克隆，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/cmusatyalab/openface.git&#10;git clone https://github.com/bamos/dcgan-completion.tensorflow.git</span><br></pre></td></tr></table></figure>
<p>接着是安装 Python2 版本的 <a href="http://opencv.org/" target="_blank" rel="external">OpenCV</a>和 <a href="http://dlib.net/" target="_blank" rel="external">dlib</a>（采用 Python2 版本是因为 OpenFace 采用这个版本，当然你也可以尝试修改为适应 Python3 版本）。对于 OpenFace 的 Python 库安装，可以查看其安装指导教程，链接如下：</p>
<p><a href="http://cmusatyalab.github.io/openface/setup/" target="_blank" rel="external">http://cmusatyalab.github.io/openface/setup/</a></p>
<p>此外，如果你没有采用一个虚拟环境，那么需要加入<code>sudo</code>命令来运行<code>setup.py</code>实现全局的安装 OpenFace，当然如果安装这部分有问题，也可以采用 OpenFace 的 docker 镜像安装。安装的命令如下所示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd openface&#10;pip2 install -r requirements.txt&#10;python2 setup.py install&#10;models/get-models.sh&#10;cd ..</span><br></pre></td></tr></table></figure>
<p>接着就是下载一些人脸图片数据集了，这里并不要求它们是否带有标签，因为不需要。目前开源可选的数据集包括 </p>
<ul>
<li>MS-Celeb-1M—<a href="https://www.microsoft.com/en-us/research/project/msr-image-recognition-challenge-irc/" target="_blank" rel="external">https://www.microsoft.com/en-us/research/project/msr-image-recognition-challenge-irc/</a></li>
<li>CelebA—<a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank" rel="external">http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</a></li>
<li>CASIA-WebFace—<a href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html" target="_blank" rel="external">http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html</a></li>
<li>FaceScrub—<a href="http://vintage.winklerbros.net/facescrub.html" target="_blank" rel="external">http://vintage.winklerbros.net/facescrub.html</a></li>
<li>LFW—<a href="http://vis-www.cs.umass.edu/lfw/" target="_blank" rel="external">http://vis-www.cs.umass.edu/lfw/</a></li>
<li>MegaFace—<a href="http://megaface.cs.washington.edu/" target="_blank" rel="external">http://megaface.cs.washington.edu/</a></li>
</ul>
<p>然后将数据集放到目录<code>dcgan-completion.tensorflow/data/your-dataset/raw</code>下表示其是原始的图片。</p>
<p>接着采用 OpenFace 的对齐工具来预处理图片并调整成<code>64x64</code>的尺寸：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./openface/util/align-dlib.py data/dcgan-completion.tensorflow/data/your-dataset/raw align innerEyesAndBottomLip data/dcgan-completion.tensorflow/data/your-dataset/aligned --size 64</span><br></pre></td></tr></table></figure>
<p>最后是整理下保存对齐图片的目录，保证只包含图片而没有其他的子文件夹：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd dcgan-completion.tensorflow/data/your-dataset/aligned&#10;find . -name &#39;*.png&#39; -exec mv &#123;&#125; . \;&#10;find . -type d -empty -delete&#10;cd ../../..</span><br></pre></td></tr></table></figure>
<p>然后确保已经安装了 TensorFlow，那么可以开始训练 DCGAN了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./train-dcgan.py --dataset ./data/your-dataset/aligned --epoch 20</span><br></pre></td></tr></table></figure>
<p>在<code>samples</code>文件夹中可以查看保存的由 G 生成的图片。这里作者是采用手上有的两个数据集 CASIA-WebFace 和 FaceScrub 进行训练，并在训练 14 个 epochs 后，生成的结果如下图所示：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/dcgan-results.png" alt=""></p>
<p>还可以通过 TensorBoard 来查看 loss 的变化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir ./logs</span><br></pre></td></tr></table></figure>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/dcgan-tensorboard-results.png" alt=""></p>
<hr>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>这就是本文的第二部分内容，主要是介绍了 DCGAN 的基本原理以及代码实现，还有就是训练前的准备和开始训练，训练的实验结果。</p>
<p>在下一篇将介绍最后一步内容，如何利用 DCGAN 来实现图像修复的工作！</p>
<p>欢迎关注我的微信公众号—机器学习与计算机视觉，或者扫描下方的二维码，在后台留言，和我分享你的建议和看法，指正文章中可能存在的错误，大家一起交流，学习和进步！</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/qrcode_new.jpg" alt=""></p>
<hr>
<p><strong>推荐阅读</strong></p>
<p>1.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483667&amp;idx=1&amp;sn=c6b6feb241897ede16bd745d595cef92&amp;chksm=fe3b0f66c94c86701e9b071e62750d189c254fd3ebe9bb6251505162139efefdf866093b38c3&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">机器学习入门系列(1)—机器学习概览(上)</a></p>
<p>2.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483672&amp;idx=1&amp;sn=34b6687030db92fd3e04dcdebd09fffc&amp;chksm=fe3b0f6dc94c867b2a72c427ebb90e2a683e6ad97ea2c5fbdc3a3bb86a8b159b8e5f107d2dcc&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">机器学习入门系列(2)—机器学习概览(下)</a></p>
<p>3.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483711&amp;idx=1&amp;sn=ead88d5b21e08d9df853b72f31d4b5f4&amp;chksm=fe3b0f4ac94c865cfc243123eb4815539ef2d5babdc8346f79a29b681e55eee5f964bdc61d71&amp;token=1493836032&amp;lang=zh_CN#rd" target="_blank" rel="external">[GAN学习系列] 初识GAN</a></p>
<p>4.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483732&amp;idx=1&amp;sn=99cb91edf6fb6da3c7d62132c40b0f62&amp;chksm=fe3b0f21c94c8637a8335998c3fc9d0adf1ac7dea332c2bd45e63707eac6acad8d84c1b3d16d&amp;token=985117826&amp;lang=zh_CN#rd" target="_blank" rel="external">[GAN学习系列2] GAN的起源</a></p>
<p>5.<a href="https://mp.weixin.qq.com/s/S_uiSe74Ti6N_u4Y5Fd6Fw" target="_blank" rel="external">[GAN学习系列3]采用深度学习和 TensorFlow 实现图片修复(上）</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>上一篇文章—<a href="https://mp.weixin.qq.com/s/S_uiSe74Ti6N_u4Y5Fd6Fw" target="_blank" rel="external">[GAN学习系列3]采用深度学习和 TensorFlow 实现图片修复(上）</]]>
    </summary>
    
      <category term="GAN" scheme="http://ccc013.github.io/tags/GAN/"/>
    
      <category term="image inpainting" scheme="http://ccc013.github.io/tags/image-inpainting/"/>
    
      <category term="技术" scheme="http://ccc013.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="机器学习" scheme="http://ccc013.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://ccc013.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="http://ccc013.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[[GAN学习系列3]采用深度学习和 TensorFlow 实现图片修复(上）]]></title>
    <link href="http://ccc013.github.io/2018/12/10/GAN%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%973-%E9%87%87%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C-TensorFlow-%E5%AE%9E%E7%8E%B0%E5%9B%BE%E7%89%87%E4%BF%AE%E5%A4%8D-%E4%B8%8A%EF%BC%89/"/>
    <id>http://ccc013.github.io/2018/12/10/GAN学习系列3-采用深度学习和-TensorFlow-实现图片修复-上）/</id>
    <published>2018-12-10T15:47:39.000Z</published>
    <updated>2018-12-22T10:29:50.630Z</updated>
    <content type="html"><![CDATA[<p>在之前的两篇 GAN 系列文章—<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483711&amp;idx=1&amp;sn=ead88d5b21e08d9df853b72f31d4b5f4&amp;chksm=fe3b0f4ac94c865cfc243123eb4815539ef2d5babdc8346f79a29b681e55eee5f964bdc61d71&amp;token=1760252914&amp;lang=zh_CN#rd" target="_blank" rel="external">[GAN学习系列1]初识GAN</a>以及<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483732&amp;idx=1&amp;sn=99cb91edf6fb6da3c7d62132c40b0f62&amp;chksm=fe3b0f21c94c8637a8335998c3fc9d0adf1ac7dea332c2bd45e63707eac6acad8d84c1b3d16d&amp;token=985117826&amp;lang=zh_CN#rd" target="_blank" rel="external">[GAN学习系列2] GAN的起源</a>中简单介绍了 GAN 的基本思想和原理，这次就介绍利用 GAN 来做一个图片修复的应用，主要采用的也是 GAN 在网络结构上的升级版—DCGAN，最初始的 GAN 采用的还是神经网络，即全连接网络，而 DCGAN 则是换成卷积神经网络（CNNs）了，这可以很好利用 CNN 强大的特征提取能力，更好的生成质量更好的图片。</p>
<p>原文是：</p>
<p><a href="http://bamos.github.io/2016/08/09/deep-completion/" target="_blank" rel="external">http://bamos.github.io/2016/08/09/deep-completion/</a></p>
<p>由于原文比较长，所以会分为 3 篇来介绍。</p>
<hr>
<p>这篇文章的目录如下：</p>
<ul>
<li>介绍</li>
<li>第一步：将图像解释为概率分布中的样本<ul>
<li>如何填充缺失的信息？</li>
<li>对于图片在哪里适配这些统计数据？</li>
<li>我们如何修复图片呢？</li>
</ul>
</li>
<li>第二步：快速生成假的图片<ul>
<li>从未知的概率分布中学习生成新的样本</li>
<li>[ML-Heavy] 建立 GAN 模型</li>
<li>采用 G(z) 生成假的图片</li>
<li>[ML-Heavy] 训练 DCGAN</li>
<li>目前的 GAN 和 DCGAN 实现</li>
<li>[ML-Heavy] TensorFlow 实现 DCGAN</li>
<li>在你的数据集上运行 DCGAN 模型</li>
</ul>
</li>
<li>第三步：为图像修复寻找最佳的假图片<ul>
<li>利用 DCGANs 实现图像修复</li>
<li>[ML-Heavy] 损失函数</li>
<li>[ML-Heavy] TensorFlow 实现 DCGANs 模型来实现图像修复</li>
<li>修复你的图片</li>
</ul>
</li>
<li>结论</li>
<li>对本文/项目的引用</li>
<li>供进一步阅读的部分参考书目</li>
<li>一些未实现的对于 TensorFlow 和 Torch 的想法</li>
</ul>
<p>本文会先讲述背景和第一步的工作内容。</p>
<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>设计师和摄像师习惯使用一个非常强有力的工具—内容感知填充，来修复图片中不需要或者缺失的部分。图像修复是指用于修复图像中缺失或者毁坏的部分区域。实现图像的修复有很多种方法。在本文中，介绍的是在 2016年7月26日发表在 arXiv 上的论文<a href="https://arxiv.org/abs/1607.07539" target="_blank" rel="external">“Semantic Image Inpainting with Perceptual and Contextual Losses”</a>，这篇论文介绍如何采用 <a href="https://arxiv.org/abs/1511.06434" target="_blank" rel="external">DCGAN</a> 来实现图像修复。这篇文章会即兼顾非机器学习背景和有机器学习背景的读者，带有 [ML-Heavy] 标签的标题内容表示可以跳过这部分细节内容。我们只考虑有限制的修复带有缺失像素的人脸图片的例子。TensorFlow 实现的源代码可以在下面的 Github 地址上查看：</p>
<p><a href="https://github.com/bamos/dcgan-completion.tensorflow" target="_blank" rel="external">https://github.com/bamos/dcgan-completion.tensorflow</a></p>
<p>我们将从以下三个步骤来完成图片修复工作：</p>
<ol>
<li>首先将图像解释为概率分布中的样本</li>
<li>这样的解释步骤可以让我们学习如何生成假的图片</li>
<li>为修复图片寻找最佳的生成图片</li>
</ol>
<p>下面是两张修复前和修复后的图片例子：</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/12/10/16798c319f465f78?w=640&amp;h=455&amp;f=jpeg&amp;s=176605" alt=""></p>
<p><img src="https://user-gold-cdn.xitu.io/2018/12/10/16798c319d5fb288?w=640&amp;h=221&amp;f=jpeg&amp;s=85564" alt=""></p>
<p>下面是本文将用到的带有缺失区域的人脸例子：</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/12/10/16798c319f017cea?w=886&amp;h=803&amp;f=png&amp;s=1388582" alt=""></p>
<h3 id="第一步：将图像解释为概率分布中的样本"><a href="#第一步：将图像解释为概率分布中的样本" class="headerlink" title="第一步：将图像解释为概率分布中的样本"></a>第一步：将图像解释为概率分布中的样本</h3><h4 id="如何填充缺失的信息？"><a href="#如何填充缺失的信息？" class="headerlink" title="如何填充缺失的信息？"></a>如何填充缺失的信息？</h4><p>对于上述几张图片例子，假设你正在设计一个系列来填充这些缺失的区域，你会选择如何做？你认为人脑会怎么处理它呢？你需要使用哪些信息来实现这个修复工作呢？</p>
<p>本文会主要关注下面两种信息：</p>
<ol>
<li><strong>上下文信息(Contextual information)</strong>：利用缺失像素区域周围像素提供的信息来填充</li>
<li><strong>感知信息(Perceptual information)</strong>：将填充的部分解释为“正常”，如同现实生活或者其他图片中看到的一样。</li>
</ol>
<p>这两种信息都非常重要。没有上下文信息，你怎么知道填充什么信息呢？没有感知信息，对于一个上下文来说会有很多种有效的填充方式。比如一些对于机器学习系统来说看上去是“正常”的填充信息，但对于我们人类来说其实就是非常奇怪的填充内容。</p>
<p>因此，有一个即精确又直观的捕获这两种属性，并且可以解释说明如何一步步实现图像修复的算法是再好不过了。创造出这样的算法可能只会适用于特殊的例子，但通常都没有人知道如何创造这样的算法。现在最佳的做法是使用统计数据和机器学习方法来实现一种近似的技术。</p>
<h4 id="对于图片在哪里适配这些统计数据？"><a href="#对于图片在哪里适配这些统计数据？" class="headerlink" title="对于图片在哪里适配这些统计数据？"></a>对于图片在哪里适配这些统计数据？</h4><p>为了解释这个问题，首先介绍一个非常好理解而且能简明表示的<a href="https://en.wikipedia.org/wiki/Probability_distribution" target="_blank" rel="external">概率分布</a>：<a href="https://en.wikipedia.org/wiki/Normal_distribution" target="_blank" rel="external">正态分布</a>。下面是一个正态分布的<a href="https://en.wikipedia.org/wiki/Probability_density_function" target="_blank" rel="external">概率密度函数(probability density function, PDF)</a>的图示。你可以这么理解 PDF，它是水平方向表示输入空间的数值，在垂直方向上表示默写数值发生的概率。</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/12/10/16798c319d9da7be?w=600&amp;h=450&amp;f=png&amp;s=70653" alt=""></p>
<p>上面这张图的绘制代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># !/usr/bin/env python3&#10;&#10;import numpy as np&#10;from scipy.stats import norm&#10;&#10;import matplotlib as mpl&#10;&#10;mpl.use(&#39;Agg&#39;)&#10;import matplotlib.pyplot as plt&#10;&#10;plt.style.use(&#39;bmh&#39;)&#10;import matplotlib.mlab as mlab&#10;&#10;np.random.seed(0)&#10;### &#32472;&#21046;&#19968;&#20010;&#27491;&#24577;&#20998;&#24067;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#22270;###&#10;# &#29983;&#25104;&#25968;&#25454; X&#33539;&#22260;&#26159;(-3,3),&#27493;&#36827;&#20026;0.001, Y&#30340;&#33539;&#22260;&#26159;(0,1)&#10;X = np.arange(-3, 3, 0.001)&#10;Y = norm.pdf(X, 0, 1)&#10;# &#32472;&#21046;&#10;fig = plt.figure()&#10;plt.plot(X, Y)&#10;plt.tight_layout()&#10;plt.savefig(&#34;./images/normal-pdf.png&#34;)</span><br></pre></td></tr></table></figure>
<p>接着可以从上述分布中采样得到一些样本数据，如下图所示：</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/12/10/16798c319d771942?w=700&amp;h=300&amp;f=png&amp;s=7182" alt=""></p>
<p>绘制代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">### &#32472;&#21046;&#20174;&#27491;&#24577;&#20998;&#24067;&#37319;&#26679;&#30340; 1D &#25955;&#28857;&#22270;&#20363;&#23376; ###&#10;nSamples = 35&#10;# np.random.normal &#26159;&#20174;&#27491;&#24577;&#20998;&#24067;&#20013;&#38543;&#26426;&#37319;&#26679;&#25351;&#23450;&#25968;&#37327;&#30340;&#26679;&#26412;,&#36825;&#37324;&#25351;&#23450; 35&#20010;&#10;X = np.random.normal(0, 1, nSamples)&#10;Y = np.zeros(nSamples)&#10;fig = plt.figure(figsize=(7, 3))&#10;# &#32472;&#21046;&#25955;&#28857;&#22270;&#10;plt.scatter(X, Y, color=&#39;k&#39;)&#10;plt.xlim((-3, 3))&#10;frame = plt.gca()&#10;frame.axes.get_yaxis().set_visible(False)&#10;plt.savefig(&#34;./images/normal-samples.png&#34;)</span><br></pre></td></tr></table></figure>
<p>这是 1 维概率分布的例子，因为输入数据就只是一维数据，我们也可以实现二维的例子，如下图所示：</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/12/10/16798c319d495998?w=521&amp;h=521&amp;f=png&amp;s=136815" alt=""></p>
<p>绘制代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">### &#32472;&#21046;&#20174;&#27491;&#24577;&#20998;&#24067;&#37319;&#26679;&#30340; 2D &#25955;&#28857;&#22270;&#20363;&#23376;###&#10;&#10;delta = 0.025&#10;# &#35774;&#32622; X,Y &#30340;&#25968;&#20540;&#33539;&#22260;&#21644;&#27493;&#38271;&#20540;&#65292;&#20998;&#21035;&#29983;&#25104; 240&#20010;&#25968;&#10;x = np.arange(-3.0, 3.0, delta)&#10;y = np.arange(-3.0, 3.0, delta)&#10;print(&#39;x shape&#39;, x.shape)&#10;# &#26681;&#25454;&#22352;&#26631;&#21521;&#37327;&#26469;&#29983;&#25104;&#22352;&#26631;&#30697;&#38453;&#10;X, Y = np.meshgrid(x, y)  # X, Y shape: (240, 240)&#10;&#10;print(&#39;X shape&#39;, X.shape)&#10;print(&#39;Y shape&#39;, Y.shape)&#10;# Bivariate Gaussian distribution for equal shape *X*, *Y*&#10;# &#31561;&#24418;&#29366;&#30340;&#21452;&#21464;&#37327;&#39640;&#26031;&#20998;&#24067;&#10;Z = mlab.bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0)  # Z shape (240, 240)&#10;print(&#39;Z shape&#39;, Z.shape)&#10;&#10;plt.figure()&#10;# &#32472;&#21046;&#29615;&#24418;&#22270;&#36718;&#24275;&#10;CS = plt.contour(X, Y, Z)&#10;plt.clabel(CS, inline=1, fontsize=10)&#10;&#10;nSamples = 200&#10;mean = [0, 0]&#10;cov = [[1, 0], [0, 1]]&#10;# &#20174;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#24471;&#21040;&#32467;&#26524;&#22270;&#20013;&#30340;&#40657;&#28857;&#20363;&#23376;&#10;X, Y = np.random.multivariate_normal(mean, cov, nSamples).T&#10;plt.scatter(X, Y, color=&#39;k&#39;)&#10;&#10;plt.savefig(&#34;./images/normal-2d.png&#34;)</span><br></pre></td></tr></table></figure>
<p>绘制上述三张图的完整代码如下所示，代码地址为：</p>
<p><a href="https://github.com/bamos/dcgan-completion.tensorflow/blob/master/simple-distributions.py" target="_blank" rel="external">https://github.com/bamos/dcgan-completion.tensorflow/blob/master/simple-distributions.py</a></p>
<p>图片和统计学之间的关键关系就是<strong>我们可以将图片解释为高维概率分布的样本</strong>。概率分布就体现在图片的像素上。假设你正采用你的相机进行拍照，照片的像素数量是有限的，当你用相机拍下一张照片的时候，就相当于从这个复杂的概率分布中进行采样的操作。而这个分布也是我们用来定义一张图片是否正常。和正态分布不同的是，只有图片，我们是不知道真实的概率分布，只是在收集样本而已。</p>
<p>在本文中，我们采用 <a href="https://en.wikipedia.org/wiki/RGB_color_model" target="_blank" rel="external">RGB 颜色模型</a>表示的彩色图片。我们采用的是宽和高都是 64 像素的图片，所以概率分布的维度应该是 64×64×3≈12k。</p>
<h5 id="我们如何修复图片呢？"><a href="#我们如何修复图片呢？" class="headerlink" title="我们如何修复图片呢？"></a>我们如何修复图片呢？</h5><p>首先为了更加直观，我们先考虑之前介绍的多元正态分布。给定<code>x=1</code>时，<code>y</code>最有可能的取值是什么呢？这可以通过固定<code>x=1</code>，然后最大化 PDF 的值来找到所有可能的<code>y</code>的取值。如下图所示：</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/12/10/16798c31d4458606?w=521&amp;h=521&amp;f=png&amp;s=176699" alt=""></p>
<p>上图中垂直的黑色直线经过的黑点就是符合要求的<code>y</code>值。</p>
<p>这个概念可以延伸到我们的图像概率分布中，当我们知道某些数值，然后想填补完成缺失的数值的时候。只需要将它当做寻找所有可能缺失数值的最大问题，那么找到的结果就是最有可能的图片。</p>
<p>从视觉上观察由正态分布采样得到的样本，仅凭它们就找到概率密度函数是一件似乎很合理的事情。我们只需要选择最喜欢的<a href="https://en.wikipedia.org/wiki/Statistical_model" target="_blank" rel="external">统计模型</a>并将其与数据相适应即可。</p>
<p>然而，我们并不会应用这个方法。虽然从简单分布中恢复概率密度函数是很简单，但这对于图像的复杂分布是非常困难和棘手的事情。其复杂性一定程度上是来自于复杂的<a href="https://en.wikipedia.org/wiki/Conditional_dependence" target="_blank" rel="external">条件独立性</a>：图像中的每个像素值之间都是相互依赖的。因此，最大化一个通用的概率密度函数是一个极其困难而且往往难以解决的非凸优化问题。</p>
<hr>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>第一篇主要介绍了图像修复的简单背景，然后就是开始实现的第一步，也是比较偏理论，将我们待处理的图片数据作为一个概率分布的样本，并简单用代码实现了一维和二维的正态分布函数图。</p>
<p>在下一篇将介绍第二步内容，也就是快速生成假数据的工作。</p>
<p>欢迎关注我的微信公众号—机器学习与计算机视觉，或者扫描下方的二维码，在后台留言，和我分享你的建议和看法，指正文章中可能存在的错误，大家一起交流，学习和进步！</p>
<p><img src="https://user-gold-cdn.xitu.io/2018/12/5/1677f02ffe1d3918?w=645&amp;h=283&amp;f=jpeg&amp;s=55292" alt=""></p>
<p>我的个人博客：</p>
<p><a href="http://ccc013.github.io/">http://ccc013.github.io/</a></p>
<p>CSDN 博客：</p>
<p><a href="https://blog.csdn.net/lc013/article/details/84845439" target="_blank" rel="external">https://blog.csdn.net/lc013/article/details/84845439</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>在之前的两篇 GAN 系列文章—<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483711&amp;idx=1&amp;sn=ead88d5b21e08d9df853b72f31]]>
    </summary>
    
      <category term="GAN" scheme="http://ccc013.github.io/tags/GAN/"/>
    
      <category term="image inpainting" scheme="http://ccc013.github.io/tags/image-inpainting/"/>
    
      <category term="技术" scheme="http://ccc013.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="机器学习" scheme="http://ccc013.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://ccc013.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="http://ccc013.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[[GAN学习系列2] GAN的起源]]></title>
    <link href="http://ccc013.github.io/2018/12/10/GAN%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%972-GAN%E7%9A%84%E8%B5%B7%E6%BA%90/"/>
    <id>http://ccc013.github.io/2018/12/10/GAN学习系列2-GAN的起源/</id>
    <published>2018-12-10T15:45:13.000Z</published>
    <updated>2018-12-10T15:49:32.370Z</updated>
    <content type="html"><![CDATA[<blockquote>
<p>本文大约 5000 字，阅读大约需要 10 分钟</p>
</blockquote>
<p>这是 GAN 学习系列的第二篇文章，这篇文章将开始介绍 GAN 的起源之作，鼻祖，也就是 Ian Goodfellow 在 2014 年发表在 ICLR 的论文—Generative Adversarial Networks”，当然由于数学功底有限，所以会简单介绍用到的数学公式和背后的基本原理，并介绍相应的优缺点。</p>
<h4 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h4><p>在<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483711&amp;idx=1&amp;sn=ead88d5b21e08d9df853b72f31d4b5f4&amp;chksm=fe3b0f4ac94c865cfc243123eb4815539ef2d5babdc8346f79a29b681e55eee5f964bdc61d71&amp;token=1760252914&amp;lang=zh_CN#rd" target="_blank" rel="external">[GAN学习系列] 初识GAN</a>中，介绍了 GAN 背后的基本思想就是两个网络彼此博弈。生成器 G 的目标是可以学习到输入数据的分布从而生成非常真实的图片，而判别器 D 的目标是可以正确辨别出真实图片和 G 生成的图片之间的差异。正如下图所示：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/generator_and_discriminator1.png" alt=""></p>
<p>上图给出了生成对抗网络的一个整体结构，生成器 G 和判别器 D 都是有各自的网络结构和不同的输入，其中 G 的输出，即生成的样本也是 D 的输入之一，而 D 则会为 G 提供梯度进行权重的更新。</p>
<p>那么问题来了，如果 D 是一个非常好的分类器，那么我们是否真的可以生成非常逼真的样本来欺骗它呢？</p>
<h5 id="对抗样本"><a href="#对抗样本" class="headerlink" title="对抗样本"></a>对抗样本</h5><p>在正式介绍 GAN 的原理之前，先介绍一个概念—<strong>对抗样本(adversarial example)，它是指经过精心计算得到的用于误导分类器的样本</strong>。例如下图就是一个例子，左边是一个熊猫，但是添加了少量随机噪声变成右图后，分类器给出的预测类别却是长臂猿，但视觉上左右两幅图片并没有太大改变。</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/adversarial_attack_example1.png" alt=""></p>
<p>所以为什么在简单添加了噪声后会误导分类器呢？</p>
<p>这是因为<strong>图像分类器本质上是高维空间的一个复杂的决策边界</strong>。当然涉及到图像分类的时候，由于是高维空间而不是简单的两维或者三维空间，我们无法画出这个边界出来。但是我们可以肯定的是，训练完成后，分类器是无法泛化到所有数据上，除非我们的训练集包含了分类类别的所有数据，但实际上我们做不到。而做不到泛化到所有数据的分类器，其实就会过拟合训练集的数据，这也就是我们可以利用的一点。</p>
<p>我们可以给图片添加一个非常接近于 0 的随机噪声，这可以通过控制噪声的 L2 范数来实现。L2 范数可以看做是一个向量的长度，这里有个诀窍就是图片的像素越多，即图片尺寸越大，其平均 L2 范数也就越大。因此，当添加的噪声的范数足够低，那么视觉上你不会觉得这张图片有什么不同，正如上述右边的图片一样，看起来依然和左边原始图片一模一样；但是，在向量空间上，添加噪声后的图片和原始图片已经有很大的距离了！</p>
<p>为什么会这样呢？</p>
<p>因为在 L2 范数看来，对于熊猫和长臂猿的决策边界并没有那么远，添加了非常微弱的随机噪声的图片可能就远离了熊猫的决策边界内，到达长臂猿的预测范围内，因此欺骗了分类器。</p>
<p>除了这种简单的添加随机噪声，还可以通过图像变形的方式，使得新图像和原始图像视觉上一样的情况下，让分类器得到有很高置信度的错误分类结果。这种过程也被称为<strong>对抗攻击(adversarial attack)</strong>，这种生成方式的简单性也是给 GAN 提供了解释。</p>
<h5 id="生成器和判别器"><a href="#生成器和判别器" class="headerlink" title="生成器和判别器"></a>生成器和判别器</h5><p>现在如果将上述说的分类器设定为二值分类器，即判断真和假，那么根据 Ian Goodfellow 的原始论文的说法，它就是判别器（Discriminator）。</p>
<p>有了判别器，那还需要有生成假样本来欺骗判别器的网络，也就是生成器 （Generator）。这两个网络结合起来就是生成对抗网络（GAN），根据原始论文，它的目标如下：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/gan_minmax.png" alt=""></p>
<p>两个网络的工作原理可以如下图所示，D 的目标就是判别真实图片和 G 生成的图片的真假，而 G 是输入一个随机噪声来生成图片，并努力欺骗 D 。</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/discriminator_flow1.jpg" alt=""></p>
<p>简单来说，GAN 的基本思想就是一个最小最大定理，当两个玩家（D 和 G）彼此竞争时（零和博弈），双方都假设对方采取最优的步骤而自己也以最优的策略应对（最小最大策略），那么结果就已经预先确定了，玩家无法改变它（纳什均衡）。</p>
<p>因此，它们的损失函数，D 的是<img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/d_loss.png" alt=""></p>
<p>G 的是</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/g_loss.png" alt=""></p>
<p>这里根据它们的损失函数分析下，G 网络的训练目标就是让 <strong>D(G(z)) 趋近于 1</strong>，这也是让其 loss 变小的做法；而 D 网络的训练目标是区分真假数据，自然是<strong>让 D(x) 趋近于 1，而 D(G(z)) 趋近于 0</strong> 。这就是两个网络相互对抗，彼此博弈的过程了。</p>
<p>那么，它们相互对抗的效果是怎样的呢？在论文中 Ian Goodfellow 用下图来描述这个过程：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/gan_adversial_process.png" alt=""></p>
<p>上图中，黑色曲线表示输入数据 x 的实际分布，绿色曲线表示的是 G 网络生成数据的分布，我们的目标自然是希望着两条曲线可以相互重合，也就是两个数据分布一致了。而蓝色的曲线表示的是生成数据对应于 D 的分布。</p>
<p>在 a 图中是刚开始训练的时候，D 的分类能力还不是最好，因此有所波动，而生成数据的分布也自然和真实数据分布不同，毕竟 G 网络输入是随机生成的噪声；到了 b 图的时候，D 网络的分类能力就比较好了，可以看到对于真实数据和生成数据，它是明显可以区分出来，也就是给出的概率是不同的；</p>
<p>而绿色的曲线，即 G 网络的目标是学习真实数据的分布，所以它会往蓝色曲线方向移动，也就是 c 图了，并且因为 G 和 D 是相互对抗的，当 G 网络提升，也会影响 D 网络的分辨能力。论文中，Ian Goodfellow 做出了证明，当假设 G 网络不变，训练 D 网络，最优的情况会是：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/gan_optimal_d.png" alt=""></p>
<p>也就是<strong>当生成数据的分布 $p_g(x)$ 趋近于真实数据分布 $p_{data}(x) $的时候，D 网络输出的概率 $D_G^*(x)$ 会趋近于 0.5</strong>，也就是 d 图的结果，这也是最终希望达到的训练结果，这时候 G 和 D 网络也就达到一个平衡状态。</p>
<h4 id="训练策略和算法实现"><a href="#训练策略和算法实现" class="headerlink" title="训练策略和算法实现"></a>训练策略和算法实现</h4><p>论文给出的算法实现过程如下所示：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/gan_train.png" alt=""></p>
<p>这里包含了一些训练的技巧和方法：</p>
<ol>
<li>首先 G 和 D 是同步训练，但两者训练次数不一样，通常是 <strong>D 网络训练 k 次后，G 训练一次</strong>。主要原因是 GAN 刚开始训练时候会很不稳定；</li>
<li>D 的训练是<strong>同时输入真实数据和生成数据来计算 loss，而不是采用交叉熵（cross entropy）分开计算</strong>。不采用 cross entropy 的原因是这会让 D(G(z)) 变为 0，导致没有梯度提供给 G 更新，而现在 GAN 的做法是会收敛到 0.5；</li>
<li>实际训练的时候，作者是采用 $-log(D(G(z)))$ 来代替 $log(1-D(G(z)))$ ，这是希望在训练初始就可以加大梯度信息，这是因为初始阶段 D 的分类能力会远大于 G 生成足够真实数据的能力，但这种修改也将让整个 GAN 不再是一个完美的零和博弈。</li>
</ol>
<h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><p>GAN 在巧妙设计了目标函数后，它就拥有以下两个优点。</p>
<ul>
<li>首先，GAN 中的 G 作为生成模型，不需要像传统图模型一样，需要一个严格的生成数据的表达式。这就避免了当数据非常复杂的时候，复杂度过度增长导致的不可计算。</li>
<li>其次，它也不需要 inference 模型中的一些庞大计算量的求和计算。它唯一的需要的就是，一个噪音输入，一堆无标准的真实数据，两个可以逼近函数的网络。</li>
</ul>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><p>虽然 GAN 避免了传统生成模型方法的缺陷，但是在它刚出来两年后，在 2016 年才开始逐渐有非常多和 GAN 相关的论文发表，其原因自然是初代 GAN 的缺点也是非常难解决：</p>
<ul>
<li>首当其冲的缺点就是 GAN 过于自由导致<strong>训练难以收敛以及不稳定</strong>；</li>
<li>其次，原始 G 的损失函数 $log(1-D(G(z)))$ 没有意义，<strong>它是让G 最小化 D 识别出自己生成的假样本的概率，但实际上它会导致梯度消失问题</strong>，这是由于开始训练的时候，G 生成的图片非常糟糕，D 可以轻而易举的识别出来，这样 D 的训练没有任何损失，也就没有有效的梯度信息回传给 G 去优化它自己，这就是梯度消失了；</li>
<li>最后，虽然作者意识到这个问题，在实际应用中改用 $-log(D(G(z)))$ 来代替，这相当于从最小化 D 揪出自己的概率，变成了最大化 D 抓不到自己的概率。虽然直观上感觉是一致的，<strong>但其实并不在理论上等价，也更没有了理论保证在这样的替代目标函数训练下，GAN 还会达到平衡。这个结果会导致模式奔溃问题</strong>，其实也就是<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483711&amp;idx=1&amp;sn=ead88d5b21e08d9df853b72f31d4b5f4&amp;chksm=fe3b0f4ac94c865cfc243123eb4815539ef2d5babdc8346f79a29b681e55eee5f964bdc61d71&amp;token=1760252914&amp;lang=zh_CN#rd" target="_blank" rel="external">[GAN学习系列] 初识GAN</a>中提到的两个缺陷。</li>
</ul>
<p>当然，上述的问题在最近两年各种 GAN 变体中逐渐得到解决方法，比如对于训练太自由的，出现了 cGAN，即提供了一些条件信息给 G 网络，比如类别标签等信息；对于 loss 问题，也出现如 WGAN 等设计新的 loss 来解决这个问题。后续会继续介绍不同的 GAN 的变体，它们在不同方面改进原始 GAN 的问题，并且也应用在多个方面。</p>
<p>参考文章：</p>
<ul>
<li><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="external">Goodfellow et al., “Generative Adversarial Networks”. ICLR 2014.</a></li>
<li><a href="https://sigmoidal.io/beginners-review-of-gan-architectures/" target="_blank" rel="external">beginners-review-of-gan-architectures</a></li>
<li><a href="https://mp.weixin.qq.com/s/dVDDMXS6RA_NWc4EpLQJdw" target="_blank" rel="external">干货 | 深入浅出 GAN·原理篇文字版（完整）</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650730721&amp;idx=2&amp;sn=95b97b80188f507c409f4c72bd0a2767&amp;chksm=871b349fb06cbd891771f72d77563f77986afc9b144f42c8232db44c7c56c1d2bc019458c4e4&amp;scene=21#wechat_redirect" target="_blank" rel="external">深度 | 生成对抗网络初学入门：一文读懂GAN的基本原理（附资源）</a></li>
</ul>
<p>配图来自网络和论文 Generative Adversarial Networks</p>
<p>以上就是本文的主要内容和总结，可以留言给出你对本文的建议和看法。</p>
<p>欢迎关注我的微信公众号—机器学习与计算机视觉或者扫描下方的二维码，和我分享你的建议和看法，指正文章中可能存在的错误，大家一起交流，学习和进步！</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/qrcode_new.jpg" alt=""></p>
<hr>
<p><strong>推荐阅读</strong></p>
<p>1.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483667&amp;idx=1&amp;sn=c6b6feb241897ede16bd745d595cef92&amp;chksm=fe3b0f66c94c86701e9b071e62750d189c254fd3ebe9bb6251505162139efefdf866093b38c3&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">机器学习入门系列(1)—机器学习概览(上)</a><br>2.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483672&amp;idx=1&amp;sn=34b6687030db92fd3e04dcdebd09fffc&amp;chksm=fe3b0f6dc94c867b2a72c427ebb90e2a683e6ad97ea2c5fbdc3a3bb86a8b159b8e5f107d2dcc&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">机器学习入门系列(2)—机器学习概览(下)</a><br>3.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483711&amp;idx=1&amp;sn=ead88d5b21e08d9df853b72f31d4b5f4&amp;chksm=fe3b0f4ac94c865cfc243123eb4815539ef2d5babdc8346f79a29b681e55eee5f964bdc61d71&amp;token=1760252914&amp;lang=zh_CN#rd" target="_blank" rel="external">[GAN学习系列] 初识GAN</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<blockquote>
<p>本文大约 5000 字，阅读大约需要 10 分钟</p>
</blockquote>
<p>这是 GAN 学习系列的第二篇文章，这篇文章将开始介绍 GAN 的起源之作，鼻祖，也就是 Ian Goodfellow 在 2014 年发表在 ICLR 的]]>
    </summary>
    
      <category term="GAN" scheme="http://ccc013.github.io/tags/GAN/"/>
    
      <category term="技术" scheme="http://ccc013.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="机器学习" scheme="http://ccc013.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://ccc013.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="http://ccc013.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[[GAN学习系列] 初识GAN]]></title>
    <link href="http://ccc013.github.io/2018/12/10/GAN%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97-%E5%88%9D%E8%AF%86GAN/"/>
    <id>http://ccc013.github.io/2018/12/10/GAN学习系列-初识GAN/</id>
    <published>2018-12-10T15:43:10.000Z</published>
    <updated>2018-12-10T15:48:58.584Z</updated>
    <content type="html"><![CDATA[<blockquote>
<p>本文大约 3800 字，阅读大约需要 8 分钟</p>
</blockquote>
<p>要说最近几年在深度学习领域最火的莫过于生成对抗网络，即 Generative Adversarial Networks(GANs)了。它是 Ian Goodfellow 在 2014 年发表的，也是这四年来出现的各种 GAN 的变种的开山鼻祖了，下图表示这四年来有关 GAN 的论文的每个月发表数量，可以看出在 2014 年提出后到 2016 年相关的论文是比较少的，但是从 2016 年，或者是 2017 年到今年这两年的时间，相关的论文是真的呈现井喷式增长。</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/cumulative_gans.jpg" alt=""></p>
<p>那么，GAN 究竟是什么呢，它为何会成为这几年这么火的一个研究领域呢？</p>
<p>GAN，即生成对抗网络，是<strong>一个生成模型，也是半监督和无监督学习模型，它可以在不需要大量标注数据的情况下学习深度表征。最大的特点就是提出了一种让两个深度网络对抗训练的方法。</strong></p>
<p>目前机器学习按照数据集是否有标签可以分为三种，监督学习、半监督学习和无监督学习，发展最成熟，效果最好的目前还是监督学习的方法，但是在数据集数量要求更多更大的情况下，获取标签的成本也更加昂贵了，因此越来越多的研究人员都希望能够在无监督学习方面有更好的发展，而 GAN 的出现，一来它是不太需要很多标注数据，甚至可以不需要标签，二来它可以做到很多事情，目前对它的应用包括图像合成、图像编辑、风格迁移、图像超分辨率以及图像转换等。</p>
<p>比如字体的转换，在 <a href="https://github.com/kaonashi-tyc/zi2zi" target="_blank" rel="external">zi2zi</a> 这个项目中，给出了对中文文字的字体的变换，效果如下图所示，GAN 可以学习到不同字体，然后将其进行变换。</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/zi2zi.gif" alt="zi2zi_examples"></p>
<p>除了字体的学习，还有对图片的转换， <a href="https://github.com/affinelayer/pix2pix-tensorflow" target="_blank" rel="external">pix2pix</a> 就可以做到，其结果如下图所示，分割图变成真实照片，从黑白图变成彩色图，从线条画变成富含纹理、阴影和光泽的图等等，这些都是这个 pix2pixGAN 实现的结果。</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/pix2pix_examples.jpg" alt="pix2pix_examples"></p>
<p><a href="https://github.com/junyanz/CycleGAN" target="_blank" rel="external">CycleGAN</a> 则可以做到风格迁移，其实现结果如下图所示，真实照片变成印象画，普通的马和斑马的互换，季节的变换等。</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/cycleGAN_examples.jpg" alt="cycleGAN_examples"></p>
<p>上述是 GAN 的一些应用例子，接下来会简单介绍 GAN 的原理以及其优缺点，当然也还有为啥等它提出两年后才开始有越来越多的 GAN 相关的论文发表。</p>
<h3 id="1-基本原理"><a href="#1-基本原理" class="headerlink" title="1. 基本原理"></a>1. 基本原理</h3><p>GAN 的思想其实非常简单，就是<strong>生成器网络和判别器网络的彼此博弈。</strong></p>
<p>GAN 主要就是两个网络组成，生成器网络(Generator)和判别器网络(Discriminator)，通过这两个网络的互相博弈，让生成器网络最终能够学习到输入数据的分布，这也就是 GAN 想达到的目的—<strong>学习输入数据的分布</strong>。其基本结构如下图所示，从下图可以更好理解G 和 D 的功能，分别为：</p>
<ul>
<li>D 是判别器，负责对输入的真实数据和由 G 生成的假数据进行判断，其输出是 0 和 1，即它本质上是一个二值分类器，目标就是对输入为真实数据输出是 1，对假数据的输入，输出是 0；</li>
<li>G 是生成器，它接收的是一个随机噪声，并生成图像。</li>
</ul>
<p>在训练的过程中，G 的目标是尽可能生成足够真实的数据去迷惑 D，而 D 就是要将 G 生成的图片都辨别出来，这样两者就是互相博弈，最终是要达到一个平衡，也就是纳什均衡。</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/GAN_basic_structures.png" alt=""></p>
<h3 id="2-优点"><a href="#2-优点" class="headerlink" title="2. 优点"></a>2. 优点</h3><p>(以下优点和缺点主要来自 Ian Goodfellow 在 Quora 上的回答，以及知乎上的回答)</p>
<ul>
<li>GAN 模型只用到了反向传播,而不需要马尔科夫链</li>
<li>训练时不需要对隐变量做推断</li>
<li>理论上,只要是可微分函数都可以用于构建 D 和 G ,因为能够与深度神经网络结合做深度生成式模型</li>
<li><strong>G 的参数更新不是直接来自数据样本,而是使用来自 D 的反向传播</strong></li>
<li>相比其他生成模型（VAE、玻尔兹曼机），可以生成更好的生成样本</li>
<li>GAN 是一种半监督学习模型，对训练集不需要太多有标签的数据；</li>
<li>没有必要遵循任何种类的因子分解去设计模型,所有的生成器和鉴别器都可以正常工作</li>
<li></li>
</ul>
<h3 id="3-缺点"><a href="#3-缺点" class="headerlink" title="3. 缺点"></a>3. 缺点</h3><ul>
<li>可解释性差,生成模型的分布 <code>Pg(G)</code>没有显式的表达</li>
<li>比较难训练, D 与 G 之间需要很好的同步,例如 D 更新 k 次而 G 更新一次</li>
<li>训练 GAN 需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到.我们还没有找到很好的达到纳什均衡的方法,所以训练 GAN 相比 VAE 或者 PixelRNN 是不稳定的,但我认为在实践中它还是比训练玻尔兹曼机稳定的多.</li>
<li>它很难去学习生成离散的数据,就像文本</li>
<li>相比玻尔兹曼机,GANs 很难根据一个像素值去猜测另外一个像素值,GANs 天生就是做一件事的,那就是一次产生所有像素,你可以用 BiGAN 来修正这个特性,它能让你像使用玻尔兹曼机一样去使用 Gibbs 采样来猜测缺失值</li>
<li>训练不稳定，G 和 D 很难收敛；</li>
<li>训练还会遭遇梯度消失、模式崩溃的问题</li>
<li>缺乏比较有效的直接可观的评估模型生成效果的方法</li>
</ul>
<h4 id="3-1-为什么训练会出现梯度消失和模式奔溃"><a href="#3-1-为什么训练会出现梯度消失和模式奔溃" class="headerlink" title="3.1 为什么训练会出现梯度消失和模式奔溃"></a>3.1 为什么训练会出现梯度消失和模式奔溃</h4><p>GAN 的本质就是 G 和 D 互相博弈并最终达到一个纳什平衡点，但这只是一个理想的情况，正常情况是容易出现一方强大另一方弱小，并且一旦这个关系形成，而没有及时找到方法平衡，那么就会出现问题了。而梯度消失和模式奔溃其实就是这种情况下的两个结果，分别对应 D 和 G 是强大的一方的结果。</p>
<p>首先对于梯度消失的情况是<strong>D 越好，G 的梯度消失越严重</strong>，因为 G 的梯度更新来自 D，而在训练初始阶段，G 的输入是随机生成的噪声，肯定不会生成很好的图片，D 会很容易就判断出来真假样本，也就是 D 的训练几乎没有损失，也就没有有效的梯度信息回传给 G 让 G 去优化自己。这样的现象叫做 gradient vanishing，梯度消失问题。</p>
<p>其次，对于模式奔溃（mode collapse）问题，主要就是 G 比较强，导致 D 不能很好区分出真实图片和 G 生成的假图片，而如果此时 G 其实还不能完全生成足够真实的图片的时候，但 D 却分辨不出来，并且给出了正确的评价，那么 G 就会认为这张图片是正确的，接下来就继续这么输出这张或者这些图片，然后 D 还是给出正确的评价，于是两者就是这么相互欺骗，这样 G 其实就只会输出固定的一些图片，导致的结果除了生成图片不够真实，还有就是多样性不足的问题。</p>
<p>更详细的解释可以参考 <a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="external">令人拍案叫绝的Wasserstein GAN</a>，这篇文章更详细解释了原始 GAN 的问题，主要就是出现在 loss 函数上。</p>
<h4 id="3-2-为什么GAN不适合处理文本数据"><a href="#3-2-为什么GAN不适合处理文本数据" class="headerlink" title="3.2 为什么GAN不适合处理文本数据"></a>3.2 为什么GAN不适合处理文本数据</h4><ol>
<li>文本数据相比较图片数据来说是离散的，因为对于文本来说，通常需要将一个词映射为一个高维的向量，最终预测的输出是一个one-hot向量，假设 softmax 的输出是<code>（0.2， 0.3， 0.1，0.2，0.15，0.05）</code>，那么变为 onehot是（0，1，0，0，0，0），如果softmax输出是（0.2， 0.25， 0.2， 0.1，0.15，0.1 ），one-hot 仍然是<code>（0， 1， 0， 0， 0， 0）</code>，所以对于生成器来说，G 输出了不同的结果, 但是 D 给出了同样的判别结果，并不能将梯度更新信息很好的传递到 G 中去，所以 D 最终输出的判别没有意义。</li>
<li>GAN 的损失函数是 JS 散度，JS 散度不适合衡量不想交分布之间的距离。（WGAN 虽然使用 wassertein 距离代替了 JS 散度，但是在生成文本上能力还是有限，GAN 在生成文本上的应用有 seq-GAN,和强化学习结合的产物）</li>
</ol>
<h4 id="3-3-为什么GAN中的优化器不常用SGD"><a href="#3-3-为什么GAN中的优化器不常用SGD" class="headerlink" title="3.3 为什么GAN中的优化器不常用SGD"></a>3.3 为什么GAN中的优化器不常用SGD</h4><ol>
<li>SGD 容易震荡，容易使 GAN 的训练更加不稳定，</li>
<li>GAN 的目的是在高维非凸的参数空间中找到<strong>纳什均衡点</strong>，GAN 的纳什均衡点是一个<strong>鞍点</strong>，但是 SGD 只会找到<strong>局部极小值</strong>，因为 SGD 解决的是一个寻找最小值的问题，但 GAN 是一个博弈问题。</li>
</ol>
<p>对于鞍点，来自百度百科的解释是：</p>
<blockquote>
<p>鞍点（Saddle point）在微分方程中，沿着某一方向是稳定的，另一条方向是不稳定的奇点，叫做鞍点。在<a href="https://baike.baidu.com/item/%E6%B3%9B%E5%87%BD/9410786" target="_blank" rel="external">泛函</a>中，既不是极大值点也不是极小值点的<a href="https://baike.baidu.com/item/%E4%B8%B4%E7%95%8C%E7%82%B9/2867636" target="_blank" rel="external">临界点</a>，叫做鞍点。在矩阵中，一个数在所在行中是最大值，在所在列中是最小值，则被称为鞍点。在物理上要广泛一些，指在一个方向是极大值，另一个方向是极小值的点。</p>
</blockquote>
<p>鞍点和局部极小值点、局部极大值点的区别如下图所示：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/%E9%9E%8D%E7%82%B9%E5%92%8C%E5%B1%80%E9%83%A8%E6%9E%81%E5%B0%8F%E5%80%BC%E7%82%B9%E7%9A%84%E5%8C%BA%E5%88%AB.png" alt="局部极小值点和鞍点的对比"></p>
<h3 id="4-训练的技巧"><a href="#4-训练的技巧" class="headerlink" title="4. 训练的技巧"></a>4. 训练的技巧</h3><p>训练的技巧主要来自<a href="https://github.com/soumith/ganhacks" target="_blank" rel="external">Tips and tricks to make GANs work</a>。</p>
<h5 id="1-对输入进行规范化"><a href="#1-对输入进行规范化" class="headerlink" title="1. 对输入进行规范化"></a>1. 对输入进行规范化</h5><ul>
<li>将输入规范化到 -1 和 1 之间</li>
<li>G 的输出层采用<code>Tanh</code>激活函数</li>
</ul>
<h5 id="2-采用修正的损失函数"><a href="#2-采用修正的损失函数" class="headerlink" title="2. 采用修正的损失函数"></a>2. 采用修正的损失函数</h5><p>在原始 GAN 论文中，损失函数 G 是要 $min (log(1-D))$, 但实际使用的时候是采用 $max(logD)$，作者给出的原因是前者会导致梯度消失问题。</p>
<p>但实际上，即便是作者提出的这种实际应用的损失函数也是存在问题，即模式奔溃的问题，在接下来提出的 GAN 相关的论文中，就有不少论文是针对这个问题进行改进的，如 WGAN 模型就提出一种新的损失函数。</p>
<h5 id="3-从球体上采样噪声"><a href="#3-从球体上采样噪声" class="headerlink" title="3. 从球体上采样噪声"></a>3. 从球体上采样噪声</h5><ul>
<li>不要采用均匀分布来采样</li>
<li>从高斯分布中采样得到随机噪声</li>
<li>当进行插值操作的时候，从大圆进行该操作，而不要直接从点 A 到 点 B 直线操作，如下图所示</li>
</ul>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/sphere.png" alt=""></p>
<ul>
<li>更多细节可以参考 Tom White’s 的论文 <a href="https://arxiv.org/abs/1609.04468" target="_blank" rel="external">Sampling Generative Networks</a> 以及代码  <a href="https://github.com/dribnet/plat" target="_blank" rel="external">https://github.com/dribnet/plat</a></li>
</ul>
<h5 id="4-BatchNorm"><a href="#4-BatchNorm" class="headerlink" title="4. BatchNorm"></a>4. BatchNorm</h5><ul>
<li>采用 mini-batch BatchNorm，要保证每个 mini-batch 都是同样的真实图片或者是生成图片</li>
<li>不采用 BatchNorm 的时候，可以采用 instance normalization（对每个样本的规范化操作） </li>
<li>可以使用<strong>虚拟批量归一化</strong>(virtural batch normalization):开始训练之前预定义一个 batch R，对每一个新的 batch X，都使用 R+X 的级联来计算归一化参数</li>
</ul>
<h5 id="5-避免稀疏的梯度：Relus、MaxPool"><a href="#5-避免稀疏的梯度：Relus、MaxPool" class="headerlink" title="5. 避免稀疏的梯度：Relus、MaxPool"></a>5. 避免稀疏的梯度：Relus、MaxPool</h5><ul>
<li>稀疏梯度会影响 GAN 的稳定性</li>
<li>在 G 和 D 中采用 LeakyReLU 代替 Relu 激活函数</li>
<li>对于下采样操作，可以采用平均池化(Average Pooling) 和 Conv2d+stride 的替代方案</li>
<li>对于上采样操作，可以使用 PixelShuffle(<a href="https://arxiv.org/abs/1609.05158" target="_blank" rel="external">https://arxiv.org/abs/1609.05158</a>), ConvTranspose2d + stride</li>
</ul>
<h5 id="6-标签的使用"><a href="#6-标签的使用" class="headerlink" title="6. 标签的使用"></a>6. 标签的使用</h5><ul>
<li>标签平滑。也就是如果有两个目标标签，假设真实图片标签是 1，生成图片标签是 0，那么对每个输入例子，如果是真实图片，采用 0.7 到 1.2 之间的一个随机数字来作为标签，而不是 1；一般是采用单边标签平滑</li>
<li>在训练 D 的时候，偶尔翻转标签</li>
<li>有标签数据就尽量使用标签</li>
</ul>
<h5 id="7-使用-Adam-优化器"><a href="#7-使用-Adam-优化器" class="headerlink" title="7. 使用 Adam 优化器"></a>7. 使用 Adam 优化器</h5><h5 id="8-尽早追踪失败的原因"><a href="#8-尽早追踪失败的原因" class="headerlink" title="8. 尽早追踪失败的原因"></a>8. 尽早追踪失败的原因</h5><ul>
<li>D 的 loss 变成 0，那么这就是训练失败了</li>
<li>检查规范的梯度：如果超过 100，那出问题了</li>
<li>如果训练正常，那么 D loss 有低方差并且随着时间降低</li>
<li>如果 g loss 稳定下降，那么它是用糟糕的生成样本欺骗了 D</li>
</ul>
<h5 id="9-不要通过统计学来平衡-loss"><a href="#9-不要通过统计学来平衡-loss" class="headerlink" title="9. 不要通过统计学来平衡 loss"></a>9. 不要通过统计学来平衡 loss</h5><h5 id="10-给输入添加噪声"><a href="#10-给输入添加噪声" class="headerlink" title="10. 给输入添加噪声"></a>10. 给输入添加噪声</h5><ul>
<li>给 D 的输入添加人为的噪声<ul>
<li><a href="http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/" target="_blank" rel="external">http://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/</a></li>
<li><a href="https://openreview.net/forum?id=Hk4_qw5xe" target="_blank" rel="external">https://openreview.net/forum?id=Hk4_qw5xe</a></li>
</ul>
</li>
<li>给 G 的每层都添加高斯噪声</li>
</ul>
<h5 id="11-对于-Conditional-GANs-的离散变量"><a href="#11-对于-Conditional-GANs-的离散变量" class="headerlink" title="11. 对于 Conditional GANs 的离散变量"></a>11. 对于 Conditional GANs 的离散变量</h5><ul>
<li>使用一个 Embedding 层</li>
<li>对输入图片添加一个额外的通道</li>
<li>保持 embedding 低维并通过上采样操作来匹配图像的通道大小</li>
</ul>
<h5 id="12-在-G-的训练和测试阶段使用-Dropouts"><a href="#12-在-G-的训练和测试阶段使用-Dropouts" class="headerlink" title="12 在 G 的训练和测试阶段使用 Dropouts"></a>12 在 G 的训练和测试阶段使用 Dropouts</h5><ul>
<li>以 dropout 的形式提供噪声(50%的概率)</li>
<li>训练和测试阶段，在 G 的几层使用</li>
<li><a href="https://arxiv.org/pdf/1611.07004v1.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1611.07004v1.pdf</a></li>
</ul>
<p>参考文章：</p>
<ul>
<li><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="external">Goodfellow et al., “Generative Adversarial Networks”. ICLR 2014. </a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&amp;mid=2247484964&amp;idx=1&amp;sn=a859222f408a991dbade1909917595ae&amp;chksm=f9d158bccea6d1aa5a7afb17d39c704d719a7b47613250bff50928343fe49a63a72c27e7bab0&amp;scene=21#wechat_redirect" target="_blank" rel="external">GAN系列学习(1)——前生今世</a></li>
<li><a href="https://mp.weixin.qq.com/s/dVDDMXS6RA_NWc4EpLQJdw" target="_blank" rel="external">干货 | 深入浅出 GAN·原理篇文字版（完整）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25071913" target="_blank" rel="external">令人拍案叫绝的Wasserstein GAN</a></li>
<li><a href="https://www.zhihu.com/question/56171002/answer/148593584" target="_blank" rel="external">生成对抗网络(GAN)相比传统训练方法有什么优势?</a></li>
<li><a href="https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="external">the-gan-zoo</a></li>
<li><a href="https://www.quora.com/What-is-the-advantage-of-generative-adversarial-networks-compared-with-other-generative-models" target="_blank" rel="external">What-is-the-advantage-of-generative-adversarial-networks-compared-with-other-generative-models</a></li>
<li><a href="https://www.quora.com/What-are-the-pros-and-cons-of-using-generative-adversarial-networks-a-type-of-neural-network-Could-they-be-applied-to-things-like-audio-waveform-via-RNN-Why-or-why-not" target="_blank" rel="external">What-are-the-pros-and-cons-of-using-generative-adversarial-networks-a-type-of-neural-network-Could-they-be-applied-to-things-like-audio-waveform-via-RNN-Why-or-why-not</a></li>
<li><a href="https://github.com/soumith/ganhacks" target="_blank" rel="external">Tips and tricks to make GANs work</a></li>
</ul>
<p>注：配图来自网络和参考文章</p>
<hr>
<p>以上就是本文的主要内容和总结，可以留言给出你对本文的建议和看法。</p>
<p>同时也欢迎关注我的微信公众号—机器学习与计算机视觉或者扫描下方的二维码，和我分享你的建议和看法，指正文章中可能存在的错误，大家一起交流，学习和进步！</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/qrcode_new.jpg" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<blockquote>
<p>本文大约 3800 字，阅读大约需要 8 分钟</p>
</blockquote>
<p>要说最近几年在深度学习领域最火的莫过于生成对抗网络，即 Generative Adversarial Networks(GANs)了。它是 Ian Goodf]]>
    </summary>
    
      <category term="GAN" scheme="http://ccc013.github.io/tags/GAN/"/>
    
      <category term="技术" scheme="http://ccc013.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="机器学习" scheme="http://ccc013.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://ccc013.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="http://ccc013.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[[资源分享] 吴恩达最新《机器学习训练秘籍》中文版可以免费下载了]]></title>
    <link href="http://ccc013.github.io/2018/10/27/%E8%B5%84%E6%BA%90%E5%88%86%E4%BA%AB-%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%80%E6%96%B0%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E7%A7%98%E7%B1%8D%E3%80%8B%E4%B8%AD%E6%96%87%E7%89%88%E5%8F%AF%E4%BB%A5%E5%85%8D%E8%B4%B9%E4%B8%8B%E8%BD%BD%E4%BA%86/"/>
    <id>http://ccc013.github.io/2018/10/27/资源分享-吴恩达最新《机器学习训练秘籍》中文版可以免费下载了/</id>
    <published>2018-10-27T09:50:11.000Z</published>
    <updated>2018-10-27T10:00:11.692Z</updated>
    <content type="html"><![CDATA[<blockquote>
<p>本文大约 600 字， 阅读大约需要 2 分钟</p>
</blockquote>
<p>吴恩达老师在上个月底宣布终于完成了他最新的书籍《Machine Learning Yearning》的最后几个章节：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/Ng_twitter1.png" alt=""></p>
<p>而最近这本书也有了免费的完整中文版下载了，中文版的名称是《机器学习训练秘籍》，封面如下：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/Ng_book1.png" alt=""></p>
<p>正如书名所言，这本书主要介绍的就是机器学习训练中的一些技巧和注意事项，包括如何设置训练集和测试集、处理偏差和方差问题等。目录如下所示：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/Ng_book3.png" alt=""></p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/Ng_book4.png" alt=""></p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/Ng_book8.png" alt=""></p>
<p>英文版的官方下载地址是：</p>
<p><a href="https://www.deeplearning.ai/machine-learning-yearning/" target="_blank" rel="external">https://www.deeplearning.ai/machine-learning-yearning/</a></p>
<p>其官网是：</p>
<p><a href="https://www.deeplearning.ai/" target="_blank" rel="external">https://www.deeplearning.ai/</a></p>
<p><strong>中文版的Github地址为</strong></p>
<p><a href="https://github.com/AcceptedDoge/machine-learning-yearning-cn" target="_blank" rel="external">https://github.com/AcceptedDoge/machine-learning-yearning-cn</a></p>
<p>不过目前的中文版的翻译还并非最终版本，如果发现了问题，可以去 Github 上提出，帮助修改。</p>
<p>这里我已经分别下载好中英文两个版本的 pdf，并且简单做好目录标签，下载链接是：</p>
<p><a href="https://pan.baidu.com/s/1D5y0-44pFh4KSMiviSNGRw" target="_blank" rel="external">https://pan.baidu.com/s/1D5y0-44pFh4KSMiviSNGRw</a></p>
<p>也可以在『公众号后台回复 <strong>“MLY” 或者 “机器学习训练秘籍”</strong>』获取下载链接和上述官网、中文版 Github 地址。</p>
<p>这里也非常感谢翻译并且免费分享中文版在 Github 上的几位大佬，也祝各位无论是刚入门还是在进阶机器学习方面知识和技能，都能从这本书中收获匪浅！最后如果你觉得这次分享的资源不错，可以点个赞或者分享出去，谢谢！</p>
<hr>
<p>欢迎关注我的微信公众号—机器学习与计算机视觉或者扫描下方的二维码，和我分享你的建议和看法，指正文章中可能存在的错误，大家一起交流，学习和进步！</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/qrcode_new.jpg" alt=""></p>
<p><strong>推荐阅读</strong><br>1.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483667&amp;idx=1&amp;sn=c6b6feb241897ede16bd745d595cef92&amp;chksm=fe3b0f66c94c86701e9b071e62750d189c254fd3ebe9bb6251505162139efefdf866093b38c3&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">机器学习入门系列(1)—机器学习概览(上)</a></p>
<p>2.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483672&amp;idx=1&amp;sn=34b6687030db92fd3e04dcdebd09fffc&amp;chksm=fe3b0f6dc94c867b2a72c427ebb90e2a683e6ad97ea2c5fbdc3a3bb86a8b159b8e5f107d2dcc&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">机器学习入门系列(2)—机器学习概览(下)</a></p>
<p>3.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483679&amp;idx=1&amp;sn=229eaae83f0fad327d4ae419dc6bf865&amp;chksm=fe3b0f6ac94c867cf72992dd2ec118d165c3990818ddd45d5a87736bac907b8871e8a006e9ab&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">[实战] 图片转素描图</a></p>
<p>4.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483711&amp;idx=1&amp;sn=ead88d5b21e08d9df853b72f31d4b5f4&amp;chksm=fe3b0f4ac94c865cfc243123eb4815539ef2d5babdc8346f79a29b681e55eee5f964bdc61d71&amp;token=1493836032&amp;lang=zh_CN#rd" target="_blank" rel="external">[GAN学习系列] 初识GAN</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<blockquote>
<p>本文大约 600 字， 阅读大约需要 2 分钟</p>
</blockquote>
<p>吴恩达老师在上个月底宣布终于完成了他最新的书籍《Machine Learning Yearning》的最后几个章节：</p>
<p><img src="htt]]>
    </summary>
    
      <category term="机器学习" scheme="http://ccc013.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="资源教程" scheme="http://ccc013.github.io/tags/%E8%B5%84%E6%BA%90%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[vim快速入门]]></title>
    <link href="http://ccc013.github.io/2018/10/19/vim%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"/>
    <id>http://ccc013.github.io/2018/10/19/vim快速入门/</id>
    <published>2018-10-19T13:53:10.000Z</published>
    <updated>2018-10-19T13:59:40.606Z</updated>
    <content type="html"><![CDATA[<blockquote>
<p>本文大约 5000 字， 阅读大约需要  10 分钟</p>
</blockquote>
<p>在 Linux 下最常使用的文本编辑器就是 vi 或者 vim 了，如果能很好掌握这个编辑器，非常有利于我们更好的在 Linux 下面进行编程开发。</p>
<h4 id="vim-和-vi"><a href="#vim-和-vi" class="headerlink" title="vim 和 vi"></a>vim 和 vi</h4><p>Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。</p>
<p>简单的来说， vi 是老式的字处理器，不过功能已经很齐全了，但是还是有可以进步的地方。 vim 则可以说是程序开发者的一项很好用的工具。</p>
<p>下面是 vim 快捷键盘图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1171928-dccc21832804abca.gif?imageMogr2/auto-orient/strip" alt="vim 快捷键"></p>
<h4 id="vi-vim-的工作模式"><a href="#vi-vim-的工作模式" class="headerlink" title="vi/vim 的工作模式"></a>vi/vim 的工作模式</h4><p>基本上 vi/vim 共分为三种模式，分别是命令模式（Command mode），输入模式（Insert mode）和底线命令模式（Last line mode）。 这三种模式的作用分别是：</p>
<h5 id="命令模式"><a href="#命令模式" class="headerlink" title="命令模式"></a>命令模式</h5><p>当使用 vi/vim 打开一个文件就进入了命令模式（也可称为一般模式），这是默认的模式。在这个模式，你可以采用『上下左右』按键来移动光标，你可以使用『删除字符』或『删除整行』来处理档案内容，也可以使用『复制、贴上』来处理你的文件数据。</p>
<h5 id="输入模式"><a href="#输入模式" class="headerlink" title="输入模式"></a>输入模式</h5><p>在命令模式并不能编辑文件，需要输入如『i, I, o, O, a, A, r,R』等任何一个字母之后才会进入输入模式（也称为编辑模式）。注意了！通常在 Linux 中，按下这些按键时，在画面的左下方会出现『 INSERT 或 REPLACE 』的字样，此时才可以进行编辑。而如果要回到命令模式时，则必须要按下『Esc』这个按键即可退出编辑模式。</p>
<h5 id="底线命令模式"><a href="#底线命令模式" class="headerlink" title="底线命令模式"></a>底线命令模式</h5><p>在命令模式下，按下『:,/,?』中任意一个，就可以将光标移动到最底下那一行，进入底线命令模式（也称为指令列命令模式）。在这个模式当中， 可以提供你『搜寻资料』的动作，而读取、存盘、大量取代字符、退出、显示行号等等的动作则是在此模式中达成的！同理，必须按下『Esc』按钮才可以退出该模式，返回命令模式</p>
<p>三种模式的切换和功能可以用下图来总结：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1171928-4a5e2f5c0d5f9ad2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="vi/vim 工作模式"></p>
<h4 id="简易示例"><a href="#简易示例" class="headerlink" title="简易示例"></a>简易示例</h4><h6 id="1-使用-vim-打开文件"><a href="#1-使用-vim-打开文件" class="headerlink" title="1. 使用 vim 打开文件"></a>1. 使用 vim 打开文件</h6><p>在命令行中输入如下命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim test.txt</span><br></pre></td></tr></table></figure>
<p>采用 <code>vi 文件名</code> 或者 <code>vim 文件名</code> 就可以打开文件并且进入了命令模式。这里文件名是必须添加的，当文件不存在的时候，也能打开，并且进行编辑保存后就是创建一个新的文件。打开后的界面如下图所示：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1171928-2d14cd1bed279c98.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="vim1.png"></p>
<p>整个界面可以分为两个部分，最底下一行和上面的部分，最底下一行主要是显示当前文件名和文件的行数、列数，上图是一个新的文件，所以最底下显示的是文件名，而且后面括号也说是新文件，而下图是一个已经有内容的文件，那么上面部分就显示文件内容，最底下一行显示了文件名，文件的行数和列数，并且在最右侧部分会显示当前坐标的位置，比如图中是显示 (4,1) 表示当前坐标在第四行第一列的位置。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1171928-3e4ea8bb56dc11c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="vim2.png"></p>
<h6 id="2-进入编辑模式"><a href="#2-进入编辑模式" class="headerlink" title="2. 进入编辑模式"></a>2. 进入编辑模式</h6><p>接下来就是开始对文件进行编辑，也就是需要进入编辑模式。只要按下『i, I, o, O, a, A, r,R』等字符就可以进入编辑模式了！在编辑模式当中，你可以发现在左下角状态栏中会出现 –INSERT- 的字样，那就是可以输入任意字符的提示啰！这个时候，键盘上除了 [Esc] 这个按键之外，其他的按键都可以视作为一般的输入按钮了，所以你可以进行任何的编辑！</p>
<p>如下图所示：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1171928-f0b360a2d0f97a00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="vim3.png"></p>
<p>注意：在 vim/vi 中 [Tab] 键是向右移动 8 个空格字符。</p>
<h6 id="3-按下-ESC-按钮回到命令模式"><a href="#3-按下-ESC-按钮回到命令模式" class="headerlink" title="3. 按下 [ESC] 按钮回到命令模式"></a>3. 按下 [ESC] 按钮回到命令模式</h6><p>如果对文件编辑完毕了，那么应该要如何退出呢？此时只需要按下 [Esc] 这个按钮即可！马上你就会发现画面左下角的 – INSERT – 不见了！并且返回了命令模式了</p>
<h6 id="4-退出"><a href="#4-退出" class="headerlink" title="4. 退出"></a>4. 退出</h6><p>最后就是存盘并离开，指令很简单，输入『:wq』即可存档离开！ (注意了，按下 : 该光标就会移动到最底下一行去！) ，如下图所示：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1171928-744511feb00c0e7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="vim4.png"></p>
<h4 id="更多按键说明"><a href="#更多按键说明" class="headerlink" title="更多按键说明"></a>更多按键说明</h4><p>上述简易示例只是使用了简单的几个按键，但是从 vim 快捷键图可以知道 vim 是有很多快捷键的。</p>
<p>vim 更多快捷键可以如下思维导图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/1171928-82328fac0d759f04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="vim 快捷键.png"></p>
<h4 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h4><p>题目是来自<a href="http://cn.linux.vbird.org/linux_basic/0310vi.php" target="_blank" rel="external">vim 程序编辑器</a>的练习，如下所示，使用的操作文件 man_db.conf 可以在 <a href="http://linux.vbird.org/linux_basic/0310vi/man_db.conf" target="_blank" rel="external">http://linux.vbird.org/linux_basic/0310vi/man_db.conf</a><br>处获取。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1. &#35531;&#22312; /tmp &#36889;&#20491;&#30446;&#37636;&#19979;&#24314;&#31435;&#19968;&#20491;&#21517;&#28858; vitest &#30340;&#30446;&#37636;&#65307;&#10;2. &#36914;&#20837; vitest &#36889;&#20491;&#30446;&#37636;&#30070;&#20013;&#65307;&#10;3. &#23559; /etc/man_db.conf &#35079;&#35069;&#21040;&#26412;&#30446;&#37636;&#24213;&#19979;(&#25110;&#30001;&#19978;&#36848;&#30340;&#36899;&#32080;&#19979;&#36617; man_db.conf &#27284;&#26696;)&#65307;&#10;4. &#20351;&#29992; vi &#38283;&#21855;&#26412;&#30446;&#37636;&#19979;&#30340; man_db.conf &#36889;&#20491;&#27284;&#26696;&#65307;&#10;5. &#22312; vi &#20013;&#35373;&#23450;&#19968;&#19979;&#34892;&#34399;&#65307;&#10;6. &#31227;&#21205;&#21040;&#31532; 43 &#21015;&#65292;&#21521;&#21491;&#31227;&#21205; 59 &#20491;&#23383;&#20803;&#65292;&#35531;&#21839;&#20320;&#30475;&#21040;&#30340;&#23567;&#25324;&#34399;&#20839;&#26159;&#21738;&#20491;&#25991;&#23383;&#65311;&#10;7. &#31227;&#21205;&#21040;&#31532;&#19968;&#21015;&#65292;&#20006;&#19988;&#21521;&#19979;&#25628;&#23563;&#19968;&#19979;&#12302; gzip &#12303;&#36889;&#20491;&#23383;&#20018;&#65292;&#35531;&#21839;&#20182;&#22312;&#31532;&#24190;&#21015;&#65311;&#10;8. &#25509;&#33879;&#19979;&#20358;&#65292;&#25105;&#35201;&#23559; 29 &#21040; 41 &#21015;&#20043;&#38291;&#30340;&#12302;&#23567;&#23531; man &#23383;&#20018;&#12303;&#25913;&#28858;&#12302;&#22823;&#23531; MAN &#23383;&#20018;&#12303;&#65292;&#20006;&#19988;&#19968;&#20491;&#19968;&#20491;&#25361;&#36984;&#26159;&#21542;&#38656;&#35201;&#20462;&#25913;&#65292;&#22914;&#20309;&#19979;&#36948;&#25351;&#20196;&#65311;&#22914;&#26524;&#22312;&#25361;&#36984;&#36942;&#31243;&#20013;&#19968;&#30452;&#25353;&#12302;y&#12303;&#65292; &#32080;&#26524;&#26371;&#22312;&#26368;&#24460;&#19968;&#21015;&#20986;&#29694;&#25913;&#35722;&#20102;&#24190;&#20491; man &#21602;&#65311;&#10;9. &#20462;&#25913;&#23436;&#20043;&#24460;&#65292;&#31361;&#28982;&#21453;&#24724;&#20102;&#65292;&#35201;&#20840;&#37096;&#24489;&#21407;&#65292;&#26377;&#21738;&#20123;&#26041;&#27861;&#65311;&#10;10. &#25105;&#35201;&#35079;&#35069; 66 &#21040; 71 &#36889; 6 &#21015;&#30340;&#20839;&#23481;(&#21547;&#26377;MANDB_MAP)&#65292;&#20006;&#19988;&#36028;&#21040;&#26368;&#24460;&#19968;&#21015;&#20043;&#24460;&#65307;&#10;11. 113 &#21040; 128 &#21015;&#20043;&#38291;&#30340;&#38283;&#38957;&#28858; # &#31526;&#34399;&#30340;&#35387;&#35299;&#36039;&#26009;&#25105;&#19981;&#35201;&#20102;&#65292;&#35201;&#22914;&#20309;&#21034;&#38500;&#65311;&#10;12. &#23559;&#36889;&#20491;&#27284;&#26696;&#21478;&#23384;&#25104;&#19968;&#20491; man.test.config &#30340;&#27284;&#21517;&#65307;&#10;13. &#21435;&#21040;&#31532; 25 &#21015;&#65292;&#20006;&#19988;&#21034;&#38500; 15 &#20491;&#23383;&#20803;&#65292;&#32080;&#26524;&#20986;&#29694;&#30340;&#31532;&#19968;&#20491;&#21934;&#23383;&#26159;&#20160;&#40636;&#65311;&#10;14. &#22312;&#31532;&#19968;&#21015;&#26032;&#22686;&#19968;&#21015;&#65292;&#35442;&#21015;&#20839;&#23481;&#36664;&#20837;&#12302;I am a student...&#12303;&#65307;&#10;15. &#20786;&#23384;&#24460;&#38626;&#38283;&#21543;&#65281;</span><br></pre></td></tr></table></figure>
<p>那么，整体步骤应该如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1. mkdir vitest&#10;2. cd vitest&#10;3. mv /etc/man_db.conf .&#10;4. vi man_db.conf&#10;5. :set nu&#10;6. 43G -&#62; 59l -&#62;&#25324;&#21495;&#20869;&#26159; as &#36825;&#20010;&#21333;&#35789;&#10;7. gg &#25110; 1G -&#62; /gzip -&#62; &#22312;&#31532; 93 &#21015;&#10;8. &#36755;&#20837;&#21629;&#20196; [:29,41s/man/MAN/gc] -&#62; &#28982;&#21518;&#19968;&#30452;&#28857;&#20987; y &#65292;&#24635;&#20849;&#38656;&#35201;&#26367;&#25442; 13 &#20010;&#10;9. &#19968;&#30452;&#25353; u &#38190;&#21363;&#21487;&#22797;&#21407;&#65307;&#26356;&#21152;&#31616;&#21333;&#31895;&#26292;&#30340;&#23601;&#26159;&#24378;&#21046;&#36864;&#20986;&#65292;&#20063;&#23601;&#26159;&#36755;&#20837; :q!&#10;10. 66G &#36339;&#21040; 66 &#34892; -&#62; 6yy &#22797;&#21046; 6 &#34892;&#20869;&#23481;(&#36755;&#20837;&#21518;&#65292;&#23631;&#24149;&#26368;&#21518;&#19968;&#34892;&#20250;&#26174;&#31034; 6 lines yanked) -&#62; G &#36339;&#21040;&#26368;&#21518;&#19968;&#34892;&#65292;&#36755;&#20837; p &#22797;&#21046;&#21040;&#26368;&#21518;&#19968;&#34892;&#30340;&#21518;&#38754;&#10;11. 113G &#36339;&#21040; 113 &#34892; -&#62; &#24635;&#20849;&#38656;&#35201;&#21024;&#38500; 16 &#34892;&#20869;&#23481;&#65292;&#25152;&#20197;&#36755;&#20837; 16dd&#65292;&#21024;&#38500;&#21518;&#20809;&#26631;&#25152;&#22312;&#34892;&#24320;&#22836;&#23601;&#26159; &#8216;#Flags&#8217;&#10;12. &#36755;&#20837; [:w man.test.config] &#23454;&#29616;&#20445;&#23384;&#25805;&#20316;&#65292;&#25509;&#30528;&#21487;&#20197;&#36755;&#20837; [:! ls -l]&#65292;&#21363;&#26174;&#31034;&#26597;&#30475;&#24403;&#21069;&#25991;&#20214;&#22841;&#20869;&#25991;&#20214;&#20869;&#23481;&#30340;&#21629;&#20196; ls -l &#26174;&#31034;&#30340;&#20869;&#23481;&#22312; vim &#20869;&#65292;&#20877;&#27425;&#25353;&#19979;&#22238;&#36710;&#38190;&#21363;&#22238;&#21040; vim &#21629;&#20196;&#27169;&#24335;&#10;13. &#36755;&#20837; 25G &#21040; 25 &#34892; -&#62; 15x &#21024;&#38500; 15 &#20010;&#23383;&#31526;&#65292;&#28982;&#21518;&#26174;&#31034;&#30340;&#26159; tree&#10;14. gg / 1G &#21040; &#31532;&#19968;&#34892; -&#62; O &#22312;&#19978;&#26041;&#26032;&#22686;&#19968;&#34892;&#65292;&#28982;&#21518;&#36755;&#20837; &#12302;I am a student...&#12303;-&#62; Esc &#38190;&#36820;&#22238;&#21629;&#20196;&#27169;&#24335;&#10;15. [:wq] &#25110;&#32773; ZZ &#20445;&#23384;&#31163;&#24320;&#25991;&#20214;</span><br></pre></td></tr></table></figure></p>
<p>本文参考文章如下：</p>
<ul>
<li>vim 程序编辑器(<a href="http://cn.linux.vbird.org/linux_basic/0310vi.php" target="_blank" rel="external">http://cn.linux.vbird.org/linux_basic/0310vi.php</a>)</li>
<li>Linux vi/vim(<a href="http://www.runoob.com/linux/linux-vim.html" target="_blank" rel="external">http://www.runoob.com/linux/linux-vim.html</a>)</li>
</ul>
<hr>
<p>以上就是本文的主要内容和总结，欢迎关注我的微信公众号—机器学习与计算机视觉或者扫描下方的二维码，和我分享你的建议和看法，指正文章中可能存在的错误，大家一起交流，学习和进步！</p>
<p>公众号后台回复“vim快捷键”可以获取 vim 思维导图！</p>
<p><img src="http://upload-images.jianshu.io/upload_images/1171928-51f7b495edfa7210.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p>
<p><strong>推荐阅读</strong><br>1.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483667&amp;idx=1&amp;sn=c6b6feb241897ede16bd745d595cef92&amp;chksm=fe3b0f66c94c86701e9b071e62750d189c254fd3ebe9bb6251505162139efefdf866093b38c3&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">机器学习入门系列(1)—机器学习概览(上)</a><br>2.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483672&amp;idx=1&amp;sn=34b6687030db92fd3e04dcdebd09fffc&amp;chksm=fe3b0f6dc94c867b2a72c427ebb90e2a683e6ad97ea2c5fbdc3a3bb86a8b159b8e5f107d2dcc&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">机器学习入门系列(2)—机器学习概览(下)</a><br>3.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483679&amp;idx=1&amp;sn=229eaae83f0fad327d4ae419dc6bf865&amp;chksm=fe3b0f6ac94c867cf72992dd2ec118d165c3990818ddd45d5a87736bac907b8871e8a006e9ab&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">[实战] 图片转素描图</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<blockquote>
<p>本文大约 5000 字， 阅读大约需要  10 分钟</p>
</blockquote>
<p>在 Linux 下最常使用的文本编辑器就是 vi 或者 vim 了，如果能很好掌握这个编辑器，非常有利于我们更好的在 Linux 下面进行编程开发。</]]>
    </summary>
    
      <category term="Linux" scheme="http://ccc013.github.io/tags/Linux/"/>
    
      <category term="vim" scheme="http://ccc013.github.io/tags/vim/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[[实战]制作简单的公众号二维码关注图]]></title>
    <link href="http://ccc013.github.io/2018/10/14/%E5%AE%9E%E6%88%98-%E5%88%B6%E4%BD%9C%E7%AE%80%E5%8D%95%E7%9A%84%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BA%8C%E7%BB%B4%E7%A0%81%E5%85%B3%E6%B3%A8%E5%9B%BE/"/>
    <id>http://ccc013.github.io/2018/10/14/实战-制作简单的公众号二维码关注图/</id>
    <published>2018-10-14T12:42:49.000Z</published>
    <updated>2018-10-14T13:52:21.066Z</updated>
    <content type="html"><![CDATA[<blockquote>
<p>本文大约 1933 字，阅读大约需要  6 分钟</p>
</blockquote>
<p>最近刚刚更换了公众号名字，然后自然就需要更换下文章末尾的二维码关注图，但是之前是通过 windows 自带的画图软件做的，但是之前弄的时候其实还是比较麻烦的，所以我就想作为一名程序猿，当然要努力用代码解决这个问题。</p>
<p>而且最近发现了一个新的图像处理方面的库—Wand，它是 ImageMagick 库的 Python 接口。于是，我就打算用这个库来实现简单的制作一个二维码关注图，主要是完成以下几个工作：</p>
<ol>
<li>制作一个白色的背景图；</li>
<li>将背景图和公众号二维码图合成；</li>
<li>添加文字得到最终的合成图</li>
</ol>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>Wand 是基于 ctypes 库的适用于 Python 的 ImageMagick 的封装库。</p>
<p>相比其他对 ImageMagick 的封装库，Wand 有以下几个优势：</p>
<ol>
<li>符合 Python 习惯和现代化的接口</li>
<li>有好的文档</li>
<li>通过 ctypes 进行封装</li>
<li>可以采用 pip 安装</li>
</ol>
<h3 id="安装教程"><a href="#安装教程" class="headerlink" title="安装教程"></a>安装教程</h3><p>在 ubuntu下，可以直接按照下列命令安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install libmagickwand-dev&#10;$ pip install Wand</span><br></pre></td></tr></table></figure>
<h4 id="安装要求"><a href="#安装要求" class="headerlink" title="安装要求"></a>安装要求</h4><p><strong>对 Python 版本要求：</strong></p>
<ul>
<li>Python 2.6+</li>
<li>CPython 2.6+</li>
<li>CPython 3.2+ or higher</li>
<li>PyPy 1.5+ or higher</li>
</ul>
<p><strong>MagickWand library</strong></p>
<ul>
<li>Debian/Ubuntu 系统：采用 apt-get 安装 libmagickwand-dev</li>
<li>Mac 系统：用 MacPorts/Homebrew 安装 imagemagick</li>
<li>CentOS 系统： 使用 yum 安装 ImageMagick-devel</li>
</ul>
<h4 id="Windows-注意事项"><a href="#Windows-注意事项" class="headerlink" title="Windows 注意事项"></a>Windows 注意事项</h4><p>主要还是参照第一篇文章来安装，并且主要是在 Windows 下安装，其中下载 ImageMagick 的时候，在<a href="http://www.imagemagick.org/download/binaries/" target="_blank" rel="external">下载地址</a>中需要选择 6.9版本的 dll 的 exe 执行文件安装，而不能选择最新版本的 7.0+，否则在 Python 中调用的时候，会出现问题<code>ImportError: MagickWand shared library not found.</code>，原因根据<a href="https://stackoverflow.com/questions/25003117/python-doesnt-find-magickwand-libraries-despite-correct-location" target="_blank" rel="external">Python doesn’t find MagickWand Libraries (despite correct location?)</a>中的说法是</p>
<blockquote>
<p>A few sources said that Image Magick 7.x is not compatible with magick Wand so make sure you’re using 6.x. Additionally, “static” suffix versions do not work. The one that finally worked for me was “ImageMagick-6.9.8-10-Q8-x64-dll.exe”</p>
</blockquote>
<p>也就是说  Image Magick 7.x 版本和 Wand 并不适配，所以只能采用 6+ 版本的。</p>
<h3 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h3><p>安装完成后，这里首先需要准备一张或者几张要合成的图片，比如作为背景的图片和前景图片，这里我是先给定大小来生成背景图片，而前景图片自然是我的公众号二维码图片了：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/qrcode_for_gh_207dddb4bd42_258.jpg" alt="公众号二维码"></p>
<p>首先是需要导入以下这些包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> wand.image <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> wand.drawing <span class="keyword">import</span> Drawing</span><br><span class="line"><span class="keyword">from</span> wand.color <span class="keyword">import</span> Color</span><br><span class="line"><span class="keyword">from</span> wand.display <span class="keyword">import</span> display</span><br></pre></td></tr></table></figure>
<h4 id="1-生成背景图片"><a href="#1-生成背景图片" class="headerlink" title="1. 生成背景图片"></a>1. 生成背景图片</h4><p>生成背景图片的代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 画一个纯白背景，并保存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_bg</span><span class="params">(self, width, height, filename=None, color=<span class="string">'white'</span>)</span>:</span></span><br><span class="line">    img = Image(width=width, height=height, background=Color(color))</span><br><span class="line">    <span class="keyword">if</span> filename <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        img.save(filename=filename)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure>
<p>这里就是根据传入的宽和高<code>width,height</code>，以及背景颜色<code>color</code>，生成指定大小和颜色的背景图片。</p>
<h4 id="2-将背景图和公众号二维码图合成"><a href="#2-将背景图和公众号二维码图合成" class="headerlink" title="2. 将背景图和公众号二维码图合成"></a>2. 将背景图和公众号二维码图合成</h4><p>接着就是合成图片的函数代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 合成图片</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">composite_with_image</span><span class="params">(self, img_back, img, left, top, save_name=None, is_display=False)</span>:</span></span><br><span class="line">  draw = Drawing()</span><br><span class="line">  draw.composite(operator=<span class="string">'atop'</span>,</span><br><span class="line">                left=left, top=top,</span><br><span class="line">                width=img.width,</span><br><span class="line">                height=img.height,</span><br><span class="line">                image=img)</span><br><span class="line">  draw(img_back)</span><br><span class="line">  <span class="keyword">if</span> is_display:</span><br><span class="line">  	display(img_back)</span><br><span class="line">  <span class="keyword">if</span> save_name <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">  	img_back.save(filename=save_name)</span><br><span class="line">  <span class="keyword">return</span> img_back</span><br><span class="line"><span class="comment"># 读取图片</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_image</span><span class="params">(self, image_name)</span>:</span></span><br><span class="line">  img = Image(filename=image_name)</span><br><span class="line">  print(<span class="string">'width='</span>, img.width)</span><br><span class="line">  print(<span class="string">'height='</span>, img.height)</span><br><span class="line">  print(<span class="string">'size='</span>, img.size)</span><br><span class="line">  <span class="keyword">return</span> img, img.width, img.height</span><br></pre></td></tr></table></figure>
<p>首先是用<code>read_image()</code>函数读取待合成的图片，然后利用<code>composite_with_image</code>函数来合成输入的两张图片，其中<code>img_back</code>表示背景图片，而<code>img</code>就是前景图片，<code>left, top</code>分别是前景图片在背景图片的左上角坐标位置。</p>
<p>这一步得到的结果如下所示，这里我设置的背景图片大小为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">image_name = <span class="string">'qrcode.jpg'</span></span><br><span class="line">qrcode_img, width, height = read_image(images_name)</span><br><span class="line">bg_width = int(width * <span class="number">2.5</span>)</span><br><span class="line">bg_height = int(height * <span class="number">1.1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/composition.jpg" alt="二维码和背景合成图"></p>
<h4 id="3-添加文字"><a href="#3-添加文字" class="headerlink" title="3. 添加文字"></a>3. 添加文字</h4><p>最后一步就是添加文字了，前面两步其实都非常简单，直接调用接口即可，但是添加文字的时候，却出现问题了。是什么问题呢？</p>
<p>首先先给出<code>wand</code>添加文字的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_text</span><span class="params">(self, image, x, y, text, font_size=<span class="number">15</span>, font_style=<span class="string">'normal'</span>, text_alignment=<span class="string">'left'</span>,text_color=<span class="string">'Black'</span>, filename=None, is_display=False)</span>:</span></span><br><span class="line">    draw = Drawing()</span><br><span class="line">    draw.fill_color = Color(text_color)</span><br><span class="line">    draw.font_size = font_size</span><br><span class="line">    draw.font_style = font_style</span><br><span class="line">    draw.text_alignment = text_alignment</span><br><span class="line">    draw.text(x, y, text)</span><br><span class="line">    draw(image)</span><br><span class="line">    <span class="keyword">if</span> is_display:</span><br><span class="line">        display(image)</span><br><span class="line">    <span class="keyword">if</span> filename <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        image.save(filename=filename)</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure>
<p>刚刚说的问题，其实也是 Python 很常见的问题，就是如果使用到中文的字符串的问题，本来我认为也是编码问题，但是我发现设置一个只包含英文字符串，和包含有中文字符串的结果是这样的：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/qrcode_composition1.jpg" alt=""></p>
<p>代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">text1 = <span class="string">'Hello world'</span></span><br><span class="line">text2 = <span class="string">'wechat:机器学习与计算机视觉'</span></span><br><span class="line"> x = int(width * <span class="number">1.5</span>) + <span class="number">50</span></span><br><span class="line"> margin = <span class="number">60</span></span><br><span class="line">y2 = int(bg_height // <span class="number">2</span>)</span><br><span class="line">y1 = y2 - margin</span><br><span class="line">x1 = x2 = x + <span class="number">20</span></span><br><span class="line"></span><br><span class="line">result1 = draw_text(composite_images, x1, y1, text1, font_size=<span class="number">20</span>, text_color=<span class="string">'Gray'</span>, 			 text_alignment=<span class="string">'center'</span>, filename=<span class="string">'qrcode_composition.jpg'</span>, is_display=<span class="keyword">False</span>)</span><br><span class="line">result2 = draw_text(result1, x2, y2, text2, font_size=<span class="number">30</span>, text_color=<span class="string">'Black'</span>,</span><br><span class="line">                      text_alignment=<span class="string">'center'</span>,filename=<span class="string">'qrcode_composition.jpg'</span>,</span><br><span class="line">                                             is_display=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>所以这应该不是编码问题，通过谷歌搜索后，发现应该是<code>wand</code>默认不支持中文字符的原因，接着在看到参考文章4后，我发现可以通过<code>wand.drawing.Drawing.font()</code>接口导入支持中文的字体来解决这个问题，而这些字体在哪里可以找到呢，其实在<code>c:\windows\fonts\</code>目录下面就可以找到了，宋体、微软雅黑的字体，只要指定字体路径即可，因此更新后的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">FONT_DICT = &#123;<span class="string">'宋体'</span>: <span class="string">'songti.ttc'</span>,</span><br><span class="line">             <span class="string">'微软雅黑1'</span>: <span class="string">'msyh.ttc'</span>,</span><br><span class="line">             <span class="string">'微软雅黑2'</span>: <span class="string">'msyhbd.ttc'</span>,</span><br><span class="line">             <span class="string">'微软雅黑3'</span>: <span class="string">'msyhl.ttc'</span>&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_text</span><span class="params">(self, image, x, y, text, font_size=<span class="number">15</span>, font_style=<span class="string">'normal'</span>,font=None,  text_alignment=<span class="string">'left'</span>,text_color=<span class="string">'Black'</span>, filename=None, is_display=False)</span>:</span></span><br><span class="line">    draw = Drawing()</span><br><span class="line"> 	<span class="keyword">if</span> font <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        draw.font = font</span><br><span class="line">    draw.fill_color = Color(text_color)</span><br><span class="line">    draw.font_size = font_size</span><br><span class="line">    draw.font_style = font_style</span><br><span class="line">    draw.text_alignment = text_alignment</span><br><span class="line">    draw.text(x, y, text)</span><br><span class="line">    draw(image)</span><br><span class="line">    <span class="keyword">if</span> is_display:</span><br><span class="line">        display(image)</span><br><span class="line">    <span class="keyword">if</span> filename <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        image.save(filename=filename)</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure>
<p>最终合成的结果如下：</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/qrcode_new.jpg" alt=""></p>
<p>完整代码可以到我的Github上查看—<a href="https://github.com/ccc013/CodingPractise/blob/master/Python/Fun_Project/image_composition/image_composition.py" target="_blank" rel="external">image_composition.py</a></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>这次的实战练习其实非常简单，唯一比较有困难的就是解决如何添加中文的文字了，但是还是非常实用的，熟练学会这个<code>Wand</code>后，就可以自己合成各种图片了，并且添加文字或者是其他图形等，具体可以查阅官方文档。</p>
<p>本文参考文章：</p>
<blockquote>
<ol>
<li><a href="http://docs.wand-py.org/en/0.4.2/guide/install.html" target="_blank" rel="external">Wand—Installtion</a></li>
<li><a href="http://www.imagemagick.org/script/index.php" target="_blank" rel="external">imagemagick home</a></li>
<li><a href="http://docs.wand-py.org/en/0.4.4/" target="_blank" rel="external">Wand Documentation</a></li>
<li><a href="http://touya.iteye.com/blog/251305" target="_blank" rel="external">用ImageMagick在图片中写中文的问题及解决</a></li>
<li><a href="https://stackoverflow.com/questions/30586447/python-wand-change-text-style-with-draw-text" target="_blank" rel="external">python-wand-change-text-style-with-draw-text</a></li>
</ol>
</blockquote>
<p>以上就是本文的主要内容和总结，欢迎留言给出你对本文的建议和看法。</p>
<p>同时也欢迎关注我的微信公众号—机器学习与计算机视觉或者扫描下方的二维码，和我分享你的建议和看法，指正文章中可能存在的错误，大家一起交流，学习和进步！</p>
<p><img src="https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/qrcode_new.jpg" alt=""></p>
<p><strong>推荐阅读</strong><br>1.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483667&amp;idx=1&amp;sn=c6b6feb241897ede16bd745d595cef92&amp;chksm=fe3b0f66c94c86701e9b071e62750d189c254fd3ebe9bb6251505162139efefdf866093b38c3&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">机器学习入门系列(1)—机器学习概览(上)</a><br>2.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483672&amp;idx=1&amp;sn=34b6687030db92fd3e04dcdebd09fffc&amp;chksm=fe3b0f6dc94c867b2a72c427ebb90e2a683e6ad97ea2c5fbdc3a3bb86a8b159b8e5f107d2dcc&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">机器学习入门系列(2)—机器学习概览(下)</a><br>3.<a href="https://mp.weixin.qq.com/s?__biz=MzU5MDY5OTI5MA==&amp;mid=2247483679&amp;idx=1&amp;sn=229eaae83f0fad327d4ae419dc6bf865&amp;chksm=fe3b0f6ac94c867cf72992dd2ec118d165c3990818ddd45d5a87736bac907b8871e8a006e9ab&amp;token=2134085567&amp;lang=zh_CN#rd" target="_blank" rel="external">[实战] 图片转素描图</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<blockquote>
<p>本文大约 1933 字，阅读大约需要  6 分钟</p>
</blockquote>
<p>最近刚刚更换了公众号名字，然后自然就需要更换下文章末尾的二维码关注图，但是之前是通过 windows 自带的画图软件做的，但是之前弄的时候其实还是比较麻烦的]]>
    </summary>
    
      <category term="Python - 实战 - 技术" scheme="http://ccc013.github.io/tags/Python-%E5%AE%9E%E6%88%98-%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[[实战]四行代码生成一张素描图片]]></title>
    <link href="http://ccc013.github.io/2018/09/09/%E5%AE%9E%E6%88%98-%E5%9B%9B%E8%A1%8C%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90%E4%B8%80%E5%BC%A0%E7%B4%A0%E6%8F%8F%E5%9B%BE%E7%89%87/"/>
    <id>http://ccc013.github.io/2018/09/09/实战-四行代码生成一张素描图片/</id>
    <published>2018-09-09T05:45:41.000Z</published>
    <updated>2018-09-09T06:28:01.748Z</updated>
    <content type="html"><![CDATA[<blockquote>
<p>本文大约 2000 字，阅读大约需要 6 分钟</p>
</blockquote>
<p>我们知道图片除了最普通的彩色图，还有很多类型，比如素描、卡通、黑白等等，今天就介绍如何使用 Python 和 Opencv 来实现图片变素描图。</p>
<p>主要参考这篇文章来实现—<a href="http://www.askaswiss.com/2016/01/how-to-create-pencil-sketch-opencv-python.html" target="_blank" rel="external">How to create a beautiful pencil sketch effect with OpenCV and Python</a></p>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>如何将图片转换成素描图呢，只需要下面四个步骤即可：</p>
<ol>
<li>首先将彩色图转换成灰度图；</li>
<li>对灰度图进行求其反色的操作；</li>
<li>对第2步得到的结果采用一个高斯模糊的操作；</li>
<li>采用颜色亮化(color dodge)的技术将第一步的灰度图和第三步操作后的图片进行混合。</li>
</ol>
<p>事先准备，首先是安装好 opencv，可以直接通过 pip 进行安装：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install opencv-python</span><br></pre></td></tr></table></figure>
<p>接着准备一张图片，最好是颜色鲜明一点的图片，方便对比转换的效果。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/example.jpg" alt="原图"></p>
<h4 id="第一步：彩色图变灰度图"><a href="#第一步：彩色图变灰度图" class="headerlink" title="第一步：彩色图变灰度图"></a>第一步：彩色图变灰度图</h4><p>第一步变成灰度图，其实非常简单，直接调用 opencv 的函数即可，如下面代码所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img_rgb = cv2.imread(<span class="string">'example.jpg'</span>)</span><br><span class="line">img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)</span><br></pre></td></tr></table></figure>
<p>图片转换效果如下所示：</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/rgb2sketch_example1.png" alt="图片转灰度图"></p>
<p>上面的代码是读取图片后，再通过调用<code>cv2.cvtColor</code>函数将图片转换成灰度图，实际上我们可以直接在读取图片时候就直接转换图片，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img_gray = cv2.imread(<span class="string">'example.jpg'</span>, cv2.IMREAD_GRAYSCALE)</span><br></pre></td></tr></table></figure>
<p>这里调用<code>cv2.imread</code>函数时，设置了<code>cv2.IMREAD_GRAYSCALE</code>的标志，表示加载灰度图。在<code>imread</code>函数中是设置了三种标志，分别是</p>
<ul>
<li>cv2.IMREAD_COLOR : 默认使用该种标识。加载一张彩色图片，忽视它的透明度。</li>
<li>cv2.IMREAD_GRAYSCALE : 加载一张灰度图。</li>
<li>cv2.IMREAD_UNCHANGED : 加载图像，包括它的Alpha 通道(Alpha 表示图片的透明度)。   </li>
</ul>
<p>另外，如果觉得以上标志太长，可以简单使用 1，0，-1 代替，效果是相同的。</p>
<h4 id="第二步：灰度图进行反色操作"><a href="#第二步：灰度图进行反色操作" class="headerlink" title="第二步：灰度图进行反色操作"></a>第二步：灰度图进行反色操作</h4><p>第二步就是对灰度图进行反色操作，其实就是非常简单的采用灰度图的最大像素值 255 减去当前像素值即可（因为灰度图的范围是[0, 255]），代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img_gray_inv = <span class="number">255</span> - img_gray</span><br></pre></td></tr></table></figure>
<p>结果如下所示：</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/rgb2sketch_example2.png" alt="灰度图反色"></p>
<p>其实就是原本比较暗的地方变光亮了，而比较亮的地方变暗了。</p>
<h4 id="第三步：高斯模糊"><a href="#第三步：高斯模糊" class="headerlink" title="第三步：高斯模糊"></a>第三步：高斯模糊</h4><p>高斯模糊操作是一个有效减少图片噪音以及对图片进行平滑操作的方法，在数学上等价于对图像采用高斯核进行卷积的操作。我们可以直接调用<code>cv2.GaussianBlur</code>来实现高斯模糊操作，这里需要设置参数<code>ksize</code>，表示高斯核的大小，<code>sigmaX</code>和<code>sigmaY</code>分别表示高斯核在 X 和 Y 方向上的标准差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">img_blur = cv2.GaussianBlur(img_gray_inv, ksize=(<span class="number">21</span>, <span class="number">21</span>),</span><br><span class="line">                            sigmaX=<span class="number">0</span>, sigmaY=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>效果如下所示，右边图是进行高斯模糊后的结果，是有了一定的模糊效果。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/rgb2sketch_example3.png" alt="高斯模糊"></p>
<h4 id="第四步：混合操作"><a href="#第四步：混合操作" class="headerlink" title="第四步：混合操作"></a>第四步：混合操作</h4><p>第四步，就是见证奇迹的时刻！这一步骤自然就是需要得到最终的素描图结果了。在传统照相技术中，当需要对图片某个区域变得更亮或者变暗，可以通过控制它的曝光时间，这里就用到亮化(Dodging)和暗化(burning)的技术。</p>
<p>在现代图像编辑工具，比如 PS 可以实现上述说的两种技术。比如对于颜色亮化技术，给定一张图片 A 和 蒙版 B，那么实现做法如下所示：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(B[idx] == 255)?B[idx]:min(255, ((A[idx] &lt;&lt; 8) / (255-B[idx])))</span><br></pre></td></tr></table></figure>
<p>通过 python 代码实现上述公式，那么原始代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dodgeNaive</span><span class="params">(image, mask)</span>:</span></span><br><span class="line">    <span class="comment"># determine the shape of the input image</span></span><br><span class="line">    width, height = image.shape[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prepare output argument with same size as image</span></span><br><span class="line">    blend = np.zeros((width, height), np.uint8)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> range(width):</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> range(height):</span><br><span class="line">            <span class="comment"># do for every pixel</span></span><br><span class="line">            <span class="keyword">if</span> mask[col, row] == <span class="number">255</span>:</span><br><span class="line">                <span class="comment"># avoid division by zero</span></span><br><span class="line">                blend[col, row] = <span class="number">255</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># shift image pixel value by 8 bits</span></span><br><span class="line">                <span class="comment"># divide by the inverse of the mask</span></span><br><span class="line">                tmp = (image[col, row] &lt;&lt; <span class="number">8</span>) / (<span class="number">255</span> - mask)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># make sure resulting value stays within bounds</span></span><br><span class="line">                <span class="keyword">if</span> tmp &gt; <span class="number">255</span>:</span><br><span class="line">                    tmp = <span class="number">255</span></span><br><span class="line">                    blend[col, row] = tmp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> blend</span><br></pre></td></tr></table></figure>
<p>上述代码虽然实现了这个功能，但是很明显会非常耗时，中间采用了一个两层循环，计算复杂度是 O(w*h) ，也就是如果图片的宽和高的乘积越大，耗时就越长，所以就有了升级版的代码版本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dodgeV2</span><span class="params">(image, mask)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> cv2.divide(image, <span class="number">255</span> - mask, scale=<span class="number">256</span>)</span><br></pre></td></tr></table></figure>
<p>运行上述代码，得到的最终结果如下所示：</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/rgb2sketch_example4.png" alt="素描图"></p>
<p>效果看起来还可以，除了右下角部分对于原图中黑色区域处理得不是很好。</p>
<p>而另一种技术—-暗化操作的代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">burnV2</span><span class="params">(image, mask)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">255</span> - cv2.divide(<span class="number">255</span> - image, <span class="number">255</span> - mask, scale=<span class="number">256</span>)</span><br></pre></td></tr></table></figure>
<p>效果如下图所示：</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/rgb2sketch_example5.png" alt="burning"></p>
<p>完整版代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dodgeNaive</span><span class="params">(image, mask)</span>:</span></span><br><span class="line">    <span class="comment"># determine the shape of the input image</span></span><br><span class="line">    width, height = image.shape[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prepare output argument with same size as image</span></span><br><span class="line">    blend = np.zeros((width, height), np.uint8)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> range(width):</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> range(height):</span><br><span class="line">            <span class="comment"># do for every pixel</span></span><br><span class="line">            <span class="keyword">if</span> mask[col, row] == <span class="number">255</span>:</span><br><span class="line">                <span class="comment"># avoid division by zero</span></span><br><span class="line">                blend[col, row] = <span class="number">255</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># shift image pixel value by 8 bits</span></span><br><span class="line">                <span class="comment"># divide by the inverse of the mask</span></span><br><span class="line">                tmp = (image[col, row] &lt;&lt; <span class="number">8</span>) / (<span class="number">255</span> - mask)</span><br><span class="line">                <span class="comment"># print('tmp=&#123;&#125;'.format(tmp.shape))</span></span><br><span class="line">                <span class="comment"># make sure resulting value stays within bounds</span></span><br><span class="line">                <span class="keyword">if</span> tmp.any() &gt; <span class="number">255</span>:</span><br><span class="line">                    tmp = <span class="number">255</span></span><br><span class="line">                    blend[col, row] = tmp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> blend</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dodgeV2</span><span class="params">(image, mask)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> cv2.divide(image, <span class="number">255</span> - mask, scale=<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">burnV2</span><span class="params">(image, mask)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">255</span> - cv2.divide(<span class="number">255</span> - image, <span class="number">255</span> - mask, scale=<span class="number">256</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rgb_to_sketch</span><span class="params">(src_image_name, dst_image_name)</span>:</span></span><br><span class="line">    img_rgb = cv2.imread(src_image_name)</span><br><span class="line">    img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)</span><br><span class="line">    <span class="comment"># 读取图片时直接转换操作</span></span><br><span class="line">    <span class="comment"># img_gray = cv2.imread('example.jpg', cv2.IMREAD_GRAYSCALE)</span></span><br><span class="line"></span><br><span class="line">    img_gray_inv = <span class="number">255</span> - img_gray</span><br><span class="line">    img_blur = cv2.GaussianBlur(img_gray_inv, ksize=(<span class="number">21</span>, <span class="number">21</span>),</span><br><span class="line">                                sigmaX=<span class="number">0</span>, sigmaY=<span class="number">0</span>)</span><br><span class="line">    img_blend = dodgeV2(img_gray, img_blur)</span><br><span class="line"></span><br><span class="line">    cv2.imshow(<span class="string">'original'</span>, img_rgb)</span><br><span class="line">    cv2.imshow(<span class="string">'gray'</span>, img_gray)</span><br><span class="line">    cv2.imshow(<span class="string">'gray_inv'</span>, img_gray_inv)</span><br><span class="line">    cv2.imshow(<span class="string">'gray_blur'</span>, img_blur)</span><br><span class="line">    cv2.imshow(<span class="string">"pencil sketch"</span>, img_blend)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line">    cv2.imwrite(dst_image_name, img_blend)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    src_image_name = <span class="string">'example.jpg'</span></span><br><span class="line">    dst_image_name = <span class="string">'sketch_example.jpg'</span></span><br><span class="line">    rgb_to_sketch(src_image_name, dst_image_name)</span><br></pre></td></tr></table></figure>
<p>最后，还有一种更加快速的实现，代码如下所示，仅需四行代码即可实现转换成素描图的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rgb_to_sketch_v2</span><span class="params">(src_image_name)</span>:</span></span><br><span class="line">    img_gray = cv2.imread(src_image_name, <span class="number">0</span>)</span><br><span class="line">    img_blur = cv2.GaussianBlur(img_gray, (<span class="number">21</span>, <span class="number">21</span>), <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    img_blend = cv2.divide(img_gray, img_blur, scale=<span class="number">256</span>)</span><br><span class="line">    img_result = cv2.cvtColor(img_blend, cv2.COLOR_GRAY2BGR)</span><br></pre></td></tr></table></figure>
<p>最后用本人比较喜欢的一个女演员的照片来看看这个转换的效果：</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/rgb2sketch_example6.png" alt=""></p>
<p>效果还是挺不错的！</p>
<p>以上就是本文的主要内容和总结，欢迎关注我的微信公众号—一个算法汪的技术成长之路或者扫描下方的二维码，和我分享你的建议和看法，指正文章中可能存在的错误，大家一起交流，学习和进步！</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/FqsJIpKGsF3-Qbu7bfQjzG-l0Ro9" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<blockquote>
<p>本文大约 2000 字，阅读大约需要 6 分钟</p>
</blockquote>
<p>我们知道图片除了最普通的彩色图，还有很多类型，比如素描、卡通、黑白等等，今天就介绍如何使用 Python 和 Opencv 来实现图片变素描图。</p>
<p]]>
    </summary>
    
      <category term="Python" scheme="http://ccc013.github.io/tags/Python/"/>
    
      <category term="实战" scheme="http://ccc013.github.io/tags/%E5%AE%9E%E6%88%98/"/>
    
      <category term="技术" scheme="http://ccc013.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习入门系列(2)--机器学习概览(下)]]></title>
    <link href="http://ccc013.github.io/2018/09/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97-2-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88-%E4%B8%8B/"/>
    <id>http://ccc013.github.io/2018/09/02/机器学习入门系列-2-机器学习概览-下/</id>
    <published>2018-09-02T03:04:09.000Z</published>
    <updated>2018-09-02T04:15:50.457Z</updated>
    <content type="html"><![CDATA[<p>这是本系列的第二篇，也是机器学习概览的下半部分。</p>
<h3 id="1-机器学习的主要挑战"><a href="#1-机器学习的主要挑战" class="headerlink" title="1. 机器学习的主要挑战"></a>1. 机器学习的主要挑战</h3><p>在介绍基于模型学习算法的流程的时候，对于预测结果不好的问题分析，主要说了是数据问题还是模型问题，这同时也就是机器学习的效果不好的两个主要原因，即错误的数据和错误的算法。</p>
<h4 id="1-1-训练数据量不足"><a href="#1-1-训练数据量不足" class="headerlink" title="1.1 训练数据量不足"></a>1.1 训练数据量不足</h4><p>第一个问题就是训练数据的数量问题，这是非常重要的问题。</p>
<p>因为即使是简单的问题，一般也需要数千的样本，这还是因为简单的问题一般采用简单的算法就可以解决，对于复杂的图像或语音问题，通常需要数百万的样本，特别是如果采用现在非常热门的深度学习算法，比如卷积神经网络模型，这些复杂的模型如果没有足够的数据量支持，非常容易陷入过拟合的情况。</p>
<p>实际上更多数量的训练集也是为了获得更有代表性的数据，能够学习到这类数据的所有特征。</p>
<p>但是，应该注意到，小型和中型的数据集仍然是非常常见的，获得额外的训练数据并不总是轻易和廉价的，所以不要抛弃算法。</p>
<h4 id="1-2-没有代表性的训练数据"><a href="#1-2-没有代表性的训练数据" class="headerlink" title="1.2 没有代表性的训练数据"></a>1.2 没有代表性的训练数据</h4><p>无论采用基于实例还是基于模型的学习，让训练数据对新数据具有代表性是非常重要的。如果训练集没有代表性，那么训练得到的模型就是不可能得到准确性的模型，比如人脸识别中，模型没有学习到某个人最明显的代表性的特征，比如高鼻梁或者没有眉毛等突出特征，那么模型对这个人的识别率就不会很高。</p>
<p>使用具有代表性的训练集对于推广到新案例是非常重要的。但是做起来比说起来要难：如果样本太小，就会有样本噪声（即会有一定概率包含没有代表性的数据），但是即使是非常大的样本也可能没有代表性，如果取样方法错误的话。这叫做样本偏差。</p>
<h4 id="1-3-低质量的数据"><a href="#1-3-低质量的数据" class="headerlink" title="1.3 低质量的数据"></a>1.3 低质量的数据</h4><p>低质量的数据指的是数据有错误、带有过多噪声或者是出现异常值等的数据，这种数据会影响系统整体的性能，因此，数据清洗对于构建一个机器学习系统或者一个机器学习项目来说都是必不可少的步骤。</p>
<p>对于这些低质量的数据，通常可以按照如下做法处理：</p>
<ul>
<li>如果一些实例是明显的异常值，最好删掉它们或尝试手工修改错误；</li>
<li>如果一些实例缺少特征（比如，你的 5% 的顾客没有说明年龄），你必须决定是否忽略这个属性、忽略这些实例、填入缺失值（比如，年龄中位数），或者训练一个含有这个特征的模型和一个不含有这个特征的模型，等等。</li>
</ul>
<h4 id="1-4-不相关的特征"><a href="#1-4-不相关的特征" class="headerlink" title="1.4 不相关的特征"></a>1.4 不相关的特征</h4><p>不相关的特征对于整个机器学习系统是有着反作用的效果，训练数据必须包含足够多的相关特征、非相关特征不多的情况下，才能训练出一个性能不错的模型。机器学习项目成功的关键之一是用好的特征进行训练。这个过程称作<strong>特征工程</strong>，包括：</p>
<ul>
<li>特征选择：在所有存在的特征中选取最有用的特征进行训练。</li>
<li>特征提取：组合存在的特征，生成一个更有用的特征（如前面看到的，可以使用降维算法）。</li>
<li>收集新数据创建新特征。</li>
</ul>
<h4 id="1-5-过拟合"><a href="#1-5-过拟合" class="headerlink" title="1.5 过拟合"></a>1.5 过拟合</h4><p>上述四种情况都是坏数据的情况，接下来是两种算法问题，也是机器学习最常见的两种算法方面的问题，过拟合和欠拟合。</p>
<p>过拟合就是指算法模型在训练集上的性能非常好，但是泛化能力很差，即在测试集上的效果却很糟糕的情况。比如下图，采用一个高阶多项式回归模型来预测生活满意度和人均 GDP 的关系，很明显看出来，这个模型过拟合了训练数据，其预测效果并不会达到在训练数据上这么好的效果。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E8%BF%87%E6%8B%9F%E5%90%88%E7%A4%BA%E4%BE%8B.png" alt="过拟合示例"></p>
<p>通常对于比较复杂的模型，比如深度神经网络，它能够检测和识别到数据中比较细微的规律和特征，但是如果训练集包含噪声，或者训练集数量太少（数量太少会引入样本噪声），这种情况下，模型同样会学习这种噪声，从而导致模型的泛化能力的下降。</p>
<p>一般解决过拟合的方法有：</p>
<ul>
<li>简化模型，这包括了采用简单点的模型、减少特征数量以及限制模型，即采用正则化；</li>
<li>增加训练数据</li>
<li>减小训练数据的噪声，即数据清洗，比如修正数据错误和去除异常值等</li>
</ul>
<p>其中正则化方法是比较常用的方法，它的作用就是限制模型，不让模型过于复杂，从而降低过拟合的风险或者是缓和过拟合的程度。常用的正则化方法是 L2 和 L1 正则化。正则化方法通常会采用一个超参数来控制其限制模型的强度。超参数是一个学习算法的参数（而不是模型的）。这样，它是不会被学习算法本身影响的，它优于训练，在训练中是保持不变的。如何调节超参数也是构建一个机器学习算法模型非常重要的一个步骤，也是让性能能够进一步提升的做法。</p>
<h4 id="1-6-欠拟合"><a href="#1-6-欠拟合" class="headerlink" title="1.6 欠拟合"></a>1.6 欠拟合</h4><p>欠拟合和过拟合刚好相反，它就是模型的性能非常差，在训练数据和测试数据上的性能都不好。</p>
<p>通常也是因为模型过于简单，没有能够很好学习到数据的有效的相关的特征，解决方法有：</p>
<ul>
<li>选择一个更强大的模型，带有更多参数</li>
<li>用更好的特征训练学习算法（特征工程）</li>
<li>减小对模型的限制（比如，减小正则化超参数）</li>
</ul>
<h3 id="2-测试和评估"><a href="#2-测试和评估" class="headerlink" title="2. 测试和评估"></a>2. 测试和评估</h3><p>当训练好一个机器学习模型后，接下来就需要对模型进行预测和评估，判断得到的模型是否可用，是否还能进行提升，并进行错误分析等操作。</p>
<p>一般在训练模型前，我们会将数据集分成两个集合，分别是训练集和测试集，通常 8:2 的比例，也就是 80% 的数据作为训练集，剩余是测试集。然后采用训练集训练模型，在测试集上用按照学习的问题采用对应评估指标评估模型的性能，比如分类问题，一般就是采用分类的准确率或者错误率作为评估的标准。</p>
<p>但这种划分数据集的方法，存在一个问题，就是如果需要调节超参数，比如对于正则化的超参数、学习率等，继续采用测试集来进行评估不同超参数对模型性能的影响，这会导致最后在测试集上测试得到性能最好的模型，实际上是过拟合了测试集，那么模型的泛化能力也不会太好。</p>
<p>所以，为了解决这个问题，我们还需要为调节超参数划分一个专门的数据集，测试集应该是用于测试最终得到的模型的性能。因此，我们再划分一个叫做验证集的数据集。</p>
<p>一种做法是可以将所有数据按照一定比例划分为训练集、验证集和测试集，比如按照 6:2:2 的比例划分；当然更通常的做法是采用交叉验证：训练集分成互补的子集，每个模型用不同的子集训练，再用剩下的子集验证。一旦确定模型类型和超参数，最终的模型使用这些超参数和全部的训练集进行训练，用测试集得到推广误差率。</p>
<h3 id="3-小结"><a href="#3-小结" class="headerlink" title="3. 小结"></a>3. 小结</h3><p>最后我们总结下：</p>
<ol>
<li><p>机器学习的四个主要挑战是</p>
<ul>
<li>数据量太少</li>
<li>数据问题，包括没有代表性数据和质量差</li>
<li>不相关特征</li>
<li>模型过拟合或者欠拟合</li>
</ul>
</li>
<li>过拟合的解法方法有：<ul>
<li>简化模型，包括采用更简单的模型和更少的参数</li>
<li>正则化方法降低模型的复杂度</li>
<li>收集或者采用更大的数据集</li>
<li>数据清洗，去除噪声和异常值等</li>
</ul>
</li>
<li>欠拟合的解决方法：<ul>
<li>采用更强大的模型，包含更多的参数和学习能力</li>
<li>降低正则化的强度</li>
<li>使用更好的特征提取方法，即使用或者改善特征工程的工作</li>
</ul>
</li>
<li>采用交叉验证方法进行超参数条件和模型的选择</li>
</ol>
<p>以上就是本文的主要内容和总结，欢迎关注我的微信公众号—一个算法汪的技术成长之路或者扫描下方的二维码，和我分享你的建议和看法，指正文章中可能存在的错误，大家一起交流，学习和进步！</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/FqsJIpKGsF3-Qbu7bfQjzG-l0Ro9" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这是本系列的第二篇，也是机器学习概览的下半部分。</p>
<h3 id="1-机器学习的主要挑战"><a href="#1-机器学习的主要挑战" class="headerlink" title="1. 机器学习的主要挑战"></a>1. 机器学习的主要挑战</h3><p>]]>
    </summary>
    
      <category term="技术" scheme="http://ccc013.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="机器学习" scheme="http://ccc013.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="http://ccc013.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习入门系列(1)--机器学习概览(上)]]></title>
    <link href="http://ccc013.github.io/2018/09/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97-1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88-%E4%B8%8A/"/>
    <id>http://ccc013.github.io/2018/09/02/机器学习入门系列-1-机器学习概览-上/</id>
    <published>2018-09-02T02:32:41.000Z</published>
    <updated>2018-09-02T04:26:29.513Z</updated>
    <content type="html"><![CDATA[<p>最近打算系统学习和整理机器学习方面的知识，会将之前看的 Andrew Ng 在 course 课程笔记以及最近看的书籍《hands-on-ml-with-sklearn-and-tf》结合起来，简单总结下机器学习的常用算法，由于数学功底有限，所以可能不会也暂时不能过多深入公式和算法原理，所以就做成一个入门系列吧。</p>
<p>这是本系列的第一篇，也是机器学习概览的上半部分。</p>
<h3 id="1-什么是机器学习"><a href="#1-什么是机器学习" class="headerlink" title="1 . 什么是机器学习"></a>1 . 什么是机器学习</h3><p>简单的定义，<strong>机器学习是通过编程让计算机从数据中进行学习的科学（和艺术）</strong>。</p>
<p>但还有另外两种定义，一个更广义的定义：</p>
<blockquote>
<p>机器学习是让计算机具有学习的能力，无需进行明确编程。    ——    亚瑟·萨缪尔，1959</p>
</blockquote>
<p>和一个工程性的定义：</p>
<blockquote>
<p>计算机程序利用经验 E 学习任务    T，性能是 P，如果针对任务 T 的性能 P 随着经验 E 不断增长，则称为机器学习。    ——    汤姆·米切尔，1997</p>
</blockquote>
<p>一个简单的例子，也是经常提及的例子：垃圾邮件过滤器。它可以根据垃圾邮件（比如，用户标记的垃圾邮件）和普通邮件（非垃圾邮件，也称作 ham）学习标记垃圾邮件。用来进行学习的样例称作训练集。每个训练样例称作训练实例（或样本）。在这个例子中，任务 T 就是标记新邮件是否是垃圾邮件，经验E是训练数据，性能 P 需要定义：例如，可以使用正确分类的比例。这个性能指标称为准确率，通常用在分类任务中。</p>
<h3 id="2-为什么要用机器学习"><a href="#2-为什么要用机器学习" class="headerlink" title="2. 为什么要用机器学习"></a>2. 为什么要用机器学习</h3><p>为什么要用机器学习方法呢？</p>
<p>原因如下：</p>
<ul>
<li>需要进行大量手工调整或需要拥有长串规则才能解决的问题：机器学习算法通常可以<strong>简化代码、提高性能</strong>。</li>
<li>问题复杂，传统方法难以解决：最好的机器学习方法可以找到解决方案。</li>
<li>环境有波动：机器学习算法可以<strong>适应新数据</strong>。</li>
<li>洞察复杂问题和大量数据</li>
</ul>
<p>一些机器学习的应用例子：</p>
<ul>
<li>数据挖掘</li>
<li>一些无法通过手动编程来编写的应用：如自然语言处理，计算机视觉、语音识别等</li>
<li>一些自助式的程序：如推荐系统等</li>
<li>理解人类是如何学习的</li>
</ul>
<h3 id="3-机器学习系统的类型"><a href="#3-机器学习系统的类型" class="headerlink" title="3. 机器学习系统的类型"></a>3. 机器学习系统的类型</h3><p>机器学习有多种类型，可以根据如下规则进行分类：</p>
<ul>
<li>是否在人类监督下进行训练（监督，非监督，半监督和强化学习）<ul>
<li>是否可以动态渐进学习（在线学习 vs批量学习）</li>
<li>它们是否只是通过简单地比较新的数据点和已知的数据点，或者在训练数据中进行模式识别，以建立一个预测模型，就像科学家所做的那样（基于实例学习 vs基于模型学习）</li>
</ul>
</li>
</ul>
<h4 id="3-1-监督-非监督学习"><a href="#3-1-监督-非监督学习" class="headerlink" title="3.1 监督/非监督学习"></a>3.1 监督/非监督学习</h4><p>第一种分类机器学习的方法是可以根据训练时监督的量和类型进行分类。主要有四类：监督学习、非监督学习、半监督学习和强化学习。</p>
<h5 id="3-1-1-监督学习"><a href="#3-1-1-监督学习" class="headerlink" title="3.1.1 监督学习"></a>3.1.1 监督学习</h5><p>监督学习，顾名思义就是带有监督的学习，而监督就是体现在训练数据都是有标签的，所有在训练模型的时候可以根据数据的真实标签不断调整模型，从而得到一个性能更好的模型。</p>
<p>监督学习主要有两个常见的典型的任务—分类和回归。</p>
<h6 id="3-1-1-1-分类"><a href="#3-1-1-1-分类" class="headerlink" title="3.1.1.1 分类"></a>3.1.1.1 分类</h6><p>分类问题主要就是预测新数据的类别问题。例如上文提到的垃圾邮件过滤器就是一个二分类问题，将邮件分为垃圾邮件还是正常的邮件，如下图所示。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%88%86%E7%B1%BB%E7%A4%BA%E4%BE%8B.png" alt=""></p>
<h6 id="3-1-1-2-回归"><a href="#3-1-1-2-回归" class="headerlink" title="3.1.1.2 回归"></a>3.1.1.2 回归</h6><p>回归问题主要是预测目标数值。比如给定预测房价的问题，给定一些特征，如房子大小、房间数量、地理位置等等，然后预测房子的价格。如下图所示：</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%9B%9E%E5%BD%92%E7%A4%BA%E4%BE%8B.png" alt=""></p>
<p>注意，一些回归算法也可以用来进行分类，反之亦然。例如，逻辑回归通常用来进行分类，它可以生成一属于每个类别的概率值，然后选择最大概率的类别作为预测的类别。</p>
<p>常用的监督学习算法有：</p>
<ul>
<li>K近邻算法</li>
<li>线性回归</li>
<li>逻辑回归</li>
<li>支持向量机（SVM）</li>
<li>决策树和随机森林</li>
<li>神经网络</li>
</ul>
<h5 id="3-1-2-非监督学习"><a href="#3-1-2-非监督学习" class="headerlink" title="3.1.2 非监督学习"></a>3.1.2 非监督学习</h5><p>和监督学习相反，非监督学习就是采用没有标签的数据集。</p>
<p>非监督主要有四个典型的任务，分别是聚类、降维、异常检测和关联规则学习。</p>
<h6 id="3-1-2-1-聚类"><a href="#3-1-2-1-聚类" class="headerlink" title="3.1.2.1. 聚类"></a>3.1.2.1. 聚类</h6><p>聚类就是将数据根据一定的规则分成多个类，通常是采用相似性。比如对于博客访客的聚类，通过聚类算法，检测相似性访客的分组，如下图所示。不需要告诉算法访客是哪个类别，它会自动根据访客的属性找到相互间的关系，比如它可能找出访客的职业关系，将访客分为有 40% 的是上班族，有 50% 的是学生，或者对于技术博客，可能就是根据开发方向，划分为前端、后台、移动开发、人工智能等等。甚至，如果采用层次聚类分析算法，还可以继续对上述的分类进行更加详细的划分。这种做法可以帮助博主知道自己博客的主要群体是谁，更好规划自己博客发表的文章应该以什么方向为主。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E8%81%9A%E7%B1%BB%E7%A4%BA%E4%BE%8B.png" alt=""></p>
<p>可视化算法也是极佳的非监督学习案例：<strong>给算法大量复杂的且不加标签的数据，算法输出数据的2D或3D图像</strong>。如下图所示，算法会试图保留数据的结构（即尝试保留输入的独立聚类，避免在图像中重叠），这样就可以明白数据是如何组织起来的，也许还能发现隐藏的规律。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%8F%AF%E8%A7%86%E5%8C%96%E7%AE%97%E6%B3%95.png" alt=""></p>
<h6 id="3-1-2-2-降维"><a href="#3-1-2-2-降维" class="headerlink" title="3.1.2.2. 降维"></a>3.1.2.2. 降维</h6><p>降维的目的是简化数据、但是不能失去大部分信息。做法之一是合并若干相关的特征。例如，汽车的里程数与车龄高度相关，降维算法就会将它们合并成一个，表示汽车的磨损。这叫做特征提取。</p>
<p>此外，在采用机器学习算法训练的时候，可以对训练集进行降维，这样有助于提高训练速度，降低占用的硬盘和内存空间，有时候也能提高算法的性能，但必须选择合适的降维算法，否则性能实际上是很有可能会下降的。</p>
<h6 id="3-1-2-3-异常检测"><a href="#3-1-2-3-异常检测" class="headerlink" title="3.1.2.3. 异常检测"></a>3.1.2.3. 异常检测</h6><p>另一个重要的非监督任务是异常检测（anomaly detection）。例如，检测异常的信用卡转账以防欺诈，检测制造缺陷，或者在训练之前自动从训练数据集去除异常值。异常检测的系统使用正常值训练的，当它碰到一个新实例，它可以判断这个新实例是像正常值还是异常值。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%A4%BA%E4%BE%8B.png" alt=""></p>
<h6 id="3-1-2-4-关联规则学习"><a href="#3-1-2-4-关联规则学习" class="headerlink" title="3.1.2.4. 关联规则学习"></a>3.1.2.4. 关联规则学习</h6><p>最后，另一个常见的非监督任务是关联规则学习，它的目标是挖掘大量数据以发现属性间有趣的关系。例如，假设你拥有一个超市。在销售日志上运行关联规则，可能发现买了烧烤酱和薯片的人也会买牛排。因此，你可以将这些商品放在一起。</p>
<p>下面是一些最重要的非监督学习算法：</p>
<ol>
<li>聚类<ul>
<li>K 均值</li>
<li>层次聚类分析（Hierarchical Cluster Analysis, HCA）</li>
<li>期望最大值</li>
</ul>
</li>
<li>可视化和降维<ul>
<li>主成分分析（Principal    Component Analysis, PCA）</li>
<li>核主成分分析</li>
<li>局部线性嵌入（Locally-Linear Embedding, LLE）</li>
<li>t-分布邻域嵌入算法（t-distributed Stochastic Neighbor Embedding, t-SNE）</li>
</ul>
</li>
<li>关联性规则学习<ul>
<li>Apriori 算法<ul>
<li>Eclat算法</li>
</ul>
</li>
</ul>
</li>
</ol>
<h5 id="3-1-3-半监督学习"><a href="#3-1-3-半监督学习" class="headerlink" title="3.1.3 半监督学习"></a>3.1.3 半监督学习</h5><p>一些算法可以处理部分带标签的训练数据，通常是大量不带标签数据加上小部分带标签数据。这称作半监督学习。如下图所示，图中灰色圆点表示没有标签的数据，仅有几个三角形和正方形点表示带标签的数据。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%A4%BA%E4%BE%8B.png" alt="半监督学习示例"></p>
<p><strong>多数半监督学习算法是非监督和监督算法的结合</strong>。例如，深度信念网络（deep belief networks）是基于被称为互相叠加的受限玻尔兹曼机（restricted Boltzmann machines，RBM）的非监督组件。RBM 是先用非监督方法进行训练，再用监督学习方法进行整个系统微调。</p>
<p>半监督学习的示例，如一些图片存储服务，比如 Google Photos，是半监督学习的好例子。一旦你上传了所有家庭相片，它就能自动识别相同的人 A 出现了相片1、5、11    中，另一个人    B 出现在了相片 2、5、7 中。这是算法的非监督部分（聚类）。现在系统需要的就是你告诉这两个人是谁。只要给每个人一个标签，算法就可以命名每张照片中的每个人，特别适合搜索照片。</p>
<h5 id="3-1-4强化学习"><a href="#3-1-4强化学习" class="headerlink" title="3.1.4强化学习"></a>3.1.4强化学习</h5><p>强化学习和上述三种学习问题是非常不同的。学习系统在这里被称为<strong>智能体</strong>（ agent），可以对环境进行观察，选择和执行动作，获得<strong>奖励</strong>（负奖励是惩罚，见下图）。然后它必须自己学习哪个是最佳方法（称为<strong>策略</strong>，policy），以得到长久的最大奖励。策略决定了智能体在给定情况下应该采取的行动 。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.png" alt="强化学习示例"></p>
<p>目前强化学习的应用还不算非常广，特别是结合了深度学习的强化学习，主要是应用在机器人方面，当然最著名的一个应用就是 DeepMind 的 AlphaGo 了，它是通过分析数百万盘棋局学习制胜策略，然后自己和自己下棋。要注意，在比赛中机器学习是关闭的；AlphaGo    只是使用它学会的策略。 </p>
<h4 id="3-2-批量和在线学习"><a href="#3-2-批量和在线学习" class="headerlink" title="3.2 批量和在线学习"></a>3.2 批量和在线学习</h4><p>第二种分类机器学习的准则是，它是否能从导入的数据流进行持续学习。也就是如果导入的是持续的数据流，机器学习算法能否在不断采用新数据来训练已经训练好的模型，并且新的模型对新旧数据都还有很好的性能。</p>
<h5 id="3-2-1-批量学习"><a href="#3-2-1-批量学习" class="headerlink" title="3.2.1 批量学习"></a>3.2.1 批量学习</h5><p>在批量学习中，<strong>系统不能进行持续学习：必须用所有可用数据进行训练</strong>。这通常会占用大量时间和计算资源，所以一般是线下做的。首先是进行训练，然后部署在生产环境且停止学习，它只是使用已经学到的策略。这称为离线学习。</p>
<p>对于批量学习算法来说，当获取到新数据的时候，就需要重新重头训练整个数据集，然后更新模型，如果是应用该算法系统，那就相当于需要更新系统，需要停掉旧版本的系统，重新上线新版本的系统。</p>
<p>当然，一般训练、评估、部署一套机器学习的系统的整个过程可以自动进行，所以即便是批量学习也可以适应改变。只要有需要，就可以方便地更新数据、训练一个新版本。并且对于更新周期，可以选择每 24 小时或者每周更新一次。</p>
<p>但是，批量学习还是存在下面的缺点：</p>
<ol>
<li>实时性差，即对于需要快速适应变化的系统，比如预测股票变化、电商推荐系统等，就不适合采用批量学习算法；</li>
<li>耗费大量计算资源，用全部数据训练需要大量计算资源（CPU、内存空间、磁盘空间、磁盘 I/O、网络 I/O 等等），特别是训练集特别大的情况，更加凸显这个问题的严峻性；</li>
<li>无法应用在资源有限的设备上，比如需要自动学习的系统，但是如果采用智能手机，每次采用大量训练数据重新训练几个小时是非常不实际的。</li>
</ol>
<h5 id="3-2-2-在线学习"><a href="#3-2-2-在线学习" class="headerlink" title="3.2.2 在线学习"></a>3.2.2 在线学习</h5><p>批量学习的缺陷和问题可以通过采用在线学习算法来解决。</p>
<p>在在线学习中，是用数据实例持续地进行训练，可以一次一个或一次几个实例（称为小批量）。每个学习步骤都很快且廉价，所以系统可以动态地学习到达的新数据。</p>
<p>在线学习虽然名字带着在线两个字，但是实际上它的训练过程也是离线的，因此应该说是持续学习或者增量学习。</p>
<p>在线学习有下面几个优点：</p>
<ol>
<li>实时性好。在线学习算法非常适合接收连续流的数据，然后自动更新模型，实时性比批量学习更好；</li>
<li>可以节省大量计算资源。在线学习算法在学习新数据后，可以扔掉训练数据，从而节省大量存储空间；此外，训练得过程不需要加载所有训练数据，对于内存、CPU 等资源的要求也大大减少；</li>
<li>实现核外学习(out-of-core learning)。当内存不足以加载训练集的时候，可以采用在线学习算法多次训练，每次加载一部分训练集，即将一部分训练集当做新数据不断加载，直到训练完所有数据。</li>
</ol>
<p>在线学习也存在两个挑战：</p>
<ol>
<li>学习速率问题。学习速率是在线学习的一个重要参数，它反映了在线学习算法有多快地适应数据的改变，必须选择一个合适的学习速率，因为学习速率过大，系统可以很快适应新数据，但是也容易遗忘旧数据，比如图像分类问题，训练了一个 50 类分类器后，增加新的 10 类数据，一旦学习速率过快，系统只会记住新的 10 个类别，忘记了前面的 50 个类别的数据。相反的，如果你设定的学习速率低，系统的惰性就会强：即，它学的更慢，但对新数据中的噪声或没有代表性的数据点结果不那么敏感。</li>
<li>坏数据的影响。如果采用坏数据训练，会破坏系统的性能。要减小这种风险，你需要密集监测，如果检测到性能下降，要快速关闭（或是滚回到一个之前的状态）。你可能还要监测输入数据，对反常数据做出反应（比如，使用异常检测算法）。</li>
</ol>
<h4 id="3-3-基于实例-vs-基于模型学习"><a href="#3-3-基于实例-vs-基于模型学习" class="headerlink" title="3.3 基于实例 vs 基于模型学习"></a>3.3 基于实例 vs 基于模型学习</h4><p>第三种分类机器学习的方法是判断它们是如何进行归纳推广的。大多机器学习任务是关于预测的。这意味着给定一定数量的训练样本，系统需要能推广到之前没见到过的样本。对训练数据集有很好的性能还不够，真正的目标是对新实例预测的性能。</p>
<p>有两种主要的归纳方法：基于实例学习和基于模型学习。</p>
<h5 id="3-3-1-基于实例学习"><a href="#3-3-1-基于实例学习" class="headerlink" title="3.3.1 基于实例学习"></a>3.3.1 基于实例学习</h5><p>基于实例学习是系统先用记忆学习案例，然后使用相似度测量推广到新的例子，如下图所示：</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%9F%BA%E4%BA%8E%E5%AE%9E%E4%BE%8B%E5%AD%A6%E4%B9%A0%E7%A4%BA%E4%BE%8B.png" alt="基于实例学习示例"></p>
<p>这种学习算法可以说是机器学习中最简单的算法了，它实际上就是采用存储的数据集进行分类或者回归，典型的算法就是 KNN 算法，即 K 近邻算法，它就是将新的输入数据和已经保存的训练数据采用相似性度量（一般采用欧式距离）得到最近的 K 个训练样本，并采用 K 个训练样本中类别出现次数最多的类别作为预测的结果。</p>
<p>所以，这种算法的缺点就比较明显了：</p>
<ul>
<li>一是对存储空间的需求很大，需要占用的空间直接取决于实例数量的大小；</li>
<li>二是运行时间比较慢，因为需要需要与已知的实例进行比对。</li>
</ul>
<h5 id="3-3-2-基于模型学习"><a href="#3-3-2-基于模型学习" class="headerlink" title="3.3.2 基于模型学习"></a>3.3.2 基于模型学习</h5><p>和基于实例学习相反的就是基于模型学习：建立这些样本的模型，然后使用这个模型进行预测。如下图所示：</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%A4%BA%E4%BE%8B.png" alt="基于模型学习"></p>
<p>基于模型学习算法的流程一般如下所示：</p>
<ul>
<li>研究数据。先对数据进行分析，这可能包含清洗数据、特征筛选、特征组合等等</li>
<li>选择模型。选择合适的模型，从简单的线性回归、逻辑回归，到慢慢复杂的随机森林、集成学习，甚至深度学习的卷积神经网络模型等等</li>
<li>用训练数据进行训练。也就是寻找最适合算法模型的参数，使得代价函数取得最小值。</li>
<li>使用模型对新案例进行预测（这称作推断）。预测结果非常好，就能上线系统；如果不好，就需要进行错误分析，问题出现在哪里，是数据问题还是模型问题，找到问题，然后继续重复这个流程。</li>
</ul>
<h3 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a>4. 小结</h3><p>最后，总结下：</p>
<ol>
<li>机器学习就是让机器通过学习数据得到解决更好解决某些问题的能力，而不需要确定的代码规则；</li>
<li>机器学习的应用非常广泛，包含图像、自然语言处理、语音、推荐系统和搜索等方面，每个方面还有更加具体详细的应用方向；</li>
<li>机器学习按照不同的划分标准可以分为不同的学习类型，包括监督和非监督学习、批量和在线学习，基于实例和基于模型学习；</li>
<li>最常见的监督学习任务是分类和回归；</li>
<li>常见的非监督学习任务是聚类、降维、异常值检测和关联规则学习；</li>
</ol>
<p>以上就是本文的主要内容和总结，欢迎关注我的微信公众号—一个算法汪的技术成长之路或者扫描下方的二维码，和我分享你的建议和看法，指正文章中可能存在的错误，大家一起交流，学习和进步！</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/FqsJIpKGsF3-Qbu7bfQjzG-l0Ro9" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>最近打算系统学习和整理机器学习方面的知识，会将之前看的 Andrew Ng 在 course 课程笔记以及最近看的书籍《hands-on-ml-with-sklearn-and-tf》结合起来，简单总结下机器学习的常用算法，由于数学功底有限，所以可能不会也暂时不能过多深入公]]>
    </summary>
    
      <category term="技术" scheme="http://ccc013.github.io/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="机器学习" scheme="http://ccc013.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="http://ccc013.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[卷积神经网络介绍]]></title>
    <link href="http://ccc013.github.io/2018/05/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/"/>
    <id>http://ccc013.github.io/2018/05/08/卷积神经网络介绍/</id>
    <published>2018-05-08T03:04:39.000Z</published>
    <updated>2018-05-08T05:40:42.214Z</updated>
    <content type="html"><![CDATA[<p>记录在学习CNN过程中的一些知识点，包括参考的文章，论文或者博客等。</p>
<p>参考文章/书籍：</p>
<ol>
<li><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="external">An Intuitive Explanation of Convolutional Neural Networks</a></li>
<li><a href="http://blog.csdn.net/jiejinquanil/article/details/50042791" target="_blank" rel="external">对CNN中pooling的理解</a></li>
<li>《深度学习轻松学：核心算法与视觉实践》</li>
<li><a href="https://blog.csdn.net/lanran2/article/details/79057994" target="_blank" rel="external">ResNet解析</a></li>
</ol>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>CNN可以应用在场景分类，图像分类，现在还可以应用到自然语言处理(NLP)方面的很多问题，比如句子分类等。</p>
<p><strong>LeNet</strong>是最早的CNN结构之一，它是由大神<strong>Yann LeCun</strong>所创造的，主要是用在字符分类问题。</p>
<p>下面是一个简单的CNN结构，图来自参考文章<a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="external">1</a>。这个网络结构是用于一个四类分类的问题，分别是狗、猫、船和鸟，图中的输入图片是属于船一类。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/CNN1.png" alt="此处输入图片的描述"></p>
<p>该结构展示了四种运算，也可以说是由四种不同的层，分别是卷积层，非线性层(也就是使用了ReLU函数)，Pooling层，全连接层，下面将一一介绍这几种网络层。</p>
<hr>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><h4 id="卷积简介"><a href="#卷积简介" class="headerlink" title="卷积简介"></a>卷积简介</h4><p>  CNN的名字由来就是因为其使用了卷积运算的缘故。卷积的目的主要是为了提取图片的特征。卷积运算可以保持像素之间的空间关系。</p>
<p>  每张图片可以当做是一个包含每个像素值的矩阵，像素值的范围是0~255,0表示黑色，255是白色。下面是一个$5 \times 5$大小的矩阵例子，它的值是0或者1。</p>
<p>  <img src="http://7xrluf.com1.z0.glb.clouddn.com/CNN2.png" alt="此处输入图片的描述"></p>
<p>  接下来是另一个$3\times 3$矩阵：</p>
<p>  <img src="http://7xrluf.com1.z0.glb.clouddn.com/CNN3.png" alt="此处输入图片的描述"></p>
<p>  上述两个矩阵通过卷积，可以得到如下图右侧粉色的矩阵结果。</p>
<p>  <img src="http://7xrluf.com1.z0.glb.clouddn.com/CNN4.png" alt="此处输入图片的描述"></p>
<p>  黄色的矩阵在绿色的矩阵上从左到右，从上到下，每次滑动的步进值是1个像素，所以得到一个$3\times 3$的矩阵。</p>
<p>  在CNN中，黄色的矩阵被叫做<strong>滤波器(filter)或者核(kernel)或者是特征提取器</strong>，而通过卷积得到的矩阵则是称为<strong>“特征图(Feature Map)”或者“Activation Map”</strong>。</p>
<p>  另外，<strong>使用不同的滤波器矩阵是可以得到不同的 Feature Map</strong> ，例子如下图所示：</p>
<p>  <img src="http://7xrluf.com1.z0.glb.clouddn.com/CNN5.png" alt="此处输入图片的描述"></p>
<p>  上图通过滤波器矩阵，实现了不同的操作，比如边缘检测，锐化以及模糊操作等。</p>
<p>  在实际应用中，CNN是可以在其训练过程中学习到这些滤波器的值，不过我们需要首先指定好滤波器的大小，数量以及网络的结构。使用越多的滤波器，可以提取到更多的图像特征，网络也就能够有更好的性能。</p>
<p>  Feature Map的尺寸是由以下三个参数来决定的：</p>
<ul>
<li><strong>深度(Depth)</strong>： <strong>深度等于滤波器的数量</strong>。</li>
<li><strong>步进(Stride)</strong>: 步进值是在使用滤波器在输入矩阵上滑动的时候，每次滑动的距离。步进值越大，得到的Feature Map的尺寸越小。</li>
<li><strong>Zero-padding</strong>: 有时候可以在输入矩阵的边界填补0，这样就可以将滤波器应用到边缘的像素点上，一个好的Zero-padding是能让我们可以控制好特征图的尺寸的。使用该方法的卷积称为<strong>wide convolution</strong>，没有使用的则是<strong>narrow convolution</strong>。</li>
</ul>
<h4 id="卷积公式和参数量"><a href="#卷积公式和参数量" class="headerlink" title="卷积公式和参数量"></a>卷积公式和参数量</h4><p> 上一小节简单介绍了卷积的操作和其实现的效果，接下来将介绍卷积运算的公式，以及CNN中卷积层的参数数量。</p>
<p> 卷积是大自然中最常见的运算，一切信号观测、采集、传输和处理都可以用卷积过程实现，其用公式表达如下：</p>
<script type="math/tex; mode=display">
\begin{align}
Y(m,n) & =X(m,n)*H(m,n) \\ 
&= \sum_{i=-\infty}^{+\infty}\sum_{j=-\infty}^{+\infty}X(i,j)H(m-i,n-j) \\ &=\sum_{i=-\infty}^{+\infty}\sum_{j=-\infty}^{+\infty}X(m-i,n-j)H(i,j)
\end{align}</script><p>上述公式中$H(m,n)$表示卷积核。</p>
<p>在CNN中的卷积层的计算步骤与上述公式定义的二维卷积有点差异，首先是维度升至三维、四维卷积，跟二维卷积相比多了一个<strong>“通道”(channel)</strong>，每个通道还是按照二维卷积方式计算，而多个通道与多个卷积核分别进行二维卷积，得到多通道输出，需要“合并”为一个通道；<strong>其次是卷积核在卷积计算时没有“翻转”，而是与输入图片做滑动窗口“相关”计算</strong>。用公式重新表达如下：</p>
<script type="math/tex; mode=display">
Y^l(m,n) =X^k(m,n)*H^{kl}(m,n) = \sum_{k=0}^{K-1}\sum_{i=0}^{I-1}\sum_{j=0}^{J-1}X^k(m+i,n+j)H^{kl}(i,j)</script><p>这里假定卷积层有$L$个输出通道和$K$个输入通道，于是需要有$KL$个卷积核实现通道数目的转换。其中$X^k$表示第$k$个输入通道的二维特征图，$Y^l$表示第$l$个输出通道的二维特征图，$H^{kl}$表示第$k$行、第$l$列二维卷积核。假定卷积核大小是$I*J$,每个输出通道的特征图大小是$M×N$，则该层每个样本做一次前向传播时卷积层的计算量是$Calculations(MAC)=I×J×M×N×K×L$。</p>
<p>卷积层的学习参数，也就是<strong>卷积核数目乘以卷积核的尺寸—$Params = I×J×K×L$。</strong></p>
<p>这里定义计算量-参数量之比是<strong>CPR</strong>=$Calculations/Params=M×N$。</p>
<p>因此可以得出结论：<strong>卷积层的输出特征图尺寸越大，CPR越大，参数重复利用率越高。若输入一批大小为B的样本，则CPR值可提高B倍。</strong></p>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><p>卷积神经网络通过<strong>『参数减少』与『权值共享』</strong>大大减少了连接的个数，也即需要训练的参数的个数。</p>
<p>假设我们的图像是<code>1000*1000</code>的，则有10^6个隐层神经元，那么它们全连接的话，也就是每个隐层神经元都连接图像的每个像素点，就有10^12个连接，也即10^12个权值参数需要训练，这显然是不值得的。但是对于一个只识别特定feature的卷积核，需要大到覆盖整个图像的所有像素点吗？通常是不需要的，<strong>一个特定feature，尤其是第一层需要提取的feature，通常都相当基础，只占图像很小的一部分。所以我们设置一个较小的局部感受区域，比如<code>10*10</code>，也即每个神经元只需要和这<code>10*10</code>的局部图像相连接，所以10^6个神经元也就有10^8个连接。这就叫参数减少。</strong></p>
<p>那什么叫权值共享呢？在上面的局部连接中，10^6个神经元，每个神经元都对应100个参数，所以是10^8个参数，<strong>那如果每个神经元所对应的参数都是相同的，那需要训练的参数就只有100个了。</strong></p>
<p>这后面隐含的道理在于，<strong>这100个参数就是一个卷积核，而卷积核是提取feature的方式，与其在图像上的位置无关，图像一个局部的统计特征与其他局部的统计特征是一样的，我们用在这个局部抽取feature的卷积核也可以用在图像上的其它任何地方。</strong></p>
<p>而且这100个参数只是一种卷积核，只能提取一种feature，我们完全可以采用100个卷积核，提取100种feature，而所需要训练的参数也不过10^4，最开始我们训练10^12个参数，还只能提取一种特征。选取100个卷积核，我们就能得到100张FM，每张FM可以看做是一张图像的不同通道。</p>
<p>CNN主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。由于CNN特征检测层通过训练数据进行学习，在使用CNN时，避免了显式的特征抽取，而隐式地从训练数据中进行学习；再者，由于同一FM上的神经元权值相同，所以网络可以并行学习，这也是卷积网络相对于神经元彼此相连网络的一大优势。卷积神经网络以其局部权值共享的特殊结构在语音识别和图像处理方面有着独特的优越性，其布局更接近于实际的生物神经网络，权值共享降低了网络的复杂性，避免了特征提取和分类过程中数据重建的复杂度。</p>
<hr>
<h3 id="非线性层-ReLU"><a href="#非线性层-ReLU" class="headerlink" title="非线性层(ReLU)"></a>非线性层(ReLU)</h3><p>  非线性修正函数<strong>ReLU(Rectified Linear Unit)</strong>如下图所示：</p>
<p>  <img src="http://7xrluf.com1.z0.glb.clouddn.com/CNN6.png" alt="此处输入图片的描述"></p>
<p>这是一个对每个像素点实现点乘运算，并用0来替换负值像素点。其目的是在CNN中加入非线性，<strong>因为使用CNN来解决的现实世界的问题都是非线性的，而卷积运算是线性运算，所以必须使用一个如ReLU的非线性函数来加入非线性的性质。</strong></p>
<p>其他非线性函数还包括<strong>tanh</strong>和<strong>Sigmoid</strong>,但是<strong>ReLU</strong>函数已经被证明在大部分情况下性能最好。</p>
<h3 id="Pooling层"><a href="#Pooling层" class="headerlink" title="Pooling层"></a>Pooling层</h3><p>  <strong>空间合并（Spatial Pooling)</strong>也可以叫做子采样或者下采样，可以在保持最重要的信息的同时降低特征图的维度。它有不同的类型，如最大化，平均，求和等等。</p>
<p>  对于<strong>Max Pooling</strong>操作，首先定义一个空间上的邻居，比如一个$2\times 2$的窗口，对该窗口内的经过ReLU的特征图提取最大的元素。除了提取最大的元素，还可以使用窗口内元素的平均值或者是求和的值。不过，<strong>Max Pooling</strong>的性能是最好的。例子可以如下图所示：</p>
<p>  <img src="http://7xrluf.com1.z0.glb.clouddn.com/CNN7.png" alt="此处输入图片的描述"></p>
<p>  上图中使用的步进值是2。</p>
<p>根据相关理论，特征提取的误差主要来自两个方面：</p>
<ol>
<li>邻域大小受限造成的估计值方差增大；</li>
<li>卷积层参数误差造成估计均值的偏移。</li>
</ol>
<p><strong>一般来说，mean-pooling能减小第一种误差，更多的保留图像的背景信息，max-pooling能减小第二种误差，更多的保留纹理信息。</strong></p>
<p>  使用Pooling的原因有如下几点：</p>
<ul>
<li>不变性，更关注是否存在某些特征而不是特征具体的位置。可以看作加了一个很强的先验，让学到的特征要能容忍一些的变化。</li>
<li>减小下一层输入大小，减小计算量和参数个数。</li>
<li>获得定长输出。（文本分类的时候输入是不定长的，可以通过池化获得定长输出）</li>
<li>防止过拟合或有可能会带来欠拟合</li>
</ul>
<hr>
<h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>  全连接层就是一个传统的多层感知器，它在输出层使用一个<strong>softmax</strong>激活函数。其主要作用就是将前面卷积层提取到的特征结合在一起然后进行分类。<strong>Softmax</strong>函数可以将输入是一个任意实数分数的向量变成一个值的范围是0~1的向量，但所有值的总和是1。</p>
<p>  在CNN出现之前，最早的深度学习网络计算类型都是全连接形式的。</p>
<p>  全连接层的主要计算类型是<strong>矩阵-向量乘（GEMV)。</strong>假设输入节点组成的向量是$x$，维度是$D$,输出节点组成的向量是$y$,维度是$V$,则全连接层计算可以表示为$y=Wx$。</p>
<p>  其中$W$是$V×D$的权值矩阵。</p>
<p>  全连接层的参数量为$Params=V×D$,其单个样本前向传播的计算量也是$Calculations(MAC)=V×D$，也就是$CPR=Calculations/Params=1$。也就是其权值利用率很低。</p>
<p>  可以将一批大小为$B$的样本$x_i$逐列拼接成矩阵$X$，一次性通过全连接层，得到一批输出向量构成的矩阵$Y$，相应地前面的矩阵-向量乘运算升为<strong>矩阵-矩阵乘计算（GEMM)：$Y=WX$</strong>。</p>
<p>  这样全连接层前向计算量提高了$B$倍，CPR相应提高了$B$倍，权重矩阵在多个样本之间实现了共享，可提高计算速度。</p>
<p>  比较卷积层和全连接层，卷积层在输出特征图维度实现了<strong>权值共享</strong>，这是降低参数量的重要举措，同时，卷积层<strong>局部连接</strong>特性（相比全连接）也大幅减少了参数量。<strong>因此卷积层参数量占比小，但计算量占比大，而全连接层是参数量占比大，计算量占比小。所以在进行计算加速优化时，重点放在卷积层；在进行参数优化、权值剪裁时，重点放在全连接层。</strong></p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>激活函数是给网络提供非线性的特性，在每个网络层中对输入数据进行非线性变换的作用，这有两个好处。</p>
<ul>
<li><strong>对数据实现归一化操作</strong></li>
</ul>
<p>激活函数都有各自的取值范围，比如Sigmoid函数取值范围是[0,1]，Tanh函数取值范围是[-1,1]，这种好处对网络的正反向训练都有好处：</p>
<p>（1）正向计算网络的时候，由于输入数值的大小没有限制，其数值差距会非常大，第一个坏处是大数值会更被重视，而小数值的重要性会被忽视，其次，随着层数加深，这种大数值会不断累积到后面的网络层，最终可能导致数值爆炸溢出的情况；</p>
<p>（2）反向计算网络的时候，每层数值大小范围不同，有的在[0,1]，有的在[0,10000]，这在模型优化时会对设定反向求导的优化步长增加难度，设置过大会让梯度较大的维度因为过量更新而造成无法预期的结果；设置过小，梯度较小的维度会得不到充分的更新，就无法有所提升。</p>
<ul>
<li><strong>打破之前的线性映射关系。</strong></li>
</ul>
<p>如果网络只有线性部分，那么叠加多个网络层是没有意义的，因为多层神经网络可以退化为一层神经网络。</p>
<hr>
<h3 id="反向传播Backpropagation"><a href="#反向传播Backpropagation" class="headerlink" title="反向传播Backpropagation"></a>反向传播Backpropagation</h3><p>  CNN的整个训练过程如下所示：</p>
<ol>
<li>首先是随机初始化所有滤波器以及其他参数和权重值；</li>
<li>输入图片，进行前向传播，也就是经过卷积层，ReLU和pooling运算，最后到达全连接层进行分类，得到一个分类的结果，也就是输出一个包含每个类预测的概率值的向量；</li>
<li>计算误差，也就是代价函数，这里代价函数可以有多种计算方法，比较常用的有平方和函数，即$实际值预测值Error = \frac{1}{2}\sum(实际值-预测值)^2$；</li>
<li>使用反向传播来计算网络中对应各个权重的误差的梯度，一般是使用梯度下降法来更新各个滤波器的权重值，目的是为了让输出的误差，也就是代价函数的值尽可能小。</li>
<li>重复上述第二到第四步，直到训练次数达到设定好的值。</li>
</ol>
<hr>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>这里简单介绍比较有名的网络结构。</p>
<h4 id="1-LeNet（1990s）"><a href="#1-LeNet（1990s）" class="headerlink" title="1.LeNet（1990s）"></a>1.LeNet（1990s）</h4><p>在开头介绍了，这是最早使用的CNN网络结构之一，主要是用于字符分类；</p>
<p>   特点如下：</p>
<ul>
<li>卷积神经网络使用3层架构：卷积、下采样、非线性激活函数</li>
<li>使用卷积提取图像空间特征<ul>
<li>下采样使用了图像平均稀疏性</li>
<li>激活函数采用了tanh或者sigmoid函数</li>
<li>多层神经网络（MLP）作为最后的分类器</li>
<li>层之间使用稀疏连接矩阵，以避免大的计算成本</li>
</ul>
</li>
</ul>
<h4 id="2-AlexNet（2012）"><a href="#2-AlexNet（2012）" class="headerlink" title="2. AlexNet（2012）"></a>2. AlexNet（2012）</h4><p>这是在2012年的ImageNet视觉挑战比赛上获得第一名所使用的网络结构，这也是使得许多视觉问题取得重大突破，让CNN变得非常热门的原因。总结下其改进地方：</p>
<ul>
<li>使用ReLU函数作为激活函数，降低了Sigmoid类函数的计算量</li>
<li>利用dropout技术在训练期间选择性地剪掉某些神经元，避免模型过度拟合</li>
<li>引入max-pooling技术</li>
<li>利用双GPU NVIDIA GTX 580显著减少训练时间</li>
</ul>
<h4 id="3-ZF-Net（2013）"><a href="#3-ZF-Net（2013）" class="headerlink" title="3. ZF Net（2013）"></a>3. ZF Net（2013）</h4><p>这是2013年ImageNet比赛的胜者，对AlexNet的结构超参数做出了调整。</p>
<h4 id="4-GoogleNet（2014）"><a href="#4-GoogleNet（2014）" class="headerlink" title="4. GoogleNet（2014）"></a>4. GoogleNet（2014）</h4><p>2014年ImageNet比赛的胜者，其主要贡献是使用了一个<strong>Inception Module</strong>，可以大幅度减少网络的参数数量，其参数数量是4M，而AlexNet的则有60M。</p>
<h4 id="5-VGGNet（2014）"><a href="#5-VGGNet（2014）" class="headerlink" title="5.VGGNet（2014）"></a>5.VGGNet（2014）</h4><p>这是一个更深的网络，使用了16层的结构。<strong>它是对原始图像进行3×3卷积，然后再进行3×3卷积，连续使用小的卷积核对图像进行多次卷积。</strong>VGG一开始提出的时候刚好与LeNet的设计原则相违背，<strong>因为LeNet相信大的卷积核能够捕获图像当中相似的特征（权值共享）。</strong>AlexNet在浅层网络开始的时候也是使用9×9、11×11卷积核，并且尽量在浅层网络的时候避免使用1×1的卷积核。但是VGG的神奇之处就是在于使用多个3×3卷积核可以模仿较大卷积核那样对图像进行局部感知。后来多个小的卷积核串联这一思想被GoogleNet和ResNet等吸收。</p>
<p>   <strong>VGG相信如果使用大的卷积核将会造成很大的时间浪费，减少的卷积核能够减少参数，节省运算开销。虽然训练的时间变长了，但是总体来说预测的时间和参数都是减少的了。</strong></p>
<h4 id="6-ResNets（2015）"><a href="#6-ResNets（2015）" class="headerlink" title="6.ResNets（2015）"></a>6.ResNets（2015）</h4><p>随着网络的加深，出现了<strong>训练集准确率下降</strong>的现象，我们可以确定<strong>这不是由于Overfit过拟合造成的(过拟合的情况训练集应该准确率很高)</strong>；所以作者针对这个问题提出了一种全新的网络，叫深度残差网络，它允许网络尽可能的加深，其中引入了全新的结构如下图；<br><img src="https://img-blog.csdn.net/20180508132926226?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xjMDEz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p><strong>残差指的是什么？</strong><br>其中ResNet提出了两种mapping：一种是<strong>identity mapping</strong>，指的就是上图中”弯弯的曲线”，另一种<strong>residual mapping</strong>，指的就是除了”弯弯的曲线“那部分，所以最后的输出是 y=F(x)+x<br>identity mapping顾名思义，就是指本身，也就是公式中的x，而residual mapping指的是“差”，也就是y−x，所以残差指的就是F(x)部分。<br><strong>为什么ResNet可以解决“随着网络加深，准确率不下降”的问题？</strong><br>理论上，对于“随着网络加深，准确率下降”的问题，Resnet提供了两种选择方式，也就是identity mapping和residual mapping，<strong>如果网络已经到达最优，继续加深网络，residual mapping将被push为0，只剩下identity mapping，这样理论上网络一直处于最优状态了，网络的性能也就不会随着深度增加而降低了。</strong></p>
<p>​    </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>记录在学习CNN过程中的一些知识点，包括参考的文章，论文或者博客等。</p>
<p>参考文章/书籍：</p>
<ol>
<li><a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/]]>
    </summary>
    
      <category term="卷积神经网络" scheme="http://ccc013.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="机器学习" scheme="http://ccc013.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://ccc013.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[2016总结 & 2017展望]]></title>
    <link href="http://ccc013.github.io/2016/12/31/2016%E6%80%BB%E7%BB%93-2017%E5%B1%95%E6%9C%9B/"/>
    <id>http://ccc013.github.io/2016/12/31/2016总结-2017展望/</id>
    <published>2016-12-31T08:30:39.000Z</published>
    <updated>2017-01-01T04:14:36.010Z</updated>
    <content type="html"><![CDATA[<h4 id="2016-总结"><a href="#2016-总结" class="headerlink" title="2016 总结"></a>2016 总结</h4><p>对于2016年，最大的感觉还是自己还总是处于变化的过程吧，主要是对求职方向的变化，上半年主要是处于学习Android的过程，但是后来听了师兄的话后，大概就是6月底，就开始转变，想要往机器学习算法岗位准备，然后到了10月份左右，随着时间和自己准备两方面考虑，又决定应该往C++开发，但是跟师兄交流后，发现似乎这个方向需要准备的东西要更多，还是好好准备机器学习算法岗位吧。这么来来回回变化，也是自己性格上的缺点吧，缺乏恒心，不能坚持太久，或者其实是自己太过浮躁了，每次都有点过于冲动地考虑，又特别容易后悔自己做过的决定，这真的是需要在未来的一年里面去改进的，需要有恒心，专心和坚持完成一件事情。</p>
<p>科研进展方向，自己是写了一篇中文的论文，第一次投稿是被拒绝了，现在第二次投稿，尚处于审稿中，希望可以顺利收录；然后就是看了多篇论文啦，还看完一本介绍Caffe的，也尝试修改了一些代码，但是基本是根据网上别人的代码来修改的，复现的方法也能找到代码，现在毕设题目也是确定了，需要做的就是设计自己的算法，能够有足够的创新性吧。</p>
<p>编程方面，上半年也是看了一两本Android方面的书，然后C++方面，看完《C++ primer plus》，看完《大话数据结构》和《数据结构算法与应用：C++描述》，算法方面则看了《剑指offer》，看完《机器学习》（年终最后一天完成），《现代操作系统》则根据师兄的建议，看完前面比较重要的六章，而Linux方面，正在看鸟哥的《Linux私房菜—基础学习篇》，看完前面十一章内容，打算是看到第三部分的，目前就剩下两章内容，估计需要多两到三天的时间。这是书籍方面的阅读，此外，还有到LeetCode上刷题，目前做了20多道题目，然后在牛客网上也做了不少编程题目，数据结构和操作系统练习，都需要继续坚持。</p>
<p>此外，还是有坚持做了一些笔记，主要是数据结构和算法学习的笔记，需要继续保持。在博客方面，我的技术博客上是发表了30篇日志，当然主要是数据结构学习的比较和总结文章。而在CSDN博客方面，也写了有37篇博文，主要是下半年开始增多，前面主要是机器学习的时候的笔记，之后就是有一些论文阅读笔记和算法学习笔记。而Github方面，今年提交次数也有两百多近三百，当然提交的也都是上述的笔记内容。</p>
<p>其他，锻炼方面也是断断续续做着，还不能很好地坚持，所以基本上体重没有比去年减少多少，跑步方面，15年是总共跑了33次，总计里程是129.59km，而16年跑步次数增加到36次，总里程是144.08km，增加得不是很明显，按照每月跑步算是，月均3次而已；而在力量训练方面，总共锻炼时间是630分钟，总共锻炼次数是47次。</p>
<p>旅游方面，上半年去了武汉参加一次会议，然后都是省内游了。</p>
<h4 id="2017-展望"><a href="#2017-展望" class="headerlink" title="2017 展望"></a>2017 展望</h4><p>首先是希望在上半年找到一份比较好的实习，目前求职岗位是机器学习算法，所以希望找到机器学习算法相关的岗位，所以这需要前面两个多月继续好好看书，刷题准备；</p>
<p>其次是希望投稿的论文可以顺利被收录，这样也可以达成毕业条件了。</p>
<p>接着就是下半年秋招可以找到一份好的工作，实习的时候也要争取能收到留下的offer。</p>
<p>最后就是搞定毕设，写好毕设论文了。</p>
<p>上述算是明年比较重要的四件事情了。在这其中，对于找实习还是工作，首先还是需要继续学习，书籍阅读方面，《统计学习方法》是需要好好看透，理解好的，然后《TCP/IP 协议》也要看，掌握网络知识；python方面的知识也要复习一下；然后就是看看《大话设计模式》和《STL源码剖析》，继续加强C++编程能力。</p>
<p>然后需要继续坚持做笔记，写博客，上传代码到Github。希望博客内容可以有更多干货，不只是阅读书籍的学习笔记，还有更多实践内容，比如一些自己做的项目代码，一些应用的实现等。</p>
<p>最后就是坚持锻炼身体，继续减脂训练，跑步和力量训练都要有序进行，计划跑步方面，总里程要达到300km左右，如果每次跑步在4km以上，那么需要总共跑75次，也就是平均每月6-7次左右；然后力量训练方面是总训练时间达到1200分钟，如果平均每次锻炼时间是30分钟，需要锻炼40次，平均到每月就是3-4次。</p>
<p>总而言之，希望能够更加专注，更加坚持，脚踏实地做好每件计划好的事情。</p>
]]></content>
    <summary type="html">
    <![CDATA[<h4 id="2016-总结"><a href="#2016-总结" class="headerlink" title="2016 总结"></a>2016 总结</h4><p>对于2016年，最大的感觉还是自己还总是处于变化的过程吧，主要是对求职方向的变化，上半年主要是处于学]]>
    </summary>
    
      <category term="总结" scheme="http://ccc013.github.io/tags/%E6%80%BB%E7%BB%93/"/>
    
      <category term="随笔" scheme="http://ccc013.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[排序算法总结]]></title>
    <link href="http://ccc013.github.io/2016/11/20/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>http://ccc013.github.io/2016/11/20/排序算法总结/</id>
    <published>2016-11-20T05:27:44.000Z</published>
    <updated>2016-12-08T03:18:55.965Z</updated>
    <content type="html"><![CDATA[<p>这是《大话数据结构》第九章排序算法的知识点总结。</p>
<h4 id="排序的基本概念与分类"><a href="#排序的基本概念与分类" class="headerlink" title="排序的基本概念与分类"></a>排序的基本概念与分类</h4><blockquote>
<p>假设含有n个记录的序列为${r_1,r_2,\cdots,r_n}$，其相应的关键字分别为${k_1,k_2,\cdots,k_n}$，需要确定$1,2, \cdots, n$的一种排列$p_1,p_2,\cdots,p_n$，使其相应的关键字满足$k_{p1}\le k_{p2}\le \cdots \le k_{pn}$非递减（或非递增）关系，即使得序列成为一个按关键字有序的序列${r_{p1}, r_{p2}, \cdots, r_{pn}}$，这样的操作就称为排序。</p>
</blockquote>
<p><strong>在排序问题中，通常将数据元素称为记录。</strong></p>
<p>排序的依据是关键字之间的大小关系，那么，对同一个记录集合，针对不同的关键字进行排序，可以得到不同序列。</p>
<p>这里关键字$k_i$可以是记录$r$的主关键字，也可以是次关键字，甚至是若干数据项的组合。</p>
<h5 id="排序的稳定性"><a href="#排序的稳定性" class="headerlink" title="排序的稳定性"></a>排序的稳定性</h5><p>由于排序不仅是针对主关键字，还有针对次关键字，因为待排序的记录序列中可能存在两个或两个以上的关键字相等的记录，排序结果可能会存在不唯一的情况，下面给出稳定与不稳定排序的定义。</p>
<blockquote>
<p>假设$k_i = k_j \ (1\le i \le n, 1\le j\le n, i\neq j)$，且在排序前的序列中$r_i$领先于$r_j$（即$i \lt j$）。如果排序后$r_i$仍领先于$r_j$，则称所用的排序方法是稳定的；反之，若可能使得排序后的序列中$r_j$领先于$r_i$，则称所用的排序方法是不稳定的。</p>
</blockquote>
<p>不稳定的排序算法有：<strong>希尔、快速、堆排和选择排序</strong>。</p>
<h5 id="内排序和外排序"><a href="#内排序和外排序" class="headerlink" title="内排序和外排序"></a>内排序和外排序</h5><p>根据在排序过程中待排序的记录是否全部被放置在内存中，排序可以分为：内排序和外排序。</p>
<blockquote>
<p>内排序是在排序整个过程中，<strong>待排序的所有记录全部被放置在内存中</strong>。外排序是由于排序的记录个数太多，不能同时放置在内存，整个排序过程需要<strong>在内外存之间多次交换数据才能进行</strong>。</p>
</blockquote>
<p>对于内排序来说，排序算法的性能主要是受到3个方面的影响：</p>
<h6 id="时间性能"><a href="#时间性能" class="headerlink" title="时间性能"></a>时间性能</h6><p>在内排序中，主要进行两种操作：<strong>比较和移动</strong>。高效率的内排序算法应该是具有尽可能少的关键字比较次数和尽可能少的记录移动次数。</p>
<h6 id="辅助空间"><a href="#辅助空间" class="headerlink" title="辅助空间"></a>辅助空间</h6><p>辅助存储空间是除了存放待排序所占用的存储空间之外，执行算法所需要的其他存储空间。</p>
<h6 id="算法的复杂性"><a href="#算法的复杂性" class="headerlink" title="算法的复杂性"></a>算法的复杂性</h6><p>这里指的是算法本身的复杂度，而不是算法的时间复杂度。</p>
<p>根据排序过程中借助的主要操作，我们把<strong>内排序分为：插入排序、交换排序、选择排序和归并排序。</strong></p>
<h5 id="排序用到的结构与函数"><a href="#排序用到的结构与函数" class="headerlink" title="排序用到的结构与函数"></a>排序用到的结构与函数</h5><p>这里先提供一个用于排序用的顺序表结构，这个结构将用于接下来介绍的所有排序算法。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#<span class="keyword">define</span> MAXSIZE <span class="number">10</span></span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">// 用于存储待排序数组</span></span><br><span class="line">  <span class="keyword">int</span> r[MAXSIZE]; </span><br><span class="line">  <span class="comment">// 用于记录顺序表的长度</span></span><br><span class="line">  <span class="keyword">int</span> length;</span><br><span class="line">&#125;SqList;</span><br></pre></td></tr></table></figure>
<p>此外，由于排序最常用到的操作是数组两元素的交换，这里写成一个函数，如下所示：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 交换L中数组r的下标为i和j的值</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(SqList *L, <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> temp = L-&gt;r[i];</span><br><span class="line">  L-&gt;r[i] = L-&gt;r[j];</span><br><span class="line">  L-&gt;r[j] = temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h4><blockquote>
<p><strong>冒泡排序(Bubble sort)是一种交换排序。</strong>它的基本思想是：两两比较相邻记录的关键字，如果反序则交换，知道没有反序的记录为止。</p>
</blockquote>
<p>首先介绍一个简单版本的冒泡排序算法的实现代码。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 冒泡排序初级版</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BubbleSort0</span><span class="params">(SqList *L)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i, j;</span><br><span class="line">	<span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; L-&gt;length - <span class="number">1</span>; i++) &#123;</span><br><span class="line">		<span class="keyword">for</span> (j = i + <span class="number">1</span>; j &lt;= L-&gt;length - <span class="number">1</span>; j++)&#123;</span><br><span class="line">			<span class="keyword">if</span> (L-&gt;r[i] &gt; L-&gt;r[j])&#123;</span><br><span class="line">				<span class="comment">// 实现递增排序</span></span><br><span class="line">				swap(L, i, j);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码不算是标准的冒泡排序算法，因为不满足“两两比较相邻记录”的冒泡排序思想，它更应该是最简单的交换排序。它的思路是让每一个关键字都和后面的每一个关键字比较，如果大或小则进行交换，这样关键字在一次循环后，第一个位置的关键字会变成最大值或者最小值。</p>
<p>这个最简单的实现算法效率是非常低的。</p>
<p>下面介绍正宗的冒泡排序算法实现。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 正宗的冒泡排序算法实现代码</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BubbleSort</span><span class="params">(SqList *L)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i, j;</span><br><span class="line">	<span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; L-&gt;length; i++) &#123;</span><br><span class="line">		<span class="keyword">for</span> (j = L-&gt;length - <span class="number">2</span>; j &gt;= i; j--)&#123;</span><br><span class="line">			<span class="comment">// j是从后往前循环</span></span><br><span class="line">			<span class="keyword">if</span> (L-&gt;r[j] &gt; L-&gt;r[j + <span class="number">1</span>])&#123;</span><br><span class="line">				<span class="comment">// 实现递增排序</span></span><br><span class="line">				swap(L, j, j + <span class="number">1</span>);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里改变的地方是在内循环中，<code>j</code>是从数组最后往前进行比较，并且是逐个往前进行相邻记录的比较，这样最大值或者最小值会在第一次循环过后，从后面浮现到第一个位置，如同气泡一样浮到上面。</p>
<p>这段实现代码其实还是可以进行优化的，例如待排序数组是<code>{2,1,3,4,5,6,7,8,9}</code>,需要进行递增排序，可以发现其实只需要交换前两个元素的位置即可完成，但是上述算法还是会在交换完这两者位置后继续进行循环，这样效率就不高了，所以可以在算法中增加一个标志，当有一次循环中没有进行数据交换，就证明数组已经是完成排序的，此时就可以退出算法，实现代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 改进版冒泡算法</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BubbleSortOptimz</span><span class="params">(SqList *L)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i, j;</span><br><span class="line">	<span class="keyword">bool</span> flag = <span class="literal">true</span>;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; L-&gt;length &amp;&amp; flag; i++)&#123;</span><br><span class="line">		<span class="comment">// 若 flag为false则退出循环</span></span><br><span class="line">		flag = <span class="literal">false</span>;</span><br><span class="line">		<span class="keyword">for</span> (j = L-&gt;length - <span class="number">2</span>; j &gt;= i; j--)&#123;</span><br><span class="line">			<span class="comment">// j是从后往前循环</span></span><br><span class="line">			<span class="keyword">if</span> (L-&gt;r[j] &gt; L-&gt;r[j + <span class="number">1</span>])&#123;</span><br><span class="line">				<span class="comment">// 实现递增排序</span></span><br><span class="line">				swap(L, j, j + <span class="number">1</span>);</span><br><span class="line">				<span class="comment">// 如果有数据交换，则flag是true</span></span><br><span class="line">				flag = <span class="literal">true</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>冒泡排序算法的时间复杂度是$O(n^2)$。</p>
<p>完整的冒泡排序算法代码可以查看<a href="https://github.com/ccc013/DataStructe-Algorithms_Study/SortAlgorithms/BubbleSortTest.cpp" target="_blank" rel="external">BubbleSort</a>。</p>
<h4 id="简单选择排序"><a href="#简单选择排序" class="headerlink" title="简单选择排序"></a>简单选择排序</h4><blockquote>
<p>简单选择排序算法(Simple Selection Sort)就是通过$n-i$次关键字间的比较，从$n-i+1$个记录中选出关键字中最小的记录，并和第$i(1\le i \le n)$个记录进行交换。</p>
</blockquote>
<p>下面是实现的代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 简单选择排序算法</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">SelectSort</span><span class="params">(SqList *L)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i, j, min;</span><br><span class="line">	<span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; L-&gt;length - <span class="number">1</span>; i++)&#123;</span><br><span class="line">		<span class="comment">// 将当前下标定义为最小值下标</span></span><br><span class="line">		min = i;</span><br><span class="line">		<span class="keyword">for</span> (j = i + <span class="number">1</span>; j &lt;= L-&gt;length - <span class="number">1</span>; j++)&#123;</span><br><span class="line">			<span class="keyword">if</span> (L-&gt;r[j] &lt; L-&gt;r[min])</span><br><span class="line">				min = j;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">// 若min不等于i，说明找到最小值，进行交换</span></span><br><span class="line">		<span class="keyword">if</span> (min != i)</span><br><span class="line">			swap(L, i, min);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>简单选择排序的最大特点就是交换移动数据次数相当少。分析其时间复杂度发现，无论最好最差的情况，比较次数都是一样的，都需要比较$\sum_{i=1}^{n-1} (n-i) = (n-1)+(n-2)+\cdots+2+1=\frac{n(n-1)}{2}$次。对于交换次数，最好的时候是交换0次，而最差的情况是$n-1$次。因此，总的时间复杂度是$O(n^2)$，虽然与冒泡排序一样的时间复杂度，但是其性能上还是略好于冒泡排序。</p>
<h4 id="直接插入排序"><a href="#直接插入排序" class="headerlink" title="直接插入排序"></a>直接插入排序</h4><blockquote>
<p>直接插入排序(Straight Insertion Sort)的基本操作是将一个记录插入到已经排好序的有序表中，从而得到一个新的、记录数增加1的有序表。</p>
</blockquote>
<p>实现代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 直接插入排序</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">InsertSort</span><span class="params">(SqList *L)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i, j,val;</span><br><span class="line">	<span class="keyword">for</span> (i = <span class="number">1</span>; i &lt;= L-&gt;length - <span class="number">1</span>; i++)&#123;</span><br><span class="line">		<span class="keyword">if</span> (L-&gt;r[i] &lt; L-&gt;r[i - <span class="number">1</span>])&#123;</span><br><span class="line">			<span class="comment">// 将L-&gt;r[i]插入有序表中,使用val保存待插入的数组元素L-&gt;r[i]</span></span><br><span class="line">			val = L-&gt;r[i];</span><br><span class="line">			<span class="keyword">for</span> (j = i - <span class="number">1</span>; L-&gt;r[j]&gt;val; j--)</span><br><span class="line">				<span class="comment">// 记录后移</span></span><br><span class="line">				L-&gt;r[j + <span class="number">1</span>] = L-&gt;r[j];	</span><br><span class="line">			<span class="comment">// 插入到正确位置</span></span><br><span class="line">			L-&gt;r[j + <span class="number">1</span>] =val;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>直接插入排序算法是需要有一个保存待插入数值的辅助空间。</p>
<p>在时间复杂度方面，最好的情况是待排序的表本身就是有序的，如{2,3,4,5,6}，比较次数则是$n-1$次，然后不需要进行移动，时间复杂度是$O(n)$。</p>
<p>最差的情况就是待排序表是逆序的情况，如{6,5,4,3,2},此时需要比较$\sum_{i=2}^{n} i = \frac{(n+2)(n-1)}{2}$次，而记录的移动次数也达到最大值$\sum_{i=2}^{n} (i+1) = \frac{(n+4)(n-1)}{2}$次。</p>
<p>如果排序记录是随机的，那么根据概率相同的原则，平均比较和移动次数约为$\frac{n^2}{4}$。因此，可以得出直接插入排序算法的时间复杂度是$O(n^2)$。同时也可以看出，直接插入排序算法会比冒泡排序和简单选择排序算法的性能要更好一些。</p>
<h4 id="希尔排序"><a href="#希尔排序" class="headerlink" title="希尔排序"></a>希尔排序</h4><p>上述三种排序算法的时间复杂度都是$O(n^2)$，而希尔排序是突破这个时间复杂度的第一批算法之一。</p>
<p>其实直接插入排序的效率在某些情况下是非常高效的，这些情况是指记录本来就很少或者待排序的表基本有序的情况，但是这两种情况都是特殊情况，在现实中比较少见。而希尔排序就是通过创造条件来改进直接插入排序的算法。</p>
<p>希尔排序的做法是<strong>将原本有大量记录数的记录进行分组，分割成若干个序列</strong>，这样每个子序列待排序的记录就比较少了，然后就可以对子序列分别进行直接插入排序，<strong>当整个序列基本有序时，再对全体记录进行一次直接插入排序。</strong></p>
<p>这里的<strong>基本有序是指小的关键字基本在前面，大的基本在后面，不大不小的在中间。</strong>像{2,1,3,6,4,7,5,8,9}可以称为基本有序。</p>
<p>这里的关键就是如何进行分割，希尔排序采取的是<strong>跳跃分割的策略：将相距某个“增量”的记录组成一个子序列，这样才能保证在子序列内分别进行直接插入排序后得到的结果是基本有序而不是局部有序。</strong></p>
<p>实现的代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 希尔排序</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ShellSort</span><span class="params">(SqList *L)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i, j,val;</span><br><span class="line">	<span class="keyword">int</span> increment = L-&gt;length;</span><br><span class="line">	<span class="keyword">do</span>&#123;</span><br><span class="line">		<span class="comment">// 增量序列</span></span><br><span class="line">		increment = increment / <span class="number">3</span> + <span class="number">1</span>;</span><br><span class="line">		<span class="keyword">for</span> (i = increment; i &lt;= L-&gt;length - <span class="number">1</span>; i++)&#123;</span><br><span class="line">			<span class="keyword">if</span> (L-&gt;r[i]&lt;L-&gt;r[i - increment])&#123;</span><br><span class="line">				<span class="comment">// 将L-&gt;r[i]插入有序表中,使用val保存待插入的数组元素L-&gt;r[i]</span></span><br><span class="line">				val = L-&gt;r[i];</span><br><span class="line">				<span class="keyword">for</span> (j = i - increment; j &gt;= <span class="number">0</span> &amp;&amp; L-&gt;r[j]&gt;val; j -= increment)</span><br><span class="line">					<span class="comment">// 记录后移，查找插入位置</span></span><br><span class="line">					L-&gt;r[j + increment] = L-&gt;r[j];</span><br><span class="line">				L-&gt;r[j + increment] = val;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; <span class="keyword">while</span> (increment &gt; <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述代码中增量的选取是<code>increment = increment / 3 + 1</code>，实际上增量的选取是非常关键的，现在还没有人找到一种最好的增量序列，但是大量研究表明，当增量序列是$\delta [k] = 2^{t-k+1} - 1 (0\le k \le t \le \lfloor log_2(n+1)\rfloor)$时，可以获得不错的效率，其时间复杂度是$O(n^{\frac{3}{2}})$，要好于直接插入排序的$O(n^2)$。当然，这里需要注意的是<strong>增量序列的最后一个增量值必须等于1才行</strong>。此外，由于记录是跳跃式的移动，<strong>希尔排序是不稳定的排序算法</strong>。</p>
<h4 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h4><p>有一个数量为Size个数的数组A，数组的值范围为(0 - Max)，然后创建一个大小为<code>Max+1</code>的数组B，每个元素都为0.从头遍历A，当读取到A[i]的时候，B[A[i]]的值+1，这样所有的A数组被遍历后，直接扫描B之后，输出表B就可以了。然后再根据B来对A进行排序。</p>
<p>实现代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//获得未排序数组中最大的一个元素值</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">GetMaxVal</span><span class="params">(<span class="keyword">int</span>* arr, <span class="keyword">int</span> len)</span></span><br><span class="line"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> maxVal = arr[<span class="number">0</span>]; <span class="comment">//假设最大为arr[0]</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; len; i++)  <span class="comment">//遍历比较，找到大的就赋值给maxVal</span></span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">if</span> (arr[i] &gt; maxVal)</span><br><span class="line">			maxVal = arr[i];</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> maxVal;  <span class="comment">//返回最大值</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BucketSort</span><span class="params">(<span class="keyword">int</span> *numbers, <span class="keyword">int</span> length)</span></span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (numbers == <span class="literal">NULL</span> || length &lt;= <span class="number">0</span>)&#123;</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="string">"wrong input!"</span>;</span><br><span class="line">		<span class="keyword">return</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">int</span> size = GetMaxVal(numbers,length) + <span class="number">1</span>;</span><br><span class="line">	<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; bucket(size);</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; length + <span class="number">1</span>; i++)&#123;</span><br><span class="line">		bucket[i] = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// 计算数组中每个元素出现的次数</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; length; i++)&#123;</span><br><span class="line">		<span class="keyword">int</span> j = numbers[i];</span><br><span class="line">		bucket[j] += <span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// 排序</span></span><br><span class="line">	<span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++)&#123;</span><br><span class="line">		<span class="keyword">if</span> (bucket[i] &gt; <span class="number">0</span>)&#123;</span><br><span class="line">			<span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; bucket[i]; j++)&#123;</span><br><span class="line">				numbers[count] = i;</span><br><span class="line">				count++;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h4><p>简单选择排序在待排序的$n$个记录中选择一个最小的记录需要比较$n-1$次，这是查找第一个数据，所以需要比较这么多次是比较正常的，但是可惜的是它没有把每一趟的比较结果保存下来，这导致在后面的比较中，实际有许多比较在前一趟中已经做过了。因此，如果可以做到每次在选择到最小记录的同时，并根据比较结果对其他记录做出相应的调整，那样排序的总体效率就会变得很高了。</p>
<p>堆排序(Heap Sort)就是对简单选择排序进行的一种改进，并且效果非常明显。</p>
<blockquote>
<p>堆是具有下列性质的完全二叉树：每个结点的值都大于或等于其左右孩子结点的值，称为最大堆或者大顶堆；或者每个结点的值都小于或等于其左右孩子结点的值，称为最小堆或者小顶堆。</p>
</blockquote>
<p>下图是一个例子，左边的是大顶堆，而右边的是小顶堆。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%951.png" alt=""></p>
<p>而根据堆的定义，可以知道根结点一定是堆中所有结点最大或者最小者。如果按照层遍历的方式给结点从1开始编号，则结点之间满足下列关系：</p>
<script type="math/tex; mode=display">
\begin{cases}
k_i \ge k_{2i} \\\
k_i \ge k_{2i+1}
\end{cases}
或 
\begin{cases}
k_i \le k_{2i} \\\
k_i \le k_{2i+1}
\end{cases}
1 \le i \le \lfloor \frac{n}{2} \rfloor</script><p>如果将上图按照层遍历存入数组，则一定满足上述关系表达，得到的数组如下图所示。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%952.png" alt=""></p>
<blockquote>
<p>堆排序的基本思想是，将待排序的序列构成一个最大堆。此时，整个序列的最大值就是堆顶的根结点。将它移走（其实就是将其与堆数组的末尾元素进行交换，此时末尾元素就是最大值），然后将剩余的$n-1$个序列重新构成一个堆，这样就会得到$n$个元素中的次最大值。如此反复执行，便能得到一个有序序列。</p>
</blockquote>
<p>下面将给出堆排序算法的代码实现。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 已知L-&gt;r[s...m]中记录的关键字除L-&gt;r[s]之外均满足堆的定义</span></span><br><span class="line"><span class="comment">// 本函数调整L-&gt;r[s]的关键字，使L-&gt;r[s..m]成为一个大顶堆</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">HeapAdjust</span><span class="params">(SqList *L, <span class="keyword">int</span> s, <span class="keyword">int</span> m)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> temp, j;</span><br><span class="line">	temp = L-&gt;r[s];</span><br><span class="line">	<span class="keyword">for</span> (j = <span class="number">2</span> * s; j &lt;= m - <span class="number">1</span>; j *= <span class="number">2</span>)&#123;</span><br><span class="line">		<span class="comment">// 沿关键字较大的孩子结点向下筛选</span></span><br><span class="line">		<span class="keyword">if</span> (j &lt; m-<span class="number">1</span> &amp;&amp; L-&gt;r[j] &lt; L-&gt;r[j + <span class="number">1</span>])</span><br><span class="line">			<span class="comment">// j是关键字中较大的记录的下标</span></span><br><span class="line">			++j;</span><br><span class="line">		<span class="keyword">if</span> (temp &gt;= L-&gt;r[j])</span><br><span class="line">			<span class="comment">// 当前值不需要进行调整</span></span><br><span class="line">			<span class="keyword">break</span>;</span><br><span class="line">		L-&gt;r[s] = L-&gt;r[j];</span><br><span class="line">		s = j;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// 插入</span></span><br><span class="line">	L-&gt;r[s] = temp;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 堆排序</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">HeapSort</span><span class="params">(SqList *L)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i;</span><br><span class="line">	<span class="keyword">for</span> (i = L-&gt;length / <span class="number">2</span>; i &gt;= <span class="number">0</span>; i--)&#123;</span><br><span class="line">		<span class="comment">// 将待排序的序列构成一个最大堆</span></span><br><span class="line">		HeapAdjust(L, i, L-&gt;length);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// 开始进行排序</span></span><br><span class="line">	<span class="keyword">for</span> (i = L-&gt;length - <span class="number">1</span>; i &gt; <span class="number">0</span>; i--)&#123;</span><br><span class="line">		<span class="comment">// 将堆顶记录与当前未经排序的子序列的最后一个记录交换</span></span><br><span class="line">		swap(L, <span class="number">0</span>, i);</span><br><span class="line">		<span class="comment">// 重新调整为最大堆</span></span><br><span class="line">		HeapAdjust(L, <span class="number">0</span>, i - <span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从代码中可以看出，堆排序分两步走，首先是将待排序的序列构造成最大堆，这也是<code>HeapSort()</code>中第一个循环所做的事情，第二个循环也就是第二步，进行堆排序，逐步将每个最大值的根结点和末尾元素进行交换，然后再调整成最大堆，重复执行。</p>
<p>而在第一步中构造最大堆的过程中，是从$\lfloor \frac{n}{2} \rfloor$的位置开始进行构造，这是从下往上、从右到左，将每个非叶结点当作根结点，将其和其子树调整成最大堆。</p>
<p>接下来就是分享堆排序的效率了。堆排序的运行时间主要是消耗在初始构造堆和在重建堆时的反复筛选上。</p>
<p>在构建堆的过程中，因为是从完全二叉树的最下层最右边的非叶结点开始构建，将它与其孩子进行比较和若有必要的交换，对每个非叶结点，最多进行两次比较和互换操作，这里需要进行这种操作的非叶结点数目是$\lfloor \frac{n}{2} \rfloor$个，所以整个构建堆的时间复杂度是$O(n)$。</p>
<p>在正式排序的时候，第$i$取堆顶记录重建堆需要用$O(log i)$的时间(完全二叉树的某个结点到根结点的距离是$\lfloor log_2i \rfloor + 1$)，并且需要取$n-1$次堆顶记录，因此，重建堆的时间复杂度是$O(nlogn)$。</p>
<p><strong>所以，总体上来说，堆排序的时间复杂度是$O(nlogn)$。</strong>由于堆排序对原始记录的排序状态并不敏感，因此它无论最好、最坏和平均时间复杂度都是$O(nlogn)$。同样由于记录的比较与交换是跳跃式进行，<strong>堆排序也不是稳定的排序算法。</strong></p>
<p>另外，由于初始构建堆需要的比较次数较多，因此，它并不适合待排序序列个数较少的情况。</p>
<h4 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h4><blockquote>
<p>归并排序(Merging Sort)就是利用归并的思想实现的排序方法，它的原理是假设初始序列有$n$个记录，则可以看成是$n$个有序的子序列，每个子序列的长度为1，然后两两合并，得到$\lceil \frac{n}{2} \rceil$($\lceil x \rceil$表示不小于$x$的最小整数)个长度为2或1的有序子序列；再两两合并，$\cdots \cdots$，如此重复，直至得到一个长度为$n$的有序序列为止，这种排序方法称为2路归并排序。</p>
</blockquote>
<p>下面是介绍实现的代码。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 归并排序,使用递归</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MergeSort</span><span class="params">(SqList *L)</span></span>&#123;</span><br><span class="line">	MSort(L-&gt;r, L -&gt;r, <span class="number">0</span>, L-&gt;length-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 将SR[s..t]归并排序为TR1[s..t]</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MSort</span><span class="params">(<span class="keyword">int</span> SR[], <span class="keyword">int</span> TR1[], <span class="keyword">int</span> s, <span class="keyword">int</span> t)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> m;</span><br><span class="line">	<span class="keyword">int</span> TR2[MAXSIZE];</span><br><span class="line">	<span class="keyword">if</span> (s == t)</span><br><span class="line">		TR1[s] = SR[s];</span><br><span class="line">	<span class="keyword">else</span>&#123;</span><br><span class="line">		<span class="comment">// 将SR[s..t]平分为SR[s...m-1]和SR[m...t]</span></span><br><span class="line">		m = (s + t) / <span class="number">2</span>+<span class="number">1</span>;</span><br><span class="line">		MSort(SR, TR2, s, m-<span class="number">1</span>);</span><br><span class="line">		MSort(SR, TR2, m, t);</span><br><span class="line">		<span class="comment">// 将TR2[s..m-1]和TR2[m..t]归并到TR1[s..t]</span></span><br><span class="line">		Merge(TR2, TR1, s, m-<span class="number">1</span>, t);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 将有序的SR[i..m]和SR[m+1..n]归并为有序的TR[i..n]</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Merge</span><span class="params">(<span class="keyword">int</span> SR[], <span class="keyword">int</span> TR[], <span class="keyword">int</span> i, <span class="keyword">int</span> m, <span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> j, k, l;</span><br><span class="line">	<span class="keyword">for</span> (j = m+<span class="number">1</span>, k = i; i &lt;= m &amp;&amp; j &lt;= n; k++)&#123;</span><br><span class="line">		<span class="comment">// 将SR中记录由小到大并入TR</span></span><br><span class="line">		<span class="keyword">if</span> (SR[i] &lt; SR[j])</span><br><span class="line">			TR[k] = SR[i++];</span><br><span class="line">		<span class="keyword">else</span></span><br><span class="line">			TR[k] = SR[j++];</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> (i &lt;= m)&#123;</span><br><span class="line">		<span class="keyword">for</span> (l = <span class="number">0</span>; l &lt;= m - i; l++)</span><br><span class="line">			<span class="comment">// 将剩余的SR[i..m]复制到TR</span></span><br><span class="line">			TR[k + l] = SR[i + l];</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> (j &lt;= n)&#123;</span><br><span class="line">		<span class="keyword">for</span> (l = <span class="number">0</span>; l &lt;= n - j; l++)</span><br><span class="line">			<span class="comment">// 将剩余的SR[j..n-1]复制到TR</span></span><br><span class="line">			TR[k + l] = SR[j + l];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述代码是一个递归版本的归并排序实现算法，其中函数<code>MSort()</code>的作用是将待排序序列进行分割，然后<code>Merge()</code>函数会对需要归并的序列进行排序并两两归并在一起。</p>
<p><strong>归并排序的时间复杂度是$O(nlogn)$，并且无论是最好、最坏还是平均都是同样的时间性能。</strong>另外，在归并过程中需要与原始记录序列同样数量的存储空间存放归并结果，并且递归时需要深度为$log_2 n$的栈空间，因此空间复杂度是$O(n+logn)$。</p>
<p>另外，归并排序是使用两两比较，不存在跳跃，这在<code>Merge()</code>中的语句<code>if(SR[i]&lt;SR[j])</code>可以看出，<strong>所以归并排序是一个稳定的排序算法。</strong></p>
<p><strong>总体来说，归并排序是一个比较占用内存，但效率高且稳定的算法。</strong></p>
<p>下面会介绍一个非递归版本的归并排序算法实现。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">// 非递归版本的归并排序</span><br><span class="line">void MergeSort2(SqList *L)&#123;</span><br><span class="line">	// 申请额外空间</span><br><span class="line">	int* TR = (int *)malloc(L-&gt;length * sizeof(int));</span><br><span class="line">	int k = 1;</span><br><span class="line">	while (k &lt; L-&gt;length)&#123;</span><br><span class="line">		MergePass(L-&gt;r, TR, k, L-&gt;length);</span><br><span class="line">		// 子序列长度加倍</span><br><span class="line">		k = 2 * k;</span><br><span class="line">		MergePass(TR, L-&gt;r, k, L-&gt;length);</span><br><span class="line">		k = 2 * k;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">// 将SR[]中相邻长度为s的子序列两两归并到TR[]</span><br><span class="line">void MergePass(int SR[], int TR[], int s, int n)&#123;</span><br><span class="line">	int i = 0;</span><br><span class="line">	int j;</span><br><span class="line">	while (i &lt;= n - 2 * s)&#123;</span><br><span class="line">		// 两两归并</span><br><span class="line">		Merge(SR, TR, i, i + s - 1, i + 2 * s - 1);</span><br><span class="line">		i = i + 2 * s;</span><br><span class="line">	&#125;</span><br><span class="line">	if (i &lt; n - s + 1)</span><br><span class="line">		// 归并最后两个子序列</span><br><span class="line">		Merge(SR, TR, i, i + s - 1, n - 1);</span><br><span class="line">	else&#123;</span><br><span class="line">		// 若最后剩下单个子序列</span><br><span class="line">		for (j = i; j &lt;= n - 1; j++)</span><br><span class="line">			TR[j] = SR[j];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line">// 将有序的SR[i..m]和SR[m+1..n]归并为有序的TR[i..n]</span><br><span class="line">void Merge(int SR[], int TR[], int i, int m, int n)&#123;</span><br><span class="line">	int j, k, l;</span><br><span class="line">	for (j = m+1, k = i; i &lt;= m &amp;&amp; j &lt;= n; k++)&#123;</span><br><span class="line">		// 将SR中记录由小到大并入TR</span><br><span class="line">		if (SR[i] &lt; SR[j])</span><br><span class="line">			TR[k] = SR[i++];</span><br><span class="line">		else</span><br><span class="line">			TR[k] = SR[j++];</span><br><span class="line">	&#125;</span><br><span class="line">	if (i &lt;= m)&#123;</span><br><span class="line">		for (l = 0; l &lt;= m - i; l++)</span><br><span class="line">			// 将剩余的SR[i..m]复制到TR</span><br><span class="line">			TR[k + l] = SR[i + l];</span><br><span class="line">	&#125;</span><br><span class="line">	if (j &lt;= n)&#123;</span><br><span class="line">		for (l = 0; l &lt;= n - j; l++)</span><br><span class="line">			// 将剩余的SR[j..n-1]复制到TR</span><br><span class="line">			TR[k + l] = SR[j + l];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>非递归版本的归并排序算法避免了递归时深度为$log_2 n$的栈空间，空间复杂度是$O(n)$，并且避免递归也在时间性能上有一定的提升。应该说，使用归并排序时，尽量考虑用非递归方法。</p>
<h4 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h4><p>在前面介绍的几种排序算法，希尔排序相当于直接插入排序的升级，它们属于插入排序类，而堆排序相当于简单选择排序的升级，它们是属于选择排序类，而接下来介绍的快速排序就是冒泡排序的升级，它们属于交换排序类。</p>
<blockquote>
<p>快速排序(Quick Sort)的基本思想是：通过一趟排序将待排序记录分割成独立的两部分，其中一部分记录的关键字均比另一部分记录的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序的目的。</p>
</blockquote>
<p>下面给出实现的快速排序算法代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 快速排序</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">QuickSort</span><span class="params">(SqList *L)</span></span>&#123;</span><br><span class="line">	QSort(L, <span class="number">0</span>, L-&gt;length - <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 对待排序序列L中的子序列L-&gt;r[low...high]做快速排序</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">QSort</span><span class="params">(SqList *L, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> pivot;</span><br><span class="line">	<span class="keyword">if</span> (low &lt; high)&#123;</span><br><span class="line">		<span class="comment">// 将L-&gt;r[low...high]一分为二，算出枢轴值pivot</span></span><br><span class="line">		pivot = Partition(L, low, high);</span><br><span class="line">		<span class="comment">// 对低子序列递归排序</span></span><br><span class="line">		QSort(L, low, pivot - <span class="number">1</span>);</span><br><span class="line">		<span class="comment">// 对高子序列递归排序</span></span><br><span class="line">		QSort(L, pivot + <span class="number">1</span>, high);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 交换待排序序列L中子表的记录，使枢轴记录到位，并返回其所在位置</span></span><br><span class="line"><span class="comment">// 并使得其之前位置的值小于它，后面位置的值大于它</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Partition</span><span class="params">(SqList *L, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> pivot_key;</span><br><span class="line">	<span class="comment">// 初始值设置为子表的第一个记录</span></span><br><span class="line">	pivot_key = L-&gt;r[low];</span><br><span class="line">	<span class="keyword">while</span> (low &lt; high)&#123;</span><br><span class="line">		<span class="keyword">while</span> (low &lt; high &amp;&amp; L-&gt;r[high] &gt;= pivot_key)</span><br><span class="line">			high--;</span><br><span class="line">		<span class="comment">// 将小于枢轴记录的值交换到低端</span></span><br><span class="line">		swap(L, low, high);</span><br><span class="line">		<span class="keyword">while</span> (low &lt; high &amp;&amp; L-&gt;r[low] &lt;= pivot_key)</span><br><span class="line">			low++;</span><br><span class="line">		<span class="comment">// 将大于枢轴记录的值交换到高端</span></span><br><span class="line">		swap(L, low, high);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> low;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述代码同样是使用了递归，其中<code>Partition()</code>函数要做的就是先选取待排序序列中的一个关键字，然后将其放在一个位置，这个位置左边的值小于它，右边的值都大于它，这样的值被称为枢轴。</p>
<p>快速排序的时间性能取决于快速排序递归的深度。在最优情况下，<code>Partition()</code>每次都划分得很均匀，如果排序$n$个关键字，其递归树的深度技术$\lfloor log_ n \rfloor +1$，即需要递归$log_2n$次，其时间复杂度是$O(nlogn)$。而最坏的情况下，待排序的序列是正序或逆序，得到的递归树是斜树，最终其时间复杂度是$O(n^2)$。</p>
<p><strong>平均情况可以得到时间复杂度是$O(nlogn)$，而空间复杂度的平均情况是$O(logn)$。但是由于关键字的比较和交换是跳跃进行的，所以快速排序也是不稳定排序。</strong></p>
<h5 id="快速排序的优化"><a href="#快速排序的优化" class="headerlink" title="快速排序的优化"></a>快速排序的优化</h5><p>快速排序算法是有许多地方可以优化的，下面给出一些优化的方案。</p>
<h6 id="优化选取枢轴"><a href="#优化选取枢轴" class="headerlink" title="优化选取枢轴"></a>优化选取枢轴</h6><p>枢轴的值太大或者太小都会影响快速排序的性能，一个改进方法是<strong>三数取中</strong>法，即<strong>取三个关键字先进行排序，将中间数作为枢轴，一般是取左端、右端和中间三个数</strong>。</p>
<p>需要在<code>Partition()</code>函数中做出下列修改：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> pivot_key;</span><br><span class="line">	<span class="comment">// 使用三数取中法选取枢轴</span></span><br><span class="line">	<span class="keyword">int</span> m = low + (high - low) / <span class="number">2</span>;</span><br><span class="line">	<span class="keyword">if</span> (L-&gt;r[low] &gt; L-&gt;r[high])</span><br><span class="line">		<span class="comment">// 保证左端最小</span></span><br><span class="line">		swap(L, low, high);</span><br><span class="line">	<span class="keyword">if</span> (L-&gt;r[m] &gt; L-&gt;r[high])</span><br><span class="line">		<span class="comment">// 保证中间较小</span></span><br><span class="line">		swap(L, high, m);</span><br><span class="line">	<span class="keyword">if</span> (L-&gt;r[m] &gt; L-&gt;r[low])</span><br><span class="line">		<span class="comment">// 保证左端较小</span></span><br><span class="line">		swap(L, m, low);</span><br><span class="line"></span><br><span class="line">	pivot_key = L-&gt;r[low];</span><br></pre></td></tr></table></figure>
<p>三数取中对小数组有很大的概率取到一个比较好的枢轴值，但是对于非常大的待排序的序列还是不足以保证得到一个比较好的枢轴值，因此还有一个办法是<strong>九数取中法</strong>，它先从数组中分三次取样，每次去三个数，三个样品各自取出中数，然后从这三个中数当中再取出一个中数作为枢轴。</p>
<h6 id="优化不必要的交换"><a href="#优化不必要的交换" class="headerlink" title="优化不必要的交换"></a>优化不必要的交换</h6><p>优化后的代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">pivot_key = L-&gt;r[low];</span><br><span class="line"><span class="keyword">int</span> temp = pivot_key;</span><br><span class="line"><span class="keyword">while</span> (low &lt; high)&#123;</span><br><span class="line">	<span class="keyword">while</span> (low &lt; high &amp;&amp; L-&gt;r[high] &gt;= pivot_key)</span><br><span class="line">		high--;</span><br><span class="line">	<span class="comment">// 将小于枢轴记录的值交换到低端</span></span><br><span class="line">	<span class="comment">// swap(L, low, high);</span></span><br><span class="line">	<span class="comment">// 采用替换而不是交换的方式进行操作</span></span><br><span class="line">	L-&gt;r[low] = L-&gt;r[high];</span><br><span class="line">	<span class="keyword">while</span> (low &lt; high &amp;&amp; L-&gt;r[low] &lt;= pivot_key)</span><br><span class="line">		low++;</span><br><span class="line">	<span class="comment">// 将大于枢轴记录的值交换到高端</span></span><br><span class="line">	<span class="comment">// swap(L, low, high);</span></span><br><span class="line">	<span class="comment">// 采用替换而不是交换的方式进行操作</span></span><br><span class="line">	L-&gt;r[high] = L-&gt;r[low];</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 将枢轴值替换回L.r[low]</span></span><br><span class="line">L-&gt;r[low] = temp;</span><br><span class="line"><span class="keyword">return</span> low;</span><br></pre></td></tr></table></figure>
<p>这里可以减少多次交换数据的操作，性能上可以得到一定的提高。</p>
<h6 id="优化小数组时的排序方案"><a href="#优化小数组时的排序方案" class="headerlink" title="优化小数组时的排序方案"></a>优化小数组时的排序方案</h6><p>当数组比较小的时候，快速排序的性能其实还不如直接插入排序(直接插入排序是简单排序中性能最好的)。其原因是快速排序使用了递归操作，在有大量数据排序时，递归操作的影响是可以忽略的，但如果只有少数记录需要排序，这个影响就比较大，所以下面给出改进的代码。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#<span class="keyword">define</span> MAX_LENGTH_INSERT_SORT <span class="number">7</span> </span></span><br><span class="line"><span class="comment">// 对待排序序列L中的子序列L-&gt;r[low...high]做快速排序</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">QSort</span><span class="params">(SqList *L, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> pivot;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> ((high - low) &gt; MAX_LENGTH_INSERT_SORT)&#123;</span><br><span class="line">		<span class="comment">// 当high - low 大于常数时用快速排序</span></span><br><span class="line">		<span class="comment">// 将L-&gt;r[low...high]一分为二，算出枢轴值pivot</span></span><br><span class="line">		pivot = Partition(L, low, high);</span><br><span class="line">		<span class="comment">// 对低子序列递归排序</span></span><br><span class="line">		QSort(L, low, pivot - <span class="number">1</span>);</span><br><span class="line">		<span class="comment">// 对高子序列递归排序</span></span><br><span class="line">		QSort(L, pivot + <span class="number">1</span>, high);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span>&#123;</span><br><span class="line">		<span class="comment">// 否则使用直接插入排序</span></span><br><span class="line">		InsertSort(L);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述代码是先进行一个判断，当数组的数量大于一个预设定的常数时，才进行快速排序，否则就进行直接插入排序。这样可以保证最大化地利用两种排序的优势来完成排序工作。</p>
<h6 id="优化递归操作"><a href="#优化递归操作" class="headerlink" title="优化递归操作"></a>优化递归操作</h6><p>递归对性能是有一定影响的，<code>QSort()</code>在其尾部有两次递归操作，如果待排序的序列划分极端不平衡，递归的深度将趋近于$n$，而不是平衡时的$log_2 n$，这就不仅仅是速度快慢的问题了。栈的大小是很有限的，每次递归调用都会耗费一定的栈空间，函数的参数越多，每次递归耗费的空间也越多。因此，如果能减少递归，将会大大提高性能。</p>
<p>下面给出对<code>QSort()</code>实施尾递归优化的代码。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 对待排序序列L中的子序列L-&gt;r[low...high]做快速排序</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">QSort</span><span class="params">(SqList *L, <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> pivot;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> ((high - low) &gt; MAX_LENGTH_INSERT_SORT)&#123;</span><br><span class="line">		<span class="comment">// 当high - low 大于常数时用快速排序</span></span><br><span class="line">		<span class="keyword">while</span> (low &lt; high)&#123;</span><br><span class="line">			<span class="comment">// 将L-&gt;r[low...high]一分为二，算出枢轴值pivot</span></span><br><span class="line">			pivot = Partition(L, low, high);</span><br><span class="line">			<span class="comment">// 对低子序列递归排序</span></span><br><span class="line">			QSort(L, low, pivot - <span class="number">1</span>);</span><br><span class="line">			<span class="comment">// 尾递归</span></span><br><span class="line">			low = pivot + <span class="number">1</span>;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span>&#123;</span><br><span class="line">		<span class="comment">// 否则使用直接插入排序</span></span><br><span class="line">		InsertSort(L);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述代码中使用<code>while</code>循环，并且去掉原来的对高子序列进行递归，改成代码<code>low = privot + 1</code>，那么在进行一次递归后，再进行循环，就相当于原来的<code>QSort(L,privot+1,high);</code>，结果相同，但是从递归变成了迭代，可以缩减堆栈深度，从而提高了整体性能。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>上述总共介绍了7种排序算法，首先是根据排序过程中借助的主要操作，将内排序分为：插入排序、交换排序、选择排序和归并排序，如下图所示。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%953.png" alt=""></p>
<p>事实上，目前还没有十全十美的排序算法，都是各有优点和缺点，即使是快速排序算法，也只是整体上性能优越，它也存在排序不稳定、需要大量辅助空间、对少量数据排序无优势等不足。</p>
<p>下面对这7种算法的各种指标进行对比，如下图所示：</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%954.png" alt=""></p>
<p>从算法的简单性来看，可以分为两类：</p>
<ul>
<li>简单算法：冒泡、简单选择、直接插入。</li>
<li>改进算法：快速、堆、希尔、归并。</li>
</ul>
<p>从平均情况看，快速、堆、归并三种改进算法都优于希尔排序，并远远胜过3种简单算法。</p>
<p>从最好情况看，冒泡和直接插入排序要更好一点，即当待排序序列是基本有序的时候，应该考虑这两种排序算法，而非4种复杂的改进算法。</p>
<p>从最坏情况看，堆和归并排序比其他排序算法都要更好。</p>
<p>从空间复杂度看，归并排序和快速排序都对空间有要求，而其他排序反而都只是$O(1)$的复杂度。</p>
<p>从稳定性上看，归并排序是改进算法中唯一稳定的算法。而不稳定的排序算法有“快些选堆”，即快速、希尔、选择和堆排序四种算法（书中给出的简单选择排序是不稳定的，但是从网上查找资料看到选择排序是一个不稳定的算法）。</p>
<p>排序算法的总结就到这里，实际上还是要根据实际问题来选择适合的排序算法。</p>
<p>全部排序算法的代码可以查看<a href="https://github.com/ccc013/DataStructe-Algorithms_Study/tree/master/SortAlgorithms" target="_blank" rel="external">排序算法实现代码</a>。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这是《大话数据结构》第九章排序算法的知识点总结。</p>
<h4 id="排序的基本概念与分类"><a href="#排序的基本概念与分类" class="headerlink" title="排序的基本概念与分类"></a>排序的基本概念与分类</h4><blockquo]]>
    </summary>
    
      <category term="数据结构" scheme="http://ccc013.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="算法" scheme="http://ccc013.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[图的基本定义]]></title>
    <link href="http://ccc013.github.io/2016/11/08/%E5%9B%BE%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%AE%9A%E4%B9%89/"/>
    <id>http://ccc013.github.io/2016/11/08/图的基本定义/</id>
    <published>2016-11-08T09:18:08.000Z</published>
    <updated>2016-11-08T09:21:46.527Z</updated>
    <content type="html"><![CDATA[<p>这是《大话数据结构》第七章图的基本知识总结，首先是总结图的基本定义和相关的术语，包括有向图，无向图，连通图等术语的定义。</p>
<blockquote>
<p>图(Graph)是由顶点的有穷非空集合和顶点之间的集合组成，通常表示为：<strong>G（V, E）</strong>，其中 G 表示一个图，V 是图 G 中顶点的集合，E 是图 G 中边的集合。</p>
</blockquote>
<p>对于上述图的定义，需要注意的是：</p>
<ul>
<li>线性表中的数据元素被称为元素，树中将数据元素称为结点，而图中数据元素被称为<strong>顶点</strong></li>
<li>线性表可以没有数据元素，称为空表；树也可以没有结点，称为空树。但是图的定义中强调了顶点集合<strong>V</strong>是有穷非空的集合，所以图结构中不能没有顶点。</li>
<li>线性表中，相邻的数据元素之间具有线性关系；树结构中，相邻两层的结点具有层次关系。而<strong>图中，任意两个顶点之间都可能有关系，顶点之间的逻辑关系用边来表示，边集可以是空的。</strong></li>
</ul>
<h5 id="各种图定义"><a href="#各种图定义" class="headerlink" title="各种图定义"></a>各种图定义</h5><p>接下来会介绍各自图的定义，包括无向图与有向图，有向完全图和无向完全图，稀疏与稠密图等。</p>
<blockquote>
<p>无向边： 若顶点$v_i$ 到$v_j$之间的边没有方向，则称这条边为<strong>无向边(Edge)</strong>,用无序偶对$(v_i, v_j)$来表示。</p>
</blockquote>
<p>如果图中任意两个顶点之间的边都是无向边，则称该图是<strong>无向图</strong>。</p>
<p>如下图左边的图就是一个无向图$G_1$，$G_1 = （V_1,{E_1}）$，其中顶点集合 $V_1 = {A,B,C,D}$,边集合是$E_1 = {(A, B), (B, C), (C, D), (D, A), (A, C)}$。</p>
<blockquote>
<p>有向边： 若顶点$v_i$ 到$v_j$之间的边有方向，则称这条边为<strong>有向边，也称为弧(Arc)</strong>。用有序偶$<v_i, v_j="">$来表示，$v_i$称为弧尾，$v_j$称为弧头。</v_i,></p>
</blockquote>
<p>如果图中任意两个顶点之间的边都是有向边，则称该图是<strong>有向图</strong>。</p>
<p>如下图右边的图就是一个有向图 $G_2$，$G_2 =（V_2, {E_2}） $,其中顶点集合 $V_2 = {A,B,C,D}$,边集合是$E_2 = {<a, d="">, <b, a="">, <c, a="">, <b, c="">}$。</b,></c,></b,></a,></p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%9B%BE1.png" alt=""></p>
<p>这里需要注意有向图中有向边的表示是不能随意乱写的，必须是按照定义中$<v_i, v_j="">$，弧尾在前，弧头在后的写法。</v_i,></p>
<blockquote>
<p>图中，若不存在顶点到其自身的边，且同一条边不重复出现，则称这样的图是简单图。</p>
</blockquote>
<p>如下图所示都不是简单图，而我们主要讨论的都是简单图。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%9B%BE2.png" alt=""></p>
<blockquote>
<p><strong>无向完全图</strong>是指在无向图中，任意两个顶点之间都存在边。</p>
</blockquote>
<p>含有$n$个顶点的无向完全图有$\frac{n\times (n-1)}{2}$条边。</p>
<blockquote>
<p><strong>有向完全图</strong>是指在有向图中，任意两个顶点之间都存在方向互为相反的两条弧。</p>
</blockquote>
<p>含有$n$个顶点的有向完全图有$n\times (n-1)$条边。</p>
<p>由此可以得到一个结论：</p>
<p><strong>对于具有$n$个顶点和$e$条边数的图，无向图有$0 \le e \le \frac{n\times (n-1)}{2}$, 有向图有$0 \le e \le n \times (n-1)$</strong>。</p>
<blockquote>
<p>有很少边或弧的图称为稀疏图，反之称为稠密图。</p>
</blockquote>
<p>这里的稀疏与稠密都是相对而言的。</p>
<blockquote>
<p>与图的边或弧相关的数值称为<strong>权(Weight)</strong>，它可以表示从一个顶点到另一个顶点的距离或耗费。这种带权的图通常称为<strong>网(Network)</strong>。</p>
</blockquote>
<p>下图就是一个带权的图的例子。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%9B%BE3.png" alt=""></p>
<blockquote>
<p>假设有两个图 $G = （V，{E}）$，和 $G^\prime = （V^\prime,  {E^\prime}） $,如果$V^\prime \subseteq V$, 且 $E^\prime \subseteq E$,则称$G^\prime$是$G$的子图。</p>
</blockquote>
<p>下面展示了无向图和有向图与其子图。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%9B%BE4.png" alt=""></p>
<h5 id="图的顶点与边的关系"><a href="#图的顶点与边的关系" class="headerlink" title="图的顶点与边的关系"></a>图的顶点与边的关系</h5><blockquote>
<p>在无向图 $G=(V,{E})$，如果边 $(v, v^\prime) \in E$,则称顶点$v和v^\prime$互为<strong>邻接点(Adjacent)</strong>,即$v 和 v^\prime$相邻接。边$(v, v^\prime)$依附(incident)于顶点$v 和 v^\prime$，或者说$(v,v^\prime)$与顶点$v 和 v^\prime$相关联。<strong>顶点$v$的度(Degree)是和$v$相关联的边的数目，记为TD($v$)。</strong></p>
</blockquote>
<p>例如对于上图中上方的无向图，顶点A与B互为邻接点，边(A, B)依附于顶点A与B上，顶点A的度为3。通过计算，可以知道，<strong>无向图的边数是各顶点度数和的一半，即$e = \frac{1}{2} \sum_{i=1}^n TD(v_i)$。</strong></p>
<blockquote>
<p>有向图 $G=(V,{E})$,如果弧$<v,v^\prime> \in E$, 则称顶点$v$邻接到顶点$v^\prime$，顶点$v^\prime$邻接自顶点$v$。弧$<v,v^\prime>$和顶点$v,v^\prime$相关联。<strong>以顶点$v$为头的弧的数目称为$v$的入度(InDegree),记为$ID(v)$；以顶点$v$为尾的弧的数目称为$v$的出度(OutDegree),记为$OD(v)$,因此顶点$v$的度为$TD(v) = ID(v) + OD(v)$。</strong> </v,v^\prime></v,v^\prime></p>
</blockquote>
<p>例如对上图中下方的有向图，顶点A的入度是2（从B到A的弧，C到A的弧），出度是1（从A到D的弧），所以顶点A的度是3。同样通过计算，可以得到$e =\sum_{i=1}^n ID(v_i) = \sum_{i=1}^n OD(v_i) $。</p>
<blockquote>
<p>在无向图 $G=(V,{E})$中从顶点$v$到$v^\prime$的<strong>路径(Path)</strong>是一个顶点序列$(v=v_{i,0},v_{i,1},\cdots,v_{i,m}=v^\prime),其中(v_{i,j-1},v_{i,j}) \in E, 1 \le j \le m$。</p>
</blockquote>
<p>下图就展示了顶点B到顶点D的四种不同路径。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%9B%BE5.png" alt=""></p>
<p>如果$G$是有向图，则路径也是有向的，顶点序列应满足$<v_{i,j-1}, v_{i,j}=""> \in E, 1 \le j \le m$。如下图所示，顶点B到D右两种路径，而顶点A到B就不存在路径。</v_{i,j-1},></p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%9B%BE6.png" alt=""></p>
<blockquote>
<p><strong>路径的长度是路径上的边或弧的数目。</strong></p>
</blockquote>
<p>如上图有向图中，左侧的路径长度是2，经过两条弧，而右侧的路径长度是3，经过3条弧。</p>
<blockquote>
<p>第一个顶点到最后一个顶点相同的路径称为<strong>回路或环(Cycle)</strong>。序列中顶点不重复出现的路径称为<strong>简单路径</strong>。除了第一个顶点和最后一个顶点之外，其余顶点不重复出现的回路，称为<strong>简单回路或简单环</strong>。</p>
</blockquote>
<p>下图中，两个图都是一个环，但左侧的图是一个简单环，而右侧图中顶点C重复出现，因此它不是一个简单环。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%9B%BE7.png" alt=""></p>
<h5 id="连通图相关术语"><a href="#连通图相关术语" class="headerlink" title="连通图相关术语"></a>连通图相关术语</h5><p>接下来会介绍有关连通图的定义和相关术语。</p>
<blockquote>
<p>无向图$G$中，如果从顶点$v$到$v^\prime$有路径，则称$v$和$v^\prime$是<strong>连通的。</strong>如果对于图中<strong>任意两个顶点$v_i、v_j \in V$，$v_i$和$v_j$都是连通的，则称$G$是连通图。</strong></p>
</blockquote>
<p><strong>无向图中的极大连通子图称为连通分量</strong>。这个连通分量的前提条件有：</p>
<ul>
<li>是子图；</li>
<li>子图是要连通的；</li>
<li>连通子图要有极大顶点数；</li>
<li>具有极大顶点数的连通子图包含依附于这些顶点的所有边。</li>
</ul>
<blockquote>
<p>有向图$G$中，如果对于每一对$v_i,v_j \in V, v_i \neq v_j$,从$v_i$到$v_j$和从$v_j$到$v_i$都存在路径，则称$G$是<strong>强连通图</strong>。有向图中的<strong>极大强连通子图</strong>称做有向图的<strong>强连通分量</strong>。</p>
</blockquote>
<p>如下图所示，图1并不是强连通图，因为顶点A到顶点D存在路径，但不存在从顶点D到顶点A的路径。图2就是强连通图，而且显然图2是图1的极大强连通子图，即是它的强连通分量。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%9B%BE8.png" alt=""></p>
<p>接下来是连通图的生成树定义：</p>
<blockquote>
<p>一个连通图的生成树是一个极小的连通子图，它含有图中全部的n个顶点，但只有足以构成一棵树的 n-1 条边。</p>
</blockquote>
<p>如下图所示，图1是一个普通图，但是显然它不是生成树，当去掉两条构成环的边后，如图2或图3，就满足生成树的条件了，即n个顶点和n-1条边且连通的定义，它们都是一棵生成树。由此可以得知，<strong>如果一个图有n个顶点和小于n-1条边，则是非连通图；如果它多于n-1条边，则必定构成一个环。</strong>但有n-1条边也不一定是生成树，如图4。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%9B%BE9.png" alt=""></p>
<p>接下来是有向树的定义：</p>
<blockquote>
<p>如果一个有向图中<strong>恰有一个顶点的入度为0，其余顶点的入度均为1</strong>，则是一棵有向树。</p>
</blockquote>
<p>这里入度为0的顶点就相当于树的根结点，而其余顶点的入度都是1，是因为树的非根结点的双亲只有1个。</p>
<blockquote>
<p>一个有向图的生成森林由<strong>若干棵有向树组成，含有图中全部顶点</strong>，但<strong>只有足以构成若干棵不相交的有向树的弧。</strong></p>
</blockquote>
<p>如下图所示，图1是一个有向图，再去掉一些弧后，得到图2和图3，也就是分解成两棵有向树，即图2和图3，而这两棵有向树也是图1有向图的生成森林。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%9B%BE10.png" alt=""></p>
<h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><p>图的基本定义就简单总结到这里，图的术语还是不比树的少，需要多看几遍，同时多使用，接下来会继续总结图的存储结构、遍历等知识点。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这是《大话数据结构》第七章图的基本知识总结，首先是总结图的基本定义和相关的术语，包括有向图，无向图，连通图等术语的定义。</p>
<blockquote>
<p>图(Graph)是由顶点的有穷非空集合和顶点之间的集合组成，通常表示为：<strong>G（V, E）</str]]>
    </summary>
    
      <category term="数据结构" scheme="http://ccc013.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="算法" scheme="http://ccc013.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[二叉树2]]></title>
    <link href="http://ccc013.github.io/2016/10/28/%E4%BA%8C%E5%8F%89%E6%A0%912/"/>
    <id>http://ccc013.github.io/2016/10/28/二叉树2/</id>
    <published>2016-10-28T09:28:47.000Z</published>
    <updated>2016-10-28T09:31:15.149Z</updated>
    <content type="html"><![CDATA[<p>这是《大话数据结构》第六章树的内容，这里总结线索二叉树，二叉树、树和森林的转换以及赫夫曼树的知识点，其中赫夫曼树的总结是在做<a href="http://www.nowcoder.com/918856" target="_blank" rel="external">牛客网</a>上的数据结构选择题的时候，遇到有关这个知识点的时候总结的内容，主要是通过百度得到的，也有结合书本的内容。</p>
<h4 id="线索二叉树"><a href="#线索二叉树" class="headerlink" title="线索二叉树"></a>线索二叉树</h4><h5 id="线索二叉树原理"><a href="#线索二叉树原理" class="headerlink" title="线索二叉树原理"></a>线索二叉树原理</h5><p>首先如下图所示的二叉树，其中<code>^</code>符号表示空指针域，对于一个有<code>n</code>个结点的二叉链表，每个结点都有指向左右孩子的两个指针域，所以一共有<code>2n</code>个指针域。而<code>n</code>个结点的二叉树是有<code>n-1</code>条分支线数，也就是存在$2n-(n-1)=n+1$个空指针域，这些空间是不存储任何东西，也就是浪费内存的资源。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E7%BA%BF%E7%B4%A2%E4%BA%8C%E5%8F%89%E6%A0%911.png" alt=""></p>
<p>另一方面，在对上图的二叉树做中序遍历时，可以得到<code>HDIBJEAFCG</code>这样的字符序列，通过这样的遍历，可以知道，结点<code>I</code>的前驱是<code>D</code>，后继是<code>B</code>。即我们可以知道任意一个结点的前驱和后继分别是哪个，但这是在经过遍历之后的结果，即每次使用都需要先遍历一次，才可以知道任意结点的前驱和后继。</p>
<p>综合上述两种情况，为了更好利用内存资源，节省时间，就有了<strong>线索二叉树</strong>了，我们将<strong>指向前驱和后继的指针称为线索，加上线索的二叉链表称为线索链表，相应的二叉树就是线索二叉树了</strong>。</p>
<p><strong>线索化是对二叉树以某种次序遍历使其变为线索二叉树的过程。</strong></p>
<p>我们对上图的二叉树按照中序遍历的方式进行线索化，可以得到下图，其中虚线箭头是表示后继，实线箭头是前驱。这里设置二叉树的<strong>左指针是指向前驱，右指针指向后继。</strong></p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E7%BA%BF%E7%B4%A2%E4%BA%8C%E5%8F%89%E6%A0%912.png" alt=""></p>
<p>但是增加了线索后，需要解决的问题就是如何判断当前结点的左指针是指向其左孩子，还是前驱呢。这里就需要在每个结点增加两个标志域<code>ltag</code>和<code>rtag</code>，用来表示左右指针指向的是左右孩子还是前驱或者后继。</p>
<h5 id="线索二叉树结构实现"><a href="#线索二叉树结构实现" class="headerlink" title="线索二叉树结构实现"></a>线索二叉树结构实现</h5><p>二叉树的线索存储结构定义如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// Link == 0 表示指向左右孩子指针； Thread == 1 表示指向前驱或者后继的线索</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">enum</span> &#123;Link, Thread&#125; PointerTag;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 二叉树线索存储结点结构</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> BiThrNode</span><br><span class="line">&#123;</span><br><span class="line">	<span class="comment">// 结点数据  </span></span><br><span class="line">	TElemType data;</span><br><span class="line">    <span class="keyword">struct</span> BiThrNode *lchild, *rchild;</span><br><span class="line">    PointerTag LTag;</span><br><span class="line">    PointerTag RTag;</span><br><span class="line">&#125;  BiThrNode, *BiThrTree;</span><br></pre></td></tr></table></figure>
<p><strong>线索化的实质就是将二叉链表中的空指针改为指向前驱或者后继的线索</strong>，因此线索化的过程就是在遍历的过程中修改空指针的过程。</p>
<p>下面是中序遍历线索化的递归函数代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 全局变量，始终指向刚刚访问过的结点</span></span><br><span class="line">BiThrTree pre;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">InThreading</span><span class="params">(BiThrTree p)</span></span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(p)&#123;</span><br><span class="line">    InThreading(p-&gt;lchild);</span><br><span class="line">    <span class="keyword">if</span>(!p-&gt;lchild)&#123;</span><br><span class="line">      <span class="comment">// 没有左孩子</span></span><br><span class="line">      p-&gt;LTag = Thread;</span><br><span class="line">      p-&gt;lchild = pre;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(!p-&gt;rchild)&#123;</span><br><span class="line">      <span class="comment">// 没有右孩子</span></span><br><span class="line">      p-&gt;RTag = Thread;</span><br><span class="line">      <span class="comment">// 指向后继，也就是当前结点p</span></span><br><span class="line">      p-&gt;rchild = p;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 保持 pre 指向p的前驱</span></span><br><span class="line">    pre = p;</span><br><span class="line">    InThreading(p-&gt;rchild);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过上述代码可以得到线索二叉树，而对它进行遍历会发现相当于是操作一个双向链表一样。同样是在二叉线索链表上添加一个头结点，如下图所示：</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E7%BA%BF%E7%B4%A2%E4%BA%8C%E5%8F%89%E6%A0%913.png" alt=""></p>
<p>这里令二叉树的中序序列中的第一个结点<code>H</code>的左指针和最后一个结点<code>G</code>的右指针指向头结点，令头结点的左指针指向根结点，右指针指向结点<code>G</code>。这样做的好处是我们既可以从第一个结点开始顺其后继进行遍历，也可以从最后一个结点开始顺前驱进行遍历。</p>
<p>遍历的代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">InOrderTraverse_Thr</span><span class="params">(BiThrTree T)</span></span>&#123;</span><br><span class="line">  BiThrTree p;</span><br><span class="line">  <span class="comment">// p 指向根结点</span></span><br><span class="line">  p = T-&gt;lchild;</span><br><span class="line">  <span class="keyword">while</span>(p != T)&#123;</span><br><span class="line">    <span class="comment">// 空树或者遍历结束时，p == T</span></span><br><span class="line">    <span class="keyword">while</span>(p-&gt;LTag == Link)</span><br><span class="line">      <span class="comment">// 循环到中序遍历序列的第一个结点</span></span><br><span class="line">      p = p-&gt;lchild;</span><br><span class="line">    <span class="comment">// 显示结点数据，也可以实现其他操作</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%c"</span>, p-&gt;data);</span><br><span class="line">    <span class="keyword">while</span>(p-&gt;RTag == Thread &amp;&amp; p-&gt;rchild != T)&#123;</span><br><span class="line">      <span class="comment">// 根据线索，寻找后继结点，并输出数值或者进行其他操作</span></span><br><span class="line">      p = p-&gt;rchild;</span><br><span class="line">      <span class="built_in">printf</span>(<span class="string">"%c"</span>, p-&gt;data);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// p 指向当前结点的右孩子，暂时结束了根据线索来寻找后继结点</span></span><br><span class="line">    p = p-&gt;rchild;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>线索二叉链表的存储结构适用于<strong>如果所用的二叉树需要经常遍历或查找结点时需要某种遍历序列中的前驱和后继。</strong></p>
<h4 id="树、森林与二叉树的转换"><a href="#树、森林与二叉树的转换" class="headerlink" title="树、森林与二叉树的转换"></a>树、森林与二叉树的转换</h4><h5 id="树转换为二叉树"><a href="#树转换为二叉树" class="headerlink" title="树转换为二叉树"></a>树转换为二叉树</h5><p>树转换为二叉树的步骤如下：</p>
<ol>
<li><strong>加线</strong>。在所有兄弟结点之间加一条连线。</li>
<li><strong>去线</strong>。对树中每个结点，只保留<strong>它与第一个孩子结点的连线</strong>，删除它与其他孩子结点之间的连线。</li>
<li><strong>层次调整</strong>。以树的根结点为轴心，将整棵树顺时针旋转一定角度，使之结构层次分明。<strong>注意第一个孩子是二叉树结点的左孩子，兄弟转换过来的孩子是结点的右孩子。</strong></li>
</ol>
<p>上述步骤可以如下图所示一样：</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E7%BA%BF%E7%B4%A2%E4%BA%8C%E5%8F%89%E6%A0%914.png" alt=""></p>
<h5 id="森林转换为二叉树"><a href="#森林转换为二叉树" class="headerlink" title="森林转换为二叉树"></a>森林转换为二叉树</h5><p>步骤如下：</p>
<ol>
<li>将每棵树先转为二叉树；</li>
<li>第一棵二叉树不动，从第二棵二叉树开始，<strong>依次把后一棵二叉树的根结点作为前一棵二叉树的根结点的右孩子，用线连接起来。</strong>当所有的二叉树连接起来后就得到了由森林转换来的二叉树。</li>
</ol>
<p>下图就是一个森林转为二叉树的例子。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E7%BA%BF%E7%B4%A2%E4%BA%8C%E5%8F%89%E6%A0%915.png" alt=""></p>
<h5 id="二叉树转换为树"><a href="#二叉树转换为树" class="headerlink" title="二叉树转换为树"></a>二叉树转换为树</h5><p>二叉树转为树是树转为二叉树的逆过程，具体步骤如下：</p>
<ol>
<li>加线。若某结点的左孩子存在，则将其左孩子的所有右孩子结点都与当前结点连接起来。</li>
<li>去线。删除原来二叉树中所有结点与其右孩子结点的连线。</li>
<li>层次调整。使之结构层次分明。</li>
</ol>
<p>下图是一个例子。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E7%BA%BF%E7%B4%A2%E4%BA%8C%E5%8F%89%E6%A0%916.png" alt=""></p>
<h5 id="二叉树转为森林"><a href="#二叉树转为森林" class="headerlink" title="二叉树转为森林"></a>二叉树转为森林</h5><p>判断一棵二叉树能够转为森林还是一棵树的方法很简单，就是<strong>看其根结点是否有右孩子，如果有就是森林，没有就是一棵树。</strong></p>
<p>转换为森林的步骤如下：</p>
<ol>
<li>从根结点开始，若右孩子存在，则把与右孩子结点的连线删除，再查看分离后的二叉树，若右孩子存在，则连线删除，如此重复，直到所有右孩子连线都删除为止，得到分离后的二叉树。</li>
<li>将所有二叉树转为树即可。</li>
</ol>
<p>下图是一个例子。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E7%BA%BF%E7%B4%A2%E4%BA%8C%E5%8F%89%E6%A0%917.png" alt=""></p>
<h5 id="树与森林的遍历"><a href="#树与森林的遍历" class="headerlink" title="树与森林的遍历"></a>树与森林的遍历</h5><p>最后是介绍树和森林的遍历问题。</p>
<h6 id="树的遍历"><a href="#树的遍历" class="headerlink" title="树的遍历"></a>树的遍历</h6><p>树的遍历分为两种方式：</p>
<ol>
<li>先根遍历树，即先访问树的根结点，然后依次先根遍历根的每棵子树。</li>
<li>后根遍历，即先依次后根遍历每棵子树，然后再访问根结点。</li>
</ol>
<p>如对下图的树，它的先根遍历序列是<code>ABEFCDG</code>，后根遍历序列是<code>EFBCGDA</code>。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E7%BA%BF%E7%B4%A2%E4%BA%8C%E5%8F%89%E6%A0%918.png" alt=""></p>
<h6 id="森林的遍历"><a href="#森林的遍历" class="headerlink" title="森林的遍历"></a>森林的遍历</h6><p>森林的遍历也是分两种：</p>
<ol>
<li><strong>前序遍历：</strong>先访问森林中第一棵树的根结点，然后再依次先根遍历根的每棵子树，再依次用同样方式遍历除去第一棵树的剩余树构成的森林。比如对于上述二叉树转为森林的例子中最后得到的三棵树的森林，前序遍历的序列是<code>ABCDEFGHJI</code>。</li>
<li><strong>后序遍历：</strong>是先访问森林中的第一棵树，然后用后根遍历的方法遍历每一棵子树，然后再访问根结点，再依次用同样方式遍历除去第一棵树的剩余树构成的森林。同样还是刚才的例子，后根遍历的序列是<code>BCDAFEJHIG</code>。</li>
</ol>
<p>对照上述例子中的二叉树的前序和中序遍历结果可以发现，<strong>森林的前序遍历和二叉树的前序遍历结果相同，森林的后序遍历和二叉树的中序遍历结果相同。</strong>同时，<strong>当以二叉链表作树的存储结构时</strong>，<strong>树的先根遍历和后根遍历完全可以借用二叉树的前序遍历和中序遍历的算法来实现。</strong></p>
<h4 id="赫夫曼树"><a href="#赫夫曼树" class="headerlink" title="赫夫曼树"></a>赫夫曼树</h4><blockquote>
<p>定义：给定n个权值作为n个叶子结点，构造一棵二叉树，若带权路径长度达到最小，<strong>称这样的二叉树为最优二叉树，也称为赫夫曼树(Huffman Tree)。</strong>赫夫曼树是带权路径长度最短的树，权值较大的结点离根较近。</p>
</blockquote>
<p>假设有$n$个权值，则构造出的赫夫曼树有$n$个叶子结点。 n个权值分别设为 $w_1,w_2,\ldots,w_n$，则赫夫曼树的构造规则为：</p>
<ol>
<li>将$w_1,w_2,\ldots,w_n$看成是有$n$ 棵树的森林(每棵树仅有一个结点)；</li>
<li>在森林中选出两个根结点的权值最小的树合并，作为一棵新树的左、右子树，且新树的根结点权值为其左、右子树根结点权值之和；</li>
<li>从森林中删除选取的两棵树，并将新树加入森林；</li>
<li>重复2、3步，直到森林中只剩一棵树为止，该树即为所求得的赫夫曼树。</li>
</ol>
<p>赫夫曼树的性质有：</p>
<ul>
<li>在一棵树中，从一个结点往下可以达到的孩子或孙子结点之间的通路，称为路径。通路中分支的数目称为路径长度。若规定根结点的层数为1，则从<strong>根结点到第L层结点的路径长度为L-1</strong>。</li>
<li>若将树中结点赋给一个有着某种含义的数值，则这个数值称为该结点的权。<strong>结点的带权路径长度为：从根结点到该结点之间的路径长度与该结点的权的乘积。</strong></li>
<li>树的带权路径长度规定为<strong>所有叶子结点的带权路径长度之和</strong>，记为WPL</li>
<li><em>赫夫曼树的形状是不唯一的，但是它的带权路径长度WPL是唯一的。*</em></li>
</ul>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>这部分内容是之前看《数据结构算法与应用：C++语言描述》时没有记录到的知识点，但是在做有关树的练习题的时候却有涉及到，比如线索二叉树和赫夫曼树，特别是后者，一般会考察如何构造赫夫曼树以及求其带权路径长度。刚好在《大话数据结构》中看到，就做下笔记，总结下。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这是《大话数据结构》第六章树的内容，这里总结线索二叉树，二叉树、树和森林的转换以及赫夫曼树的知识点，其中赫夫曼树的总结是在做<a href="http://www.nowcoder.com/918856" target="_blank" rel="external">牛客网]]>
    </summary>
    
      <category term="数据结构" scheme="http://ccc013.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="算法" scheme="http://ccc013.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[搜索树1-二叉搜索树]]></title>
    <link href="http://ccc013.github.io/2016/08/31/%E6%90%9C%E7%B4%A2%E6%A0%911-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"/>
    <id>http://ccc013.github.io/2016/08/31/搜索树1-二叉搜索树/</id>
    <published>2016-08-31T00:46:46.000Z</published>
    <updated>2016-10-25T11:50:21.630Z</updated>
    <content type="html"><![CDATA[<p>继续是《数据结构算法与应用：C++语言描述》的笔记，这是第11章搜索树的内容。</p>
<p>本节首先介绍的是二叉搜索树的内容。</p>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>  在<a href="http://ccc013.github.io/2016/07/27/%E8%B7%B3%E8%A1%A8-%E6%95%A3%E5%88%971-%E5%AD%97%E5%85%B8-%E8%B7%B3%E8%A1%A8/">跳表&amp;散列1-字典&amp;跳表</a>介绍了抽象数据类型<strong>Dictionary</strong>，从中可以发现当用<a href="http://ccc013.github.io/2016/08/07/%E8%B7%B3%E8%A1%A8-%E6%95%A3%E5%88%972-%E6%95%A3%E5%88%97/">散列</a>来描述一个字典时，字典操作（包括插入、删除和搜索）所需要的平均时间是$\theta(1)$。而这些操作最坏情况下的时间正比于字典中的元素个数$n$。如果扩充字典的抽象数据类型描述，增加以下操作，那么散列将不能再提供比较好的评价性能：<br>  1) 按关键值的升序输出字典元素；<br>  2）按升序找到第k个元素；<br>  3）删除第k个元素。</p>
<p>  为了执行操作1），需要从表中取出数据，将它们排序后输出。如果使用除数是D的链表，那么能在$\theta(D+n)$的时间内取出元素，在$O(nlogn)$时间内完成排序和$\theta(n)$时间内输出，因此共需时间$O(D+nlogn)$。如果对散列使用线性开型寻址，则取出元素所需时间是$\theta(b)$,b是桶的个数，这时需要时间是$O(b+nlogn)$。<br>  如果使用链表，操作2）和3）可以在$O(D+n)$的时间内完成，如果使用线性开型寻址，它们可以在$\theta(b)$时间内完成。</p>
<p>  如果使用平衡搜索树，那么对字典的基本操作（搜索、插入和删除）能够在$O(logn)$的时间内完成，操作1）能在$\theta(n)$的时间内完成。通过使用带索引的平衡搜索树，也能够在$O(logn)$的时间内完成操作2）和3）。</p>
<p>  在学习平衡树之前，首先来看一种叫做二叉搜索树的简单结构。</p>
<blockquote>
<p>定义 [二叉搜索树] 二叉搜索树(binary search tree)是一棵可能为空的二叉树，一棵非空的二叉搜索树满足以下特征：<br>1）每个元素有一个关键值，并且没有任意两个元素有相同的关键值；因此，所有的关键值都是唯一的。<br>2）根节点左子树的关键值（如果有的话）小于根节点的关键值。<br>3）根节点右子树的关键值（如果有的话）大于根节点的关键值。<br>4）根节点的左右子树也都是儿茶搜索树。</p>
</blockquote>
<p>下图11-1给出3个含有不同关键值的二叉树，其中11-1a的二叉树满足了上述特征1-3，但是不满足特征4，而11-b和11-c的二叉树则是二叉搜索树。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%911.png" alt="此处输入图片的描述"></p>
<p>在放弃二叉搜索树中所有元素必须拥有不同关键值的要求，然后用小于等于代替特征2）中的小于，用大于等于代替特征3）中的大于，这样就可以得到一棵<strong>有重复值的二叉搜索树</strong>。</p>
<p><strong>带索引的二叉搜索树源于普通的二叉搜索树，它只是在每个节点中添加一个LeftSize域，这个域的值是该节点左子树的元素个数加1。</strong>下图11-2是两棵带索引的二叉搜索树。注意，LeftSize同时给出了一个元素在子树中的排名。</p>
<p><img src="http://7xrluf.com1.z0.glb.clouddn.com/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%912.png" alt="此处输入图片的描述"></p>
<h3 id="类BSTree"><a href="#类BSTree" class="headerlink" title="类BSTree"></a>类BSTree</h3><p>  可以从<a href="http://ccc013.github.io/2016/08/18/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E5%AE%9E%E7%8E%B0/">二叉树的基本概念和实现</a>中介绍的类<strong>BinaryTree</strong>中派生类<strong>BSTree</strong>,这样可以大大简化类BSTree的设计，实现如下程序所示。另外，为了访问<strong>BinaryTree</strong>类的私有成员<strong>root</strong>，需要将类<strong>BSTree</strong>定义为<strong>BinaryTree</strong>的友元。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">class</span> E,<span class="keyword">class</span> K&gt;</span><br><span class="line"><span class="keyword">class</span> BSTree : <span class="keyword">public</span> BinaryTree&lt;E&gt;&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">Search</span><span class="params">(<span class="keyword">const</span> K&amp;k, E&amp; e)</span> <span class="keyword">const</span></span>;</span><br><span class="line">    BSTree&lt;E, K&gt;&amp; Insert(<span class="keyword">const</span> E&amp; e);</span><br><span class="line">    BSTree&lt;E, K&gt;&amp; Delete(<span class="keyword">const</span> K&amp;k, E&amp; e);</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Ascend</span><span class="params">()</span></span>&#123; InOutput(); &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>下面给出搜索元素的代码实现：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">class</span> E,<span class="keyword">class</span> K&gt;</span><br><span class="line"><span class="keyword">bool</span> BSTree&lt;E, K&gt;::Search(<span class="keyword">const</span> K&amp;k, E &amp;e) <span class="keyword">const</span>&#123;</span><br><span class="line">    <span class="comment">// 搜索与k匹配的元素</span></span><br><span class="line">    BinaryTreeNode&lt;E&gt; *p = root;</span><br><span class="line">    <span class="keyword">while</span> (p)&#123;</span><br><span class="line">        <span class="keyword">if</span> (k &lt; p-&gt;data)</span><br><span class="line">            p = p-&gt;LeftChild;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (k&gt;p-&gt;data)</span><br><span class="line">            p = p-&gt;RightChild;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="comment">// 找到元素</span></span><br><span class="line">            e = p-&gt;data;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>若在二叉搜索树中插入一个新元素e，首先要验证e的关键值与树中已有元素的关键值是否相同，这可以通过用e的关键值对二叉树进行搜索来实现。如果搜索不成功，那么新元素将被插入到搜索的中端点，下面给出插入函数的代码实现：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">class</span> E, <span class="keyword">class</span> K&gt;</span><br><span class="line">BSTree&lt;E, K&gt;&amp; BSTree&lt;E, K&gt;::Insert(<span class="keyword">const</span> E&amp; e)&#123;</span><br><span class="line">    <span class="comment">// 如果不出现重复，则插入e</span></span><br><span class="line">    BinaryTreeNode&lt;E&gt; *p = root, *pp = <span class="number">0</span>;   <span class="comment">// p是搜索节点，pp是p的父节点</span></span><br><span class="line">    <span class="comment">// 寻找插入点</span></span><br><span class="line">    <span class="keyword">while</span> (p)&#123;</span><br><span class="line">        pp = p;</span><br><span class="line">        <span class="keyword">if</span> (e &lt; p-&gt;data)</span><br><span class="line">            p = p-&gt;LeftChild;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (e&gt;p-&gt;data)</span><br><span class="line">            p = p-&gt;RightChild;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="comment">// 出现重复</span></span><br><span class="line">            <span class="keyword">throw</span> BadInput();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    BinaryTreeNode&lt;E&gt; *r = <span class="keyword">new</span> BinaryTreeNode&lt;E&gt;(e);</span><br><span class="line">    <span class="keyword">if</span> (root)&#123;</span><br><span class="line">        <span class="keyword">if</span> (e &lt; pp-&gt;data)</span><br><span class="line">            pp-&gt;LeftChild = r;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            pp-&gt;RightChild = r;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        root = r;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>对于删除操作，对包含被删除元素的节点p有三种情况：1）p是叶节点；2）p只有一个非空子树；3）p有两个非空子树。</p>
<p>对于第一种情况可以采用直接丢弃叶节点的方法来处理。</p>
<p>对于第二种情形，如果p没有父节点，即p是根节点，则将p丢弃，p的唯一孩子成为新的搜索树的根节点；如果p有父节点pp，则修改pp的指针，使其指向p的唯一孩子，然后删除节点p。</p>
<p>最后，对于第三种情形，<strong>只需要将元素替换成它的左子树中的最大元素或者右子树中的最小元素。</strong>注意，必须确保右子树中的最小元素以及左子树中的最大元素即不会在没有子树的节点中，也不会在只有一个子树的节点中。可以按下述方法来查找到左子树中的最大元素：<strong>首先移动到子树的根，然后沿着各节点的右孩子指针移动，直到右孩子指针为0为止。</strong>类似地，也可以找到右子树的最小元素：<strong>首先移动到子树的根，然后沿着各节点的左孩子指针移动，直到左孩子指针为0为止。</strong></p>
<p>下面程序给出删除操作实现，它一般使用左子树的最大元素来进行替换。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">class</span> E,<span class="keyword">class</span> K&gt;</span><br><span class="line">BSTree&lt;E, K&gt;&amp; BSTree&lt;E, K&gt;::Delete(<span class="keyword">const</span> K&amp; k, E&amp; e)&#123;</span><br><span class="line">    <span class="comment">// 删除关键值是k的元素，并将其放入e</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将p指向关键值是k的节点</span></span><br><span class="line">    BinaryTreeNode&lt;E&gt; *p = root, *pp = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (p &amp;&amp; p-&gt;data != k)&#123;</span><br><span class="line">        pp = p;</span><br><span class="line">        <span class="keyword">if</span> (k &lt; p-&gt;data)</span><br><span class="line">            p = p-&gt;LeftChild;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            p = p-&gt;RightChild;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!p)</span><br><span class="line">        <span class="keyword">throw</span> BadInput();</span><br><span class="line"></span><br><span class="line">    e = p-&gt;data;</span><br><span class="line">    <span class="comment">// 对树进行重构，处理p有两个孩子的情形</span></span><br><span class="line">    <span class="keyword">if</span> (p-&gt;LeftChild &amp;&amp; p-&gt;RightChild)&#123;</span><br><span class="line">        <span class="comment">// 转换成有0或1个孩子的情形，在p的左子树中寻找最大元素</span></span><br><span class="line">        BinaryTreeNode&lt;E&gt; *s = p-&gt;LeftChild, *ps = p;</span><br><span class="line">        <span class="keyword">while</span> (s-&gt;RightChild)&#123;</span><br><span class="line">            ps = s;</span><br><span class="line">            s = s-&gt;RightChild;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将最大元素从s移动到p</span></span><br><span class="line">        p-&gt;data = s-&gt;data;</span><br><span class="line">        p = s;</span><br><span class="line">        pp = ps;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对于p最多有一个孩子</span></span><br><span class="line">    BinaryTreeNode&lt;E&gt; *c;</span><br><span class="line">    <span class="keyword">if</span> (p-&gt;LeftChild)</span><br><span class="line">        c = p-&gt;LeftChild;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        c = p-&gt;RightChild;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 删除p</span></span><br><span class="line">    <span class="keyword">if</span> (p == root)</span><br><span class="line">        root = c;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (p == pp-&gt;LeftChild)</span><br><span class="line">            pp-&gt;LeftChild = c;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            pp-&gt;RightChild = c;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">delete</span> p;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="类DBSTree"><a href="#类DBSTree" class="headerlink" title="类DBSTree"></a>类DBSTree</h3><p>  若二叉搜索树中的不同元素可以包含相同的关键值，则称这种树是<strong>DBSTree</strong>。在实现该类的时候，只需要把<strong>BSTree::Insert</strong>的while循环改成如下所示即可：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (p)&#123;</span><br><span class="line">    pp = p;</span><br><span class="line">    <span class="keyword">if</span> (e &lt; p-&gt;data)</span><br><span class="line">        p = p-&gt;LeftChild;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (e&gt;p-&gt;data)</span><br><span class="line">        p = p-&gt;RightChild;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>更完整的例子可以查看<a href="https://github.com/ccc013/DataStructe-Algorithms_Study/blob/master/SearchTrees/BSTree.h" target="_blank" rel="external">二叉搜索树的实现</a></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>  本节内容就简单介绍了二叉搜索树的代码实现。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>继续是《数据结构算法与应用：C++语言描述》的笔记，这是第11章搜索树的内容。</p>
<p>本节首先介绍的是二叉搜索树的内容。</p>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本]]>
    </summary>
    
      <category term="数据结构" scheme="http://ccc013.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="算法" scheme="http://ccc013.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[优先队列2-左高树]]></title>
    <link href="http://ccc013.github.io/2016/08/24/%E4%BC%98%E5%85%88%E9%98%9F%E5%88%972-%E5%B7%A6%E9%AB%98%E6%A0%91/"/>
    <id>http://ccc013.github.io/2016/08/24/优先队列2-左高树/</id>
    <published>2016-08-24T10:50:30.000Z</published>
    <updated>2016-10-23T07:20:35.491Z</updated>
    <content type="html"><![CDATA[<p>继续是《数据结构算法与应用：C++语言描述》的笔记，这是第九章优先队列的内容。本节将介绍另一种实现优先队列的数据结构—左高树</p>
<h3 id="高度与宽度优先的最大及最小左高树"><a href="#高度与宽度优先的最大及最小左高树" class="headerlink" title="高度与宽度优先的最大及最小左高树"></a>高度与宽度优先的最大及最小左高树</h3><p>  上一节讲述的堆结构是一种<strong>隐式数据结构</strong>，用完全二叉树表示的堆在数组中时隐式存储的（即没有明确的指针或其他数据能够重构这种结构）。由于没有存储结构信息，这种描述方法空间利用率很高，事实上是没有空间浪费，尽管堆结构的时间和空间效率都很高，但它不适合所有优先队列的应用，尤其是当需要合并两个优先队列或多个长度不同的队列时，需要借助其他数据结构来实现这类应用，比如<strong>左高树(leftist tree)</strong>。</p>
<p>  考察一棵二叉树，如下图9-6a所示，它有一类特殊的节点叫做<strong>外部节点，用来代替树中的空子树，其余节点叫做内部节点。</strong>增加了外部节点的二叉树被称为扩充二叉树，如下图9-6b所示，外部节点用阴影框表示，并且为了方便起见，这些节点用a~f标注。</p>
<p>  <img src="http://7xrluf.com1.z0.glb.clouddn.com/%E5%B7%A6%E9%AB%98%E6%A0%911.png" alt="此处输入图片的描述"></p>
<p>  令$s(x)$是从节点$x$到它的子树的外部节点的所有路径横纵最短的一条，根据其定义可知，如果$x$是外部节点，则$s$=0，若$x$是内部节点，则其$s$值为$min{s(L),s(R)}+1$，其中$L,R$分别是$x$的左右子树。所以上述扩充二叉树各节点的s值如上图9-c所示。</p>
<blockquote>
<p>定义 [高度优先左高树] 当且仅当一棵二叉树的任何一个内部节点，<strong>其左孩子的$s$值大于等于右孩子的$s$值时</strong>，该二叉树是高度优先左高树(height-biased leftist tree,HBLT)。</p>
<p>定义 [最大(小)HBLT] 即同时是最大(小)树的HBLT; </p>
</blockquote>
<p>图9-6a所示的二叉树并不是HBLT，因为外部节点a的父节点，其左孩子$s$=0，右孩子$s$=1，不满足条件，如果将这两个子树进行交换就可以满足HBLT的条件。</p>
<blockquote>
<p>定理9-1 若x是一个HBLT的内部节点，则<br>1) 以$x$为根的子树的节点数目至少是$2^{s(x)}-1$.<br>2) 若子树$x$有$m$个节点，$s(x)$最多为$log_2(m+1)$<br>3) 通过最右路径（即路径是从$x$开始沿右孩子移动）从$x$到达外部节点的路径长度是$s(x)$。</p>
</blockquote>
<p><strong>可以通过考察子树的节点数目来得到另一类左高树。</strong>定义$x$的重量$w(x)$是以$x$为根的子树的内部节点数目。如果$x$是外部节点，则其重量为0；若$x$是内部节点，则其重量是其孩子节点的重量之和加1，如上图9-6d展示了二叉树各节点的重量。</p>
<blockquote>
<p>定义 [重量优先左高树] 当且仅当一棵二叉树的任何一个内部节点，其左孩子的$w$值大于等于右孩子的$w$时，该二叉树为重量优先左高树(weight-biased leftist tree,WBLT);</p>
</blockquote>
<p>[最大(小)WBLT]即同时又是最大(小)树的WBLT。</p>
<p>同HBLT类似，具有$m$个节点的WBLT的最右路径长度最多为$log_2(m+1)$。可以对WBLT和HBLT执行优先队列的查找、插入和删除操作，其时间复杂性与堆的相应操作相同。并且跟堆一样，WBLT和HBLT可以在线性时间内完成初始化。用WBLT或HBLT描述的两个优先队列可在对数时间内合并为一个，而堆描述的优先队列无法做到。</p>
<p>接下来将介绍HBLT的操作，而WBLT的查找、插入、删除、合并和初始化操作与HBLT非常相似。</p>
<h3 id="最大HBLT的插入"><a href="#最大HBLT的插入" class="headerlink" title="最大HBLT的插入"></a>最大HBLT的插入</h3><p>  <strong>插入操作可借助于合并操作来完成。</strong>它可以通过先建立一棵仅包含待插入元素的HBLT，然后与原来的HBLT合并即可。</p>
<h3 id="最大HBLT的删除"><a href="#最大HBLT的删除" class="headerlink" title="最大HBLT的删除"></a>最大HBLT的删除</h3><p>  根是最大元素，如果跟被删除，将留下分别以其左右孩子为根的两棵HBLT的子树，将其合并到一起，便得到包含除删除元素外所有元素的最大HBLT。</p>
<h3 id="合并两棵最大HBLT"><a href="#合并两棵最大HBLT" class="headerlink" title="合并两棵最大HBLT"></a>合并两棵最大HBLT</h3><p>  具有$n$个元素的最大HBLT，其最右路径的长度为$O(logn)$。合并操作操作仅需遍历欲合并的HBLT的最右路径，即仅需移动右孩子。</p>
<p> 合并策略最好用递归来实现。令$A,B$是需要合并的两棵最大HBLT，假设两者均不为空，为实现合并，首先需要检查两个根元素，较大者是合并后HBLT的根。假设$A$具有较大的根，且其左子树是$L$,$C$是由$A$的右子树与$B$合并而成的HBLT。所以$A,B$合并的结果是以$A$的根为根，$L,C$为左右子树的最大HBLT。如果$L$的$s$值小于$C$的$s$值，则$C$是右子树，$L$是左子树。</p>
<h3 id="初始化最大HBLT"><a href="#初始化最大HBLT" class="headerlink" title="初始化最大HBLT"></a>初始化最大HBLT</h3><p>  通过将$n$个元素插入到最初为空的最大HBLT中来进行初始化，所需时间是$O(logn)$。为得到具有线性时间的初始化算法，首先创建$n$个最大HBLT，每个树中仅包含$n$个元素中的某一个，这$n$棵树排成一个FIFO队列，然后从队列中依次删除两个HBLT，将其合并，然后再加入队列末尾，直到最后只有一棵HBLT。</p>
<h3 id="类MaxHBLT"><a href="#类MaxHBLT" class="headerlink" title="类MaxHBLT"></a>类MaxHBLT</h3><p>  最大HBLT的每个节点均需要$data,LeftChild,RightChild和s$四个域，相应的节点类是$HBLTNode$,如下代码所示。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> MaxHBLT;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span><br><span class="line"><span class="keyword">class</span> HBLTNode&#123;</span><br><span class="line">    <span class="keyword">friend</span> MaxHBLT&lt;T&gt;;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> s;</span><br><span class="line">    T data;</span><br><span class="line">    HBLTNode&lt;T&gt;* LeftChild, *RightChild;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    HBLTNode(<span class="keyword">const</span> T&amp;e, <span class="keyword">const</span> <span class="keyword">int</span> sh)&#123;</span><br><span class="line">        data = e;</span><br><span class="line">        s = sh;</span><br><span class="line">        LeftChild = RightChild = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>而最大HBLT可用下面代码定义的类MaxHBLT来实现。类MaxHBLT的每个对象都有一个唯一的私有成员$root$，用来指向最大HBLT的根。构造函数在初始化时将其置为0，因此初始的最大HBLT是空。析构函数通过调用私有成员函数$Free$来删除HBLT中的所有节点，该函数按后序遍历整棵HBLT，每访问一个节点就删除该节点。<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">template&lt;class T&gt;</span><br><span class="line">class MaxHBLT&#123;</span><br><span class="line">private:</span><br><span class="line">    HBLTNode&lt;T&gt; *root;</span><br><span class="line">    void PostOrder(void(*Visit)(HBLTNode&lt;T&gt;*u), HBLTNode&lt;T&gt;* t)&#123;</span><br><span class="line">        // 后序遍历</span><br><span class="line">        if (t)&#123;</span><br><span class="line">            PostOrder(Visit, t-&gt;LeftChild);</span><br><span class="line">            PostOrder(Visit, t-&gt;RightChild);</span><br><span class="line">            Visit(t);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    static void free(HBLTNode&lt;T&gt;* t)&#123;</span><br><span class="line">        delete t;</span><br><span class="line">    &#125;</span><br><span class="line">    void Free(HBLTNode&lt;T&gt; *t)&#123;</span><br><span class="line">        PostOrder(free, t);</span><br><span class="line">        t = 0;</span><br><span class="line">    &#125;</span><br><span class="line">    void Meld(HBLTNode&lt;T&gt; * &amp;x, HBLTNode&lt;T&gt;* y);</span><br><span class="line">public:</span><br><span class="line">    MaxHBLT()&#123; root = 0 &#125;;</span><br><span class="line">    ~MaxHBLT()&#123; Free(root); &#125;</span><br><span class="line">    T Max()&#123;</span><br><span class="line">        if (!root)</span><br><span class="line">            throw OutOfBounds();</span><br><span class="line">        return root-&gt;data;</span><br><span class="line">    &#125;</span><br><span class="line">    MaxHBLT&lt;T&gt;&amp; Insert(const T&amp; x);</span><br><span class="line">    MaxHBLT&lt;T&gt;&amp; DeleteMax(T&amp; x);</span><br><span class="line">    MaxHBLT&lt;T&gt;&amp; Meld(MaxHBLT&lt;T&gt;&amp; x)&#123;</span><br><span class="line">        Meld(root, x.root);</span><br><span class="line">        x.root = 0;</span><br><span class="line">        return *this;</span><br><span class="line">    &#125;</span><br><span class="line">    void Initialize(T a[], int n);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>接下来先给出合并操作的函数实现代码，该函数首先要处理合并的树中至少有一个为空的特殊情况。当没有空树时要确保$x$指向根值较大的树，如果$x$不是指向根值较大的树，则将$x$和$y$的指针进行交换。接下来把$x$的右子树与以$y$为根的最大HBLT进行递归合并。合并后为保证整棵树是最大HBLT，$x$的左右孩子可能需要交换，这是通过计算$x$的$s$值来确定的。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> MaxHBLT&lt;T&gt;::Meld(HBLTNode&lt;T&gt;* &amp;x, HBLTNode&lt;T&gt;* y)&#123;</span><br><span class="line">    <span class="comment">// 合并两棵根分别是*x和*y的左高树，返回指向新根 x的指针</span></span><br><span class="line">    <span class="keyword">if</span> (!y)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">if</span> (!x)&#123;</span><br><span class="line">        x = y;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (x-&gt;data &lt; y-&gt;data)&#123;</span><br><span class="line">        <span class="comment">// 交换x和y</span></span><br><span class="line">        HBLTNode&lt;T&gt; * temp = y;</span><br><span class="line">        y = x;</span><br><span class="line">        x = temp;</span><br><span class="line">    &#125;</span><br><span class="line">    Meld(x-&gt;RightChild, y);</span><br><span class="line">    <span class="keyword">if</span> (!x-&gt;LeftChild)&#123;</span><br><span class="line">        <span class="comment">// 左子树为空,交换子树</span></span><br><span class="line">        x-&gt;LeftChild = x-&gt;RightChild;</span><br><span class="line">        x-&gt;RightChild = <span class="number">0</span>;</span><br><span class="line">        x-&gt;s = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (x-&gt;LeftChild-&gt;s &lt; x-&gt;RightChild-&gt;s)&#123;</span><br><span class="line">            <span class="comment">// 交换左右子树</span></span><br><span class="line">            HBLTNode&lt;T&gt; * temp = x-&gt;LeftChild;</span><br><span class="line">            x-&gt;RightChild = x-&gt;LeftChild;</span><br><span class="line">            x-&gt;LeftChild = temp;</span><br><span class="line">        &#125;</span><br><span class="line">        x-&gt;s = x-&gt;RightChild-&gt;s + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>最后给出插入，删除和初始化函数的代码实现：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span><br><span class="line">MaxHBLT&lt;T&gt;&amp; MaxHBLT&lt;T&gt;::Insert(<span class="keyword">const</span> T&amp; x)&#123;</span><br><span class="line">    <span class="comment">// 将x插入左高树</span></span><br><span class="line">    HBLTNode&lt;T&gt;* q = <span class="keyword">new</span> HBLTNode&lt;T&gt;(x, <span class="number">1</span>);</span><br><span class="line">    <span class="comment">// 将q与原树合并</span></span><br><span class="line">    Meld(root, q);</span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span><br><span class="line">MaxHBLT&lt;T&gt;&amp; MaxHBLT&lt;T&gt;::DeleteMax(T&amp; x)&#123;</span><br><span class="line">    <span class="comment">// 删除最大元素，并将其放入x</span></span><br><span class="line">    <span class="keyword">if</span> (!root)</span><br><span class="line">        <span class="keyword">throw</span> OutOfBounds();</span><br><span class="line"></span><br><span class="line">    x = root-&gt;data;</span><br><span class="line">    HBLTNode&lt;T&gt;*L = root-&gt;LeftChild;</span><br><span class="line">    HBLTNode&lt;T&gt;*R = root-&gt;RightChild;</span><br><span class="line">    <span class="keyword">delete</span> root;</span><br><span class="line">    root = L;</span><br><span class="line">    Meld(root, R);</span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> MaxHBLT&lt;T&gt;::Initialize(T a[], <span class="keyword">int</span> n)&#123;</span><br><span class="line">    <span class="comment">// 初始化有n个元素的HBLT树</span></span><br><span class="line">    Queue&lt;HBLTNode&lt;T&gt;*&gt;Q(n);</span><br><span class="line">    <span class="comment">// 删除老节点</span></span><br><span class="line">    Free(root);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n; i++)&#123;</span><br><span class="line">        HBLTNode&lt;T&gt;* q = <span class="keyword">new</span> HBLTNode&lt;T&gt;(a[i-<span class="number">1</span>], <span class="number">1</span>);</span><br><span class="line">        Q.Add(q);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 不断合并队列中的树；</span></span><br><span class="line">    HBLTNode&lt;T&gt;*b, *c;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n - <span class="number">1</span>; i++) &#123;</span><br><span class="line">        Q.Delete(b).Delete(c);</span><br><span class="line">        Meld(b, c);</span><br><span class="line">        <span class="comment">// 将合并后得到的树放入对了</span></span><br><span class="line">        Q.Add(b);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (n)</span><br><span class="line">        Q.Delete(root);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>对于上述函数的复杂性，构造函数只需要耗时$\theta(1)$,而析构函数需要$\theta(n)$，其中$n$是要删除的最大HBLT中的元素个数。<strong>Max</strong>函数的复杂性是$\theta(1)$，<strong>Insert,DeleteMax</strong>及共享成员函数<strong>Meld</strong>的复杂性与私有成员函数<strong>Meld</strong>的复杂性相同，由于私有成员函数<strong>Meld</strong>仅在以$<em>x$和$</em>y$为根的树的右子树中移动，因此其复杂性是$O(x-&gt;s+y-&gt;s)$。又由于$<em>x$和$</em>y$的最大$s$值分别为$log_2(m+1)$和$log_2(n+1)$,其中$m,n$分别是以$<em>x$和$</em>y$为根的最大HBLT中的元素个数，所以私有成员函数<strong>Meld</strong>的复杂性是$O(logmn)$。</p>
<p>更完整的代码例子可以查看<a href="https://github.com/ccc013/DataStructe-Algorithms_Study/blob/master/PriorityQueue/MaxHBLT.h" target="_blank" rel="external">最大高度优先左高树的实现</a>。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>继续是《数据结构算法与应用：C++语言描述》的笔记，这是第九章优先队列的内容。本节将介绍另一种实现优先队列的数据结构—左高树</p>
<h3 id="高度与宽度优先的最大及最小左高树"><a href="#高度与宽度优先的最大及最小左高树" class="headerlin]]>
    </summary>
    
      <category term="数据结构" scheme="http://ccc013.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="算法" scheme="http://ccc013.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
